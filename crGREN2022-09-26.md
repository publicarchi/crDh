# Atelier GREN 26 septembre 2022

## eBalzac

Présente une édition absoluement fiable de la Comédie humaine. Vérifiée sur l’originale, cad l’exemplaire personnel de Balzac qui portait ses corrections. Sur certains points nous avons même corrigé la fameuse éditon de la Pléïades. Cf. article dans la revue Balzac consacré à l’édition en 2021.

Accès à la Comédie humaine selon un plan de l’œuvre voulu par B, ou encore un accès chronologique ou alphabétique.

Pour chacun des textes, nous donnons (à terme), l’intégralité des versions publiées du temps de B et revues et corrigées par l’auteur. C’est une des spécificités de l’auteur qui avait une écriture très rapide et en ancien imprimeur corrigeait à partir des épreuves et d’une édition à l’autre. Un cas assez rare d’un auteur qui considère la version imprimée comme un manuscrit de travail, il l’écrit d’ailleurs.

La Masion du chat-qui-pelote, plusieurs éditions, jusqu’à l’édition Fune monumentale, et le Furne corrigée. Avec ce site, volonté de les rendre toutes disponibles afin de permettre la lecture de B dans son processus génétique.

Pour chaque œuvre donne détail résumé et bibliographie sélective.

Accès à l’original : en cliquant sur la page, ouvre dans la colonne de gauche l’accès au facsimile, exemplaire de B avec ses corrections personnelles. Exemple page de début de Modeste mignon.

Possible de lire l’une ou l’autre version dans l’édition.

Partenariat avec ARTFL, clic sur un mot pour chercher sa signification dans les dictionnaires d’autrefois, notamment celui de l’Académie française de 1835 qui a réformé l’orthographe et le Littré. Glissements sémantiques, typiquement “orgie”.

Partie génétique du site, grâce au logiciel MEDITE développé par Ganascia. Possibilité de comparer par couple les différentes versions des textes. Exemple Chat qui pelote. On a découvert que B travaille essentiellement par supression. Mais ruine vision d’un écrivain qui travaille vite, sans corrections. Un travail de correction aussi important que Flaubert, simplement il ne se fait pas au même endroit, aux mêmes étapes de la production du texte.

Un moteur de recherche lexical. Assez classique, là encore avec ARTFL de Chicago et Glenn Roe à la Sorbonne. Possible de rechercher les mots avec concordance avec le contexte droit et gauche. Exemple science dans la comédie humaine. Nuage de mots pour voir les collocations dans la même phrase. Homme de science.

Reste partie à développer qui consiste à proposer une édition hypertextuelle de B. Cad mettre en rapport le texte avec toutes sources textuelles potentielles. Détection de réemplois, en utiliant des fragments de correspondances. Visualisation et cartographie des sources littéraires. Affiner l’intertextualité de Kristeva et Genette : ntion de sources, définitions de réemplois, homologie, plagiat en y appliquant des analyses stylistiques.

Corpus cible, l’œuvre de B. Corpus source, tous les textes qui peuvent être considérées comme sources potentielles, les romanciers de son époque. Ouvrages de littérature panoramique, Mercier, Rétif de la Bretonne, etc. et ouvrages auxquels il a contribué. Des ouvrages scientifiques contemporains (notamment dans le domaine des sciences naturelles, de la médecine et de la physiologie). Disait dans l’avant-propos que le romancier comme le zoologiste. Phrénologie, discipline du savoir qui à partir de la morphologie du crane (extérieure) veut deviner ou induire des caractéristiques de la nature intérière de l’être humain (Lavater et Gall).

Front haut et bas. Logiciel Phébus

8 062 couples
4127 Gall vs Balzac

3466 Lavater vs Balzac.

681 couples pour la Physiologie.

Au delà de la statistique couples inexploitables. Ce qu’avons fait, examiner 100 aine de résultats manuellement. Cas où non pertinentes basées sur des liens faibles. Non pertinentes basées sur des liens forts. Relations difficiles à évaluer. Relation pertinentes basées sur des liens faibles. etc.

Bouche et bonté, dans le curé du village.

Pb du tri. Mais base lexicale simple ne permet pas les synonymes et les hypernonymes.

analyse qui nous montre que la phrénologie est un modèle paradigmatique. Indiciaire, de l’extérieur peut lire l’intérieur. Structure mimétique de la démarche scientifique. Selon Lavater le visage peut se diviser en trois. Voilà que B utilise exactement la même structure. Les coocurences existes, malhereusement il faut encore les chercher à la main.

## Elena Pierrazzo, Distant Editing

Livres 2015, philologie numérique. Mais après coup, impression d’avoir manqué ce qui survient aujourd’hui. N‘y mentionnait ni l’IA, ni l’apprentissage profond ou les approches informatiques et computationnelles. Encore rares et marginales. Principales applications, collation automatique, mais souvent mal acceptées.

À l’époque l’idée que l’on puisse faire collation automatique une utopie. READ et Transkribus, débuté en janvier 2016.

Approches philogénétiques regardées avec méfiance. Boucle épistémologique, si connaît le stemma comment ne pas être influencé, et inversement. Utilisation des approches computationnelles pas inconnues du domaine. Moretti, etc. Lecture à distance et attribution de paternité.

Résultats importants venaient de sortir. Démasquage de Robert Calbraith qui soulevait des questions éthiques. Mais à l’écart de la réflexion de l’éditeur car éloignée de l’ajout de balisage au texte.

Objectivité et interprétation. Pérennité de l’édition... telles étaient les principales occupations des méthodes numériques. Certains chercheurs pionniers convaincu de l’intérêt de ces méthodes.

Apparté, le fait qu’après 70 ans puisse encore parler de pionniers et de révolution, excitant !

J.-B. Camps 2019, soit disant révoltion HTR. Pas si ancienne. Venise Time Machine, Transkribus etc.

Travail de Mike Kestemont, 2016 séduit la communauté. Possibilité de grouper des ms par type d’écriture. Impact si profond sur la communauté scientifique. Travaille actuellement sur la production des moines chartreux et montré que pouvait voir changement de mains, etc. et compréhension de la façons dont les scriptorium fonctionnaient.

Ne connaît que 9% des corpus. Lire pour la première fois ces matériaux. Millions image de qualité de ces ms. “Que pouvons-nous faire avec ces millions images de ms” Greg Crane. Standardisation des données IIIF. 

Potentiel révolutionnnaire de ces approches. Dans le domaine du matériel médiéval que les évolutions ont eu lieu. 

Dominique Strutmann, tout sauf révolutionnaire. L’un des premiers paléographes à s’engager dans les méthodes paléographiques pour la transcription. Livres prières à destination de laïcs. Source de première importance pour histoire sentiment religieux, circulation des textes, etc. car standardisés. Premier livres de rayons produits par des scriptoria en dehors de tt ordre. Richement décorés, cycles de représentation stables.

Livres d’heure souvent négligés comme production de masse. Si abondant que personne pu les étudier. Du point de vue textuel des compilation de compilations. Produits en série. Mais comme jolis, largement numérisés. Masse d’images accessibles en ligne comme Biblissima.

Premier résultats émergent et résultats époustouflants. Permet de révéler centre de productions, etc. Variations dans les contenus. Sommes en train de gagner un aperçu inimaginable sur ce matériel grâce à ce type de recherche.

Autre type de recherche celles de JB Camps. Prix innovation Utrecht. Stylometry for Noisy Medieval Data... Variations orthographe mêlées à des variations substantielles. Travail qui montre que peut utiliser du texte balisé en TEI et que cela peut être très utile pour normaliser les mots, etc.

JB Camps à l’origine d’un modèle de philologie computationenlle. De la numérisation des sources à l’édition. À chaque fois que présente le modèle se demande si en sommes déjà là ? Non, mais une étape à la fois.

eScriptorium avec Stockes, démocratisation de ces outils. Recherches pour permettre téléchargement de ces outils. Bonne gestion scripts non-latins, mais aussi tout type d’écriture de la Renaissance et moderne.

Plusieurs réactions possibles. Vieillissons, aucun doute. Mais aussi se demander les implications pour l’édition critique à plus long terme. 10 ans et plus. Cela implique-t-il que tous mis à la retraite. Bien sûr que non, et pourtant. Possible de regarder vers le passé et voir ce qui s’est passé lorsque des méthodes informatiques sont arrivées de disciplines cousines comme la linguistique.

Karen Spärck Jones, 2007. Computational linguistics : what about the linguistics ? 

Relève que pas de trace de linguistique computationnelle dans la revue. Noté comment se présentait comme innovation étonnante et surprenante mais seuls quelques chanceux pu utiliser car rares et peu faciles à développer. Dès lors émergence de doutes et peu d’apport. Avancées sans référence linguistique main-stream. Computational linguistics doies not need mainstreeam non-computational linguistics. Arrogant add-on !

Travail mené par des spécialistes de l’informatique plus que des linguistes car besoin de compétences computationnelle.

Disciplines séparées. Linguistiques computationnelles résulats où informaticiens pas des linguistes donc collaborative ou les informaticiens se sont substitués à eux.

Risque de perte de notre corps de métier qui est réel. 

Se souvenir lorsque Google books devenu le nouveau jouet avec qui tout le monde pouvait s’amuser. Présenattion NGram viewer.ç présentation Jean-Baptiste Michel, etc. al 2011. Publié dans Sciences, représentés et dépeints comme ceux qui avaient amené les sciences humaines au 21e siècle. L’ont fait sous notre nez.

Algorithmes difficiles à maîtriser. Si acquisition informatique facile à atteindre. Plus complexe de l’informatique computationnelle. Maîtrise mathématique. Demande humaniste fréquente informaticien. Réduction intellectuel car implique abandon du contrôle pour la boîte noire. Ce qu’avons essayé de ne pas abandonné en nous salissant les mains avec le code.

Quid si tourne mal. Qui finit par posséder la recherche. Approches informatiques qui ne fonctionnent qu’avec de grands ensemble de données. Car petits textes. Mais si exclus de l’informatique de pointe, plus les enfants les plus cool du quartier. Risque de financement plus dangereux.

Numérique opportunité pour renouveller critique textuelle après période où désintérêt ou seulement service. Risque que les bons résultats obtenus par nos recherche, restent dans le passé. Si nos textes pas traitables comme cela, pourrait tout simplement devenir plus édité du coup.

Hate de voir à quoi ressemble le Distant editing. Quand commencé à couper les mots. Quel auteur le plus édité à la Renaissance, etc. La communauté scientifique autour de l’édition numérique devrait adopter ces méthodes et voir ce que peut en faire. Ne pas être à la traine.

Besoin de formations. Acquérir une compréhension de base dans l’alphabétisation. Besoin d’idées et de créativité. Ce qui va être fait, ensuite la technologie suivra. Ne peut seulement suivre la technologie. Mais nous devons aussi nous soucier de l’éthique. 

Demasquage, discussion implications de genre. Anonyma des femmes. Avoir le droit de le faire, dire qui se cache derrière le nom de Robert... Recherche sur Elena Ferrante, utilisation homonyme, appel à la publicité et autorisé à le faire. Attribution paternité ou maternité d‘un texte. Méthodes utilisées pour des raisons moins critiques : analyse visage, influencer les élections. Être à la pointe de la recherche aussi parler des méthodes qui perpétue les inégalités, etc. 

Les outils pas neutres, ils ont une agency. Ethique, déontologie, autorégulation qu’associons au travail enseignant et peu à la recherche car auteurs morts depuis longtemps. Et pourtant.

Des devoirs à faire ? à quoi ressemblera l’édition numérique, le fait d’éditeurs ou d’informaticiens ? Devrons décider ces choses ou bien seront décidées magré nous.

### Discussion

Moulin : Étonnée de la dichotomie que vous présentez. Depuis trente ans, travaillons avec des informaticiens. Un bachelor en Science et humanities depuis plus de 10 ans.

Ils ne nous enlèvent rien et nous ne leur enlevons rien si travaillons dans un esprit de communication.

Alignement des questions épistémologiques. Les humanités numériques 3.0. Pas la philologie avec l’histoire de l’art. Pas une méthode mais la recherche. C’est la raison pour laquelle, je pense que l’on doit y être dedans.

Plusieurs enjeux : de fait les bibliothèques remplies de documents inconnus et n’y arrive pas. L’ordinateur ne donne pas des réponses mais nous donne des pistes. L’informatique ne donne pas des réponses, seulement des pistes.

Besoin de créativité, de nouvelles questions. Avoir la créativité de trouver des questions que l’on n’a pas encore entamées.

Labex Obvil, projet qui fédérait nombreux projets. Mais en perdition car reposait sur l’idée de demander des choses à des informaticiens. Et besoin de ça pour avancer comme un service. Alors n’a pas fonctionné.

## Elena Spadini

Sur la réutilisation des données dans les éditions numériques.

Pourquoi et quoi réutiliser. 
Mesures qui permettent le réemploi.

Dumps et APIs

Open question.

Réutilisation facilite la collaboration, le partage. Augmente visibilité de la recherche, plus de citation et maximise l’impact.

MLA 2016, Statement on the Scholarly Edition in the digital age : sampling, reuse and remix.

Nous sommes tous d’accord sur le fait que projets de réemplois des histoires à succès. Exemple Old Bailey dont les données beaucoup réutilisées.

Peut partager du code, des outils. Mais ici, il est plutôt question de parler du partage de données. Réutilisation dans le contexte numérique, cad. machine consomption en vue de traitements suplémentaires, d’annotation, etc.

Données d’une édition multiples. Bien sûr nous avons le texte qui peut être employé pour l’analyse dans différents contextes. Catalogues archivistiques, mais aussi entités et prosopography, gazetiers. L’enjeu pouvant aussi être celui d’une réplicabilité (peer review, update).

Pour partager les données, web scraping mais aussi accessible dump.

Le scrapping peu commun dans notre domaine et peu pertinent. Ce que l’on fait lorsque les données ne son pas disponibles. Manque les métdaonnées. 

APIs et autres points d’accès.

API ou SPARQL endpoint. API sans doute le futur. 

Cf. Jeffrey Witt, c. 2018 Digital editions and API.

Christophe Guéret, 2015, Stop making tools : nobody likes them anyway

19 éditions avec API, mais souvent pas de documentation.

- TEI publisher. 
- DTS Perseus projet,
- DaSCH Service Platform DSP Gusatve Roud, textes and archives
- CorrespSearch
- Carl MAria von Weber GEsamtausgabe
- eedition humboldt digital
- Registres de la comédie Française
- Sandrart.net
- The Folger Shakespeare
- The Proceeding of the Old Bailey

Documentation plus ou moins développées. Systèmes qui permettent de tester les API et de jouer avec. Les registres de la comédie franaçaise Postman editor.

Formats divers proposés. JSON, XML TEI, VSV.  TEI souvent pour les dumps. Parfois seulement accès aux métadonnées par l’API.

IDs acceptées, parfois internes ou données d’autorités.

Possibilité de cherche plusieurs ensemble de données avec système de données. Projets qui ne correspondent pas complètement.

Quelques APIs donnent accès à la structure du document. Possible accéder à un fragment de document. Certains outils donnent accès à des données de collation.

Variétés de format. Retrouver contenu et structure. Authority file records. Curated or raw data.

Question ouverte sur le versionning. 

Versionning et réplication est-elle réellement possible. Tout changera dans le temps. API une version, etc. Limitations pour la réplication.

Consommation API tâche avec laquelle les académiques familiers. For cotent queries does it make sense to define a common set of entities. FRBR, entities, etc. Déjà DTS pour les métadonnées.

## Peter Stokes

Transcription automatique et relation avec les éditions numériques. Projet numérique eScriptorium. Ben Kiessling, Daniel Stökl Ben Ezra, R. Tissot et nombreux autres.

Difficultés de réutilisation de données et enjeux du multilinguisme. eScriptorium.

Open-source HTR software par Benjamin Kiessling EPHE

- segmentation automatique d ela page
- transcription automatique
- Libre et ouvert (choix ouverture des modèle)
- optimisation pour les écritures historiques, ou non latines
- Conçu comme un block dans une chaîne de travail automatique

Kraken en ligne. Mais un logiciel en ligne de commande Python, faussement facile à utiliser. On a donc pensé ajouter une interface plus facile d’utilisation eScriptorium.

Une simplification pour rendre plus facile Kraken, et interagir avec les données. Segmentation presque impossible pour entrer les données en ligne de commande.

Projet débuté il y a 2 ou 4 années seulement. Pas une plateforme de publication. Une étape dans les flux de travail.

Code source en ligne. Différence avec Transkribus, possible de mettre en place sa propre installation. Mais besoin d’ordinateurs très puissants pour l’entraînement de modèles.

Scripta-PSL projet sur l’histoire de l’écriture en général. EPHE travaille sur toutes les écritures que l’on peut imaginer. Rêve de pouvoir créer un système qui puisse répondre aux besoins de tous ces systèmes d’écriture.

- Importation des images IIIF, PDF, zip
- Segmentation des iamges via Kraken ou import, correction de la segmentation, réentrainement du modèle
- transcription
- entraînement du modèle

Création d’interface complexes, sens des écritures, etc. Commence à avoir de très bons résultats même avec des images qui manquent de clarté. Les progrès réalisés ces dernières années sont absolument étonnant.

Réutilisation de données possible grace à des transcriptions disponibles en ligne. 

Pas encore possible de proposer une transcription automatique de vieux javanais car pas suffisamment de données, mais interface considérée comme commode par l’étudiante pour la transcription manuelle.

Export possible des données et publication sur Zenodo pour permettre leur citation.

Volonté être plus transparent sur le modèle.

Limitations du ML. 

Dépend des données utilisées pour l’entraînement ce qui a plusieurs conséquences.

- ML travaille pour toi si vous connaissez les réponse en avance
- Biais dans les données sont transférés directement à l’AI
- les données sont plus importantes que les logiciels
  c’est la raison pour laquelle la question du partage des données est critique pour ce genre de recherche

Ce qui a des conséquence. Utilisation modèle entraîné pour le latin. Application à anglais pb. Dans documents multilingues compliqué. 

Comment réutiliser les données. Souvent oui mais avec des difficultés. Besoin d’un alignement entre le texte et l’image. Possible de le faire automatiquement de plus en plus.

Un projet avec un stagiaire qui a travailé sur les papyrus grecs. Webscrapping, fonctionne bien.

Mais pose aussi des questions de transcription car il n’y a pas une manière objective de transcrire le document. Besoin de consistance. Car si change pas possibilité de réutilisation.

Penser aux standards de transcription. Pas toujours possibles, ni toujours une bonne idée car des textes différents nécessite des transcriptions différentes.

Pas baseline dans Alto.

Plusieurs initiatives en cours comme SegmOnto un vocabulaire contrôlé pour la description des mises en pages. Pas facile être absolument strict mais possible de proposer des choses.

HTR-United mise en partage des données d’entraînement. Partager des données avec un peu de standardisation. Pose aussi la question de savoir comment réutiliser les résultats pour la transcription numérique.

Pas export en TEI dans eScriptorium. Pourquoi pas mais pas si facile que cela. Ne considère pas souvent les difficultés. TEI un modèle sémantique. Bien sûr modèle document, mais souvent sémantique du texte. En général avec transcription automatique seulement l’image.

Le modèle de texte est différent entre une transcription automatique et édition TEI. Pas possible d’avoir une transformation pour tous les documents.

Partick Sahle, text wheel, très utile ici. Bien sûr pourrait produire transformation plus aisée mais dans contexte multilinguisme de plus en plus compliqué.

Une transformation de base facile, masi TEI

Pas une priorité, plus intéressant que partage nos codes pour partage et réutilisation.

Exemples d’utilisation du vocabulaire de segmentation pour produire transformation. cf. Simon Gabay

Plus complexe documents hébreux. Possible de créer un flux de travail qui répond à ces besoins. Outils créés par différents collègues qui peuvent s’inscrire dans ce flux de travail pour parvenir à aider la production des éditions.

Cas particuliers épigraphie générale, etc. Raison pour laquelle pas de modèle général. Important pour nous de faire après.

Notre approche, pas possible de rencontrer tous les besoins.

L’équipe ne peut pas tout faire.

Plus important de permetre flexibilité facilité import export, et créer outils externes

partager code et transcriptions et pratiques

travailler collectivement un effort à long terme. Progrès très encourageants.

## Discussion

Durabilité et écologie. Avantage de pouvoir partager les modèles sans avoir besoin de les réentrainer. Mais en pratique pas si évident car dès lors que besoin transcription différente. Devons y réfléchir. Réutilisation chaleur des ordinateurs pour chauffage.

Enjeux de documentation, Transcribus bon travail. Sinologues venus nous rejoindre. Parfait car possibilité réutilisation des données.

Question théorique sur la réutilisabilité, tension entre développement de modèles particuliers et réutilisabilité. Développement de modèles particuliers fait partie de notre démarche théorique. À quel point parfois la standardisation pré-informe le modèle épistémologique que l’on est en train de créer. Parfois intérêt à réinventer la roue. Sinon reprise modèle et anule la réflexion épistémologique.

Bien sûr TEI flexibilité. Pour la réutilisabilité avoir uperlevel ontologies nécessaire. Pas encore beaucoup de réutilisation des données donc difficile de se poser ces questions car reste en grande partie théorique.

Sciences humaines celles du dissensus. TEI ne force pas un consensus complet. Nous force à réinventer la roue à chaque fois. Ne plaît pas aux informaticiens car doit réinventer à chaque fois des outils. Standardisation de la pensée. Disponibilité outils généralistes comme word forcé à penser autrement. Un agency. Donc un équilibre entre standardisation et répondre à nos questions de recherches.

Intéressant que mouvement de LOD, couche supérieur qui permet de partager ce que peut partager.

Le propre de notre recherhce qui est d’être différent. Spenberg McQueen, infinite facts about text. Possibilité embrasser communauté. Langage commun et possibiliét avoir point de vue différent et être heureux de l’avori. Documentation modèle partie fondamentale de ce que l’on fait.

Utilisation didactique outils comme eScriptorium.

Pilar sur les downside du partage. Pas contrôle comment projets finalement utilisés. Quid si utilisation programmes conçus dans contexte militaire. Et si ouvre et que ne contrôle pas quoi fait avec notre travail. Comment contrôle le fait que ne soit pas enrôlés dans une économie de surveillance ? Question des licences ?

Discussion importante car choix que nous faisons limitent diversité, etc.





Question XPath

Incorporer ensemble de nos processus dans des chaînes de traitement complète. Pas seuelemnt reconnaissance écriture, mais zones, pagination, structure du texte, etc. Normalisations textuelles. Pb des standards pivots.



## Joana, parcours d’actualisation

Modélisation de l’information et représentation de l’information. Possibilité de démultiplier les parcours critique. Modalité hypertextuel de l’édition. Non-seulement les éditeurs conçoivent ces parcours en fonction des publics cibles. Différent de l’édition papier où un seul discours critique qu’il veille à nourrir.

Éditions numériques qui ne répondent pas à un modèle unique. Explique que la normalisation des éditions numériques semble compliquée et peu souhaitable.

Invention de nouveaux codes propres à l‘édition numérique. Nécessite guider le lecteur pour ne pas le décourager.

Sémantique de la page écran pour rendre intelligible la page le plus immédiatement possible. Parcours centré autour de la documentation principale. L’éditeur explicite les références culturelles implicites ou explicites qui pourraient échapper au lecteur d’aujourd’hui.

Actualisation du texte qui se fait avant tout au moyen de notes critiques. Références au texte religieux. Celles propres à la littérature, celles propres au contexte historique de l’époque. Notes d’identification, références littéraires. Dans l’édition numérique l’actualisation peut se distinguer du texte éditer pour devenir autonome et se transformer en voie de circulation à travers le corpus.

Par exemple, l’identification des noms ou des lieux. Forme très classique d’explication du texte. Dans l’édition imprimée note de bas de page, dans l’édition numérique utilise l’index qui n’est plus uniquement une liste de renvoi de références mais un lieu spécifique où regroupe ces identifications. Peut être couplé avec note au survol, mais peut devenir le pivot de ce parcours de lecture. D’expliciation ponctuelle devient forme de circulation principale dans le corpus.

Exemple, éditeurs du Voyage de Marco-Polo de l’Université de Venise. Accompagnement de l’identification d’une présentation du lieu et une carte historique.

Autre forme, mise en lien avec dictionnaires, etc.

### Tradition ésopique médiévale

Fables héritières tradition... Romulus et Avianus, deux adaptations du 5e siècle d’après deux grands fabulistes du 1er siècle : Phèdre et Babrius. Succession d’adaptation. Socle de la tradition médiévale ésopique.

Romulus de Nilan, recueil scolaire à l’origine des fables de Marie de France 11e s. 12e deux adaptations latines en vers, recueils scolaires qui donnent lieu à deux réécritures Isope 1 et II. Autre exemple recueil utilisés comme initiation au latin et illustration réthorique. Anonymus Neveleti.

Tradition moins foisonnante du côté de l’Avianus avant d’être supplantée par la tradition romuléenne. Un recueil constitué des deux traditions.

Tradition qui se pose doublement : interne et externe.

L’écrit médiéval est intrinsèquement intertextuel. Certains textes ont accentué cette effet. C’est notamment le cas de la tradition ésopique.

Traitement de l’intertextualité : répertoire interactif des citations.

Proposer aussi une lecture diachronique des fables, proposer parcours sur els formes diverses qu’ont pris certainnes fables.

Pas nécessairement besoin ête complète.

## Jason “More lives more than one”

The problems of life writing. Genre parmi les moins utilisé. Pb questionnement organisation et présentation. Qu’est-ce qui rend crédible.

## Hypertext

Comment construire une longue tradition d’hypertexte pour les éditions et mettre l’hypertexte sur des stéroïdes. Pas un textuel scholar. Travaillé sur projets d’édition, travaillé sur des outils. Mes pensées comme touriste dans votre domaine.

Vannevar Bush

Georges Landon, argumente que révolutionne notre interaction avec le texte et manière dont nous pensons. Nous savons que la textualité numérique modifie notre manière de pensée. Nombreux échanges. Gens lisent différemment et pensent différemment.

Une communauté et des standards.

Des méthodes puissantes, manière de les lire. Réseaux rapides pour donner accès information.

Mais nombreux gaps. Premier problème choses qui ne se parlent pas sur le web. Des liens mais doit les établir manuellement. Multiplicité textualité et variations difficile à représenter par un hyperliens.

Comment utiliser le web pour les éditions critiques pour aller au-delà de simples liens. LOD web annotation et IIIF. Encore besoin de réaliser le potentiel de ces technologies pour l’édition numérique. Une technologie lourde par rapport à XML.

Formal ontologies logique du premier ordre.

Open Annotation, modèle très générique. 

Don Norman, The design of everyday things, 1988

Affordance, relations entre les proprités d’un objet et les capacités de l’agent.

- discoverabiilté
- multimédialité
- multilinguisme, The secret of nature
- collaboration, The infinite Ulysse, terminé car pas standard.
- contextualisation, travail édition nuémrique 
- etc.

Un outil, un prototype mouillé pour ramener du contexte.

Idée de texte fluide.

Completeness ?!

comment inclure choses derrière Pay-wall ?

Agency

Pas le même niveau de contrôle qu’une édition papier stabilisée.

Possibilité de choisir quel type d’entités veulent, quelles sources, etc. Contrôle sur qui a produit les données. Science citoyenne, etc. Degré de précision, et rapidité. Outils pour scanner les documents et knwoledge graph. Mobilité.

Questions éthiques 

