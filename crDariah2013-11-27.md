Journée DARIAH
==========

Introduction, Sophie David
-----------------------
Succès tel que plusieurs problèmes d'organisation.
Présentation de DARIAH : Une initiative européenne qui cherche à développer et soutenir la recherche dans diverses disciplines, en sciences sociales, tout au moins les projets numériques.
Dariah a commencé en 2006, bientôt un ERIC, une structure à laquelle adhèrent des pays. Autriche, Allemagne, Luxembourg, Italie, etc. On l'espère bientôt la France et l'Irlande (mais en bonne voie). Également Grèce, Croacie, Serbie, Belgique en 2014. Dès que la France rejoint l'ERIC, lanceront un appel à contribution. Un dispositif ouvert à tout laboratoire, etc.

L'organisation d'une journée DARIAH-FR
Une journée organisée à l'initiative du comité de direction de DARIAH dont le rôle est d'organiser et de coordonner la participation française à DARIAH. Parmi ses missions, l'organisation de journées de réflexion.

Choix des vocabulaires
 - enjeux théoriques (partage de réflexion)
 - examiner les avantages, écueils de certains choix (partage d'expérience)
 
La question des vocabulaires est une question centrale dans DARIAH et que l'on rencontre très souvent. Si par exemple contribution Autriche, quels sont les formats pour les images, qui s'intéresse à la poterie du 12e s. Pour obtenir des réponses à ces questions, il est tout à fait pertinent d'avoir catégoriser ses contributions avec des vocabulaires pour qu'il soit plus facile de les retrouver.

Pour développer l'interopérabilité entre des entrepos, [DANS (Narciss)](http://www.narcis.nl/about/Language/en) et Isidore. Parce que ces triple store sont fondés sur les technologies du web sémantique, parce que formalisme exprimés dans celui du web sémantique, et parce qu'alignement des vocabulaires, possible de mettre en commun ces réservoirs de données.

Question élaboration de ces vocabulaires, leur complétude, leur usage, etc.


Introduction : Nicolas Larousse ?
-----------------------

Pour nous le web sémantique important
Présentation historique par Alexandre Monin, Thibault Grouas qui présentera projet important dont le but est double.


Alexandre Monin
-----------------------


Très heureux qu'une journée comme celle-ci se tienne aujourd'hui car crois beaucoup qu'il y a une rencontre à effectuer entre web sémantique et humanités numériques qui jusqu'à présents n'étaient pas deux domaines liés. Plusieurs projets qui s'y essayent cf. DBpédia.

Pourquoi parle-t-on d'ontologie ?
Revenir sur le sens philosophique des ontologies
Revenir sur les liaisons dangereuse du web sémantique et l'intelligence artificielle.
Mais passage obligé, montrer que le web sémantique transforme en profondeur les problématiques et les applications développées. 

1° Ontologies/philosophies, ontologies/informatiques
2° Liens entre intelligence artificielle et le web de données.

### De la philosophie à l'informatique
Ontologies/philosophies, ontologies/informatiques

#### Deux traditions

Le mot ontologie est apparu très rapidement. Ce n'est en effet qu'au tout début du 16e siècle que deux philosophe l'ont employé. Pour autant, on fait souvent remonter l'origine du concept à la _Métaphysique_ d'Aristote. Ouvrage compilé par les élèves. On y trouve la définition d'une "science introuvable", l'étude de l'être en tant qu'être et non l'étude des domaines.

Confusion entre métaphysique et ontologie fréquente. S'explique par l'origine dans la métaphysique d'Aristote. Pourtant les 2000 qui séparent les deux projets ne sauraient se concevoir sans des glissements conceptuels tectoniques importants.

Montrer que la notion d'ontologie en informatique garde certains aspects de ces concepts.
2000 ans c'est considérables, mais à l'inverse
Tradition classificatoire, suite de Porphyre. Aristote distinguait 10 grands genre de l'être qu'il appelait les catégories. Organiser les rapports entre ces catégories. La théorie des prédicat (cf. Bachimont, aspects pertinents pour l'ingénieurie des connaissance ) qui organise les relations entre sujets et prédicat au travers des être et substance
- définition
- genre
- propre
- et accident
Porphyre élargit liste de ces prédicables, dans son arbre de Porphyre. Ajoute être et substance. ??? 5 prédicables.
Chaque être peut se trouver au sommet arbre
...

En philosophie, l'opposé
Pluricité de l'être. Pour Bruno Bachimont les catégories renvoient à une homonymie de l'être. Si bien que les notions posées dans le cadre de chaque catégorie ne sont pas comparables.
équivocité de l'être cède donc à la thèse de l'univocité (l'être ne se dit plus dans 10 différents genre, mais la question de l'être rabattue sur la question du quelque chose en général sur le genre objet). L'ontologie se meut alors en une théorie de l'objet.

L'ontologie au 17e ce n'est plus la théorie classificatoire précédent, mais une théorie de l'objet.
Distinction à l'origine dans la tradition allemande entre métaphysique générale et métaphysique spéciale (l'objet en général tout ce qui est est un objet, ou l'étude de quelque chose).
Quand on parle en philosophie d'ontologie, avoir en tête "théorie de l'objet".
Néanmoins important d'avoir en tête cette histoire pour distinguer ces deux traditions. D'un côté une tradition classificatoire avec des recherches s'inscrivant dans cette tradition : la langue universelle, les arts de la mémoire, la bibliothéconomie, l'ingénierie de la connaissance. Et d'autre part, un questionnement en apparence purement philosophique. Or on le verra, il n'est pas si simple de séparer ces deux notions à propos des ontologies informatiques.


#### Une typologie des ontologies

Proposer pour s'orienter une typologie des ontologies informatiques, l'occasion de revenir sur leur diversité

D'abord une ontologie descriptive, une manière de représenter nos schèmes, nos pratiques linguistiques, portée par la difficulté de s'abstraire de nos pratiques linguistiques.

Une manière de représenter nos connaissance, et nos manières naïves d'appréhender le monde et les objets familiers. Manière "mésoscopique" sur la façon dont aborde le monde quotidien. Une ontologie dans laquelle peut se reconnaître, une ontologie aujourd'hui délaissée par la physique contemporaine.

Des ontologies formelles qui vont chercher des catégories susceptible de traiter toute la réalité. À la suite de Husserl. S'appuie sur la métrologie, c'est à dire à quelque chose de formel, et notamment les relations entre tout et partie. L'idée serait de coordonner toutes les régions de l'être. Très ambitieux, est-ce possible.

Formelle au sens scientifique, transcrire dans un idiome canonique qui est celui de la logique des prédicats. Influence importante sur l'intelligence artificielle.

Définition de Thomas Gruber (inventeur de Siri) qui a donné la définition d'une ontologie descriptive comme la conceptualisation d'une définition de domaine. Autrement dit, un catalogue d'entité spécifiant les relations entre entités, etc. modélisant les relations d'un domaine d'activité, établi par consensus, traduisant une communauté de pratique et d'intérêt.
Non seulement formalisé, mais au moyen d'un langage informatique pour construire des applications.

Des ontologies transverses de haut niveau, pour lesquelles utilise des méthodes philosophiques. Destinées à normaliser la signification des définition formalisées dans des ontologies de domaine. Sert à les coordonner. Un tel cadre ménagé par le recours à des contraintes de niveau supérieur. Métapropriétés de Nicolas Grandino ?? en Italie, Aldo Grandini ?? Paris XIII. Laisse entrevoir la possibilité de coordonner différents domaines.
Uper, toplevel, foundation ontologies.

Chacune de ces définitions reprennent la notion aristotélicienne de l'ontologie mais avec des différences notables concernant les outils employés pour développer ces ontologies.

Notion de l'ontologie, pose la question de l'héritage. Ce sans égard avec la question de l'homonymie de l'être. Par exemple dans le cas de Porphyre, des arbres les uns à côté des autres. Chaque hiérarchie ontologique chez Aristote ne peut qu'être un arbre à côté de l'autre. être ne pouvant dériver d'un autre être.
Pas des étiquettes désignant des individus, mais des significations permettant de se rapporter à des individus ou des ensembles d'individus. Ce que sont les ontologies formelles.

Les ontologies informatique, au contraire, étagent les individus. Leur but de dériver un maximum de connaissance en liant des données hétérogènes par le biais de dérivation logiques.
Le web complexifie cette perception de l'ingénierie des connaissance en utilisant les étiquettes.

Discussion :
Aurélien Berra : En revenant sur Aristote et théorie de l'objet. Et démarche informatique.
But de l'école aristotélicienne connaître le monde, mais dans un sens précis : par exemple savoir quelle langue utilise dans quel pays, à l'époque d'Aristote connaître tous les textes possibles, envoyer voyageur 6 mois, etc.
Est-ce que chez Aristote les catégories lui servent à connaître le monde et à l'observer ?

AM : Il n'y a pas d'opposition entre la recherche et l'IST. Plaide plutôt pour que l'IST soit une partie de la recherche. Pas de distinction entre empirisme et métaphysique. Aristote écrit par exemple des traités naturalistes. Il a ce rôle de scientifique qui mène des enquêtes scientifiques. En même temps théorise la démarche scientifique avec le syllogisme, etc.
Question aujourd'hui de la place de l'ontologue : participe-t-il à une enquête de terrain ? Là pour faire parler les experts de terrains ? Ou fabrique-t-il son ontologie dans son coin ? Une position épistémologique qu'il serait bien de définir. Pour ma part, je pense qu'il aurait intérêt à s'inspirer de la démarche des sciences sociales, au sens où une ontologie est avant tout un standard. Lorsque commence à aborder les ontologies en tant que standard, moins problématique. Du langage, un schème conceptuel, et pas la réalité. Alors ingénierie des connaissances rejoint la réflexion philosophique.

Aurélien : Modèle d'organisation des connaissance qui sert à permettre la réalisation des enquêtes par d'autres et accompagne par ailleurs la constitution de bibliothèques. 
Des questions qui reposaient sur des besoins d'organisation de la connaissance.

AM : La même chose chez Platon, Leibniz qui était également bibliothécaire. Il y a déjà de la matérialité quand la philosophie s'élabore. Dès le début, cette question de l'ontologie est liée à la question des supports, des standards, etc. 
[perso : cf. Pierre Hadot, humain aussi]


### Liens entre intelligence artificielle et le web de données

L'apparition du mot ontologie dans le domaine de l'intelligence artificielle, puisque le mot a été utilisé il y a 33 ans par le fondateur de l'intelligence artificielle.
On a pu parler (Patrick Heiss et Bachimont), d'un courant néo-aristotellicien dont l'idée était de représenter les connaissances comme une manière formalisée.
La connaissance devenue les connaissances : un stock discrétisé de connaissances manipulables et calculables.

Si John Mac Carthy à qui on doit d'avoir fondé le courant logique de l'intelligence artificielle, il a employé dès 1980, inspiré par un philosophe [?? Qwain] le mot ontologie pour "ce qui est".

Pour autant c'est plutôt aux travaux de Patrick Heiss sur la physique naïve que l'on doit l'idée de formaliser le sens commun pour favoriser un calcul de procédures de décisions. Par exemple système expert pour déterminer un diagnostic médical.
Avec lui, un pont qui s'établit entre certain nombre de traditions philosophiques du XXe siècle : l'intelligence artificielle et le web sémantique. Heiss est l'auteur de la sémantique formelle du web sémantique, cad RDF. Si la sémantique de la RDF fonctionne comme logique formelle grâce à lui.
Acteur passage IA web sémantique, et web sémantique et web de données en proposant d'aller au-delà.

Mac Carthy décrit son prog d'intelligence artificielle en 1981 : évoque son programme de 1959 : programme pour décrire le sens commun, représenter ce qui est connu sous forme d'énoncés dans un langage logique (la connaissance). Le sens commun en déduirait ce qu'il faut déduire comme conclusion et prendre décision.
Représenter à la fois connaissance d'une situation particulière, et connaissances générales sous forme d'assertions logiques.
Problèmes immédiatement apparus pour représenter le sens commun sous la forme de connaissances générales. Pour représenter la connaissance générale, trop grand nombre de catégories devant être représentés sans explications...
[ex. système expert proposant de faire bouillir le rein pour tuer les microbes, mais tue le patient]. Pour qu'une action soit efficace dans un contexte, il faut qu'un élément général du sens commun soit explicité. alors une explosion des apriori nécessaires pour représenter le sens commun sous la forme d'énoncés. S'est alors employé à gaver les machines de représentation du sens commun.

Idée de décider, présuppose la fondamentale disibilité du monde. Cf. Claude Humbert, l'idée de disibilité du monde issue du monde grec suppose l'impossible alliance entre : logique formelle, et calcul. Thèse que le langage permet de dire le monde, et positions telles que celles de Husserl, etc. qui consiste à dire que va pouvoir représenter le sens commun associé à un formalisme qui lui n'a rien à voir avec notre représentation du monde. Très ambitieux mais ne marche par toujours. Claude Humbert parle de "mirage", "décision computable", "terre promise des inférences effectives et les sciences exactes".
De fait, c'est un peu plus compliqué que cela. Et les premiers à s'en apercevoir ceux qui ont commencé à mettre au point ces systèmes. Le Frame problem, dans quel contexte les choses sont pertinentes ou non.

Ce rapport entre structure et ... bien le but de RDF. Dont l'acronyme porte l'ambition de décrire des ressources tout en rendant possible le fait de faire des inférences.  
Pour autant, il ne faut pas oublier que le web sémantique ne saurait se borner à réaffirmer indéfiniment les notions de l'IA.
Articulation entre web et sémantique
Il faut donc revenir sur ce qui a changé pour comprendre les modifications fondamentales au coeur de son projet.


#### Quelques principes de l'architecture du web

Quelle sont les spécificités du web, qui déterminent la manière dont l'information s'agence à travers le web. C'est lui qui va transformer l'approche.

Ce qui a changé avec le web, c'est qu'il s'agit d'une architecture très spécifique.
Permet d'expliciter comment passe du web sémantique à l'idée du web de données (reconceptualisation qui date de 2006)

L'architecture du web repose avant tout sur un système de nommage extrêmement normatif. Identifiants universels ou uniformes URI, URN, URL. Question de savoir ce qui est identifié sur le web, standards qui ont connu énormément d'itération avant d'arriver à maturité, en veut pour preuve la succession d'acronymes. Cette question n'aller pas du tout de soi, et la réponse apportée a consisté à resséparer les choses entre URN et URL. Sauf que voulait pouvoir tout nommer sur le web, y compris les choses n'y étant pas comme Tim Berners Lee.

Un système de nomade a donc été utilisé pour désigner n'importe quel de données. Ici que retrouve l'idée de l'ontologie comme théorie de l'objet. C'est ainsi qu'a été conceptualisé la notion de ressource. Roy Fielding à l'origine du style architecturesque à la demande de Tim Berners Lee lui qui rethéorisant les concepts d'architecture, réélaborant également ses grands standards (éditeur principal URI, président Apache, etc.)

Ce qui est identifié sur le web : pas un document, pas une page, mais une unité beaucoup plus abstraite qui est une ressource. Et la manière d'identifier cette ressource, c'est une ombre, simplement la manière d'identifier les choses. La manière de représenter quelque chose, un principe de coordination, en accord avec une ressource. Or cette question de maintient dans le temps, ou de fidélité, qui présente un coût.

Deuxièmé élément important : cette ressource n'est jamais explicitement identifiée. Il y a eu des tentatives de faire des définitions d'URI mais jamais passé. La ressource n'est jamais explicitée en tant que telle. Ce qui veut dire que ne peut savoir avant à quoi va accéder (ex page météo).

Le web n'est pas un hypertexte. Quand on regarde les stds c'est très clair. Si cherche dans la théorie de l'hypertexte à dire ce qu'est le web alors pas un bon hypertexte car pas de mémoire, etc. Serait sans doute plus judicieux d'envisager le web autrement qu'un hypertexte.
Au départ pour Tim Berners Lee idée de faire un hypertexte connecté, donc ouvert alors que les hypertextes traditionnellement fermés. L'erreur 404 de ce fait devient un élément important pour comprendre le caractère non hiérarchique et non fermé??? du web.

Un lien hypertexte une URI dans une balise, mais pas un lien mais un pointeur. Quelque chose qui va pointer vers une ressource qui sera éventuellement servie par des représentation. un lien hypertexte est donc un pointeur dans la représentation d'une ressource qui pointe vers une autre ressource qui éventuellement va devoir donner lieu à une nouvelle représentation, mais sans garantie.

Là que Google qui annule la variabilité en traitant la représentation comme une page, la traitant dans son index comme une page et la liant dans son index. Pour cela que devient un lien, car stocke les pages dans son index. La raison pour laquelle Google décourage l'utilisation d'éléments fondamentaux de l'architecture du web telle que la négociation de contenu qui permet d'adapter la représentation de la ressource selon le contexte. Déconseille son utilisation car casserait son pagerank, or l'économie qu'il bâtit a besoin de stabilité. Cela au prix d'un ingénierie majeur.

Le web dans cette perspective, d'abord une plate-forme de publication où va publier énormément de contenus et en maintenir l'accès.
Ce détour par l'architecture du web permet de mieux comprendre les disruption introduites par le web sémantique.


Discussion :
Aurélien Berra : Question du multilinguisme.
AM : technologies héritières de l'intelligence artificielle, porteuse d'universalité. A voulu faire une ontologie générale pour tout le monde. Or cela n'a jamais été le cas du web sémantique. Mais c'est google qui fait cette ontologie universelle avec Schema.org.
Question de la diversité linguistique qu'essaye de porter via les projets comme ceux dont Thibault va parler, mais cette manière de faire ne va pas de soi. D'autres acteurs avec d'autres manières de faire, d'autres réflexes. Cf. agencement aussi étonnant que celui de DBpédia, pas certain que le multilinguisme et sensibilité des sciences sociales soit au coeur des préoccupations de l'INRIA. Oblige donc à créer de nouveaux espaces. Pour cela que très heureux que la communauté des DH s'intéresse aujourd'hui aux DH. Car jusqu'à présent assez regrettable que se cantonne XML, et bases de données. Pense qu'il y a ici matière à faire autre chose.



Voir comment en essayant de répondre à des questions assez proches, avec des moyens presque identiques sur le papier, on a abouti à quelques chose de diamétralement distinct entre l'IA et le web de données.

Ce que change le web à l'IA
On l'a vu, une des difficultés qu'avait rencontré l'iA logique tenait à la nécessité de formaliser le sens commun pour ne pas avoir des machines imbéciles. Cf. la fille de Lord Byron qui dit qu'une machine ne fait que ce qu'on lui dit.
La réponse à la difficulté des systèmes experts a été de formaliser le sens commun, formaliser les connaissances. Heiss a écrit des textes théoriques sur le sujet comme le _Naïve texto manifesto_ cherchant à formaliser la physique du monde. Cela a ensuite trouvé un débouché avec le travail mené par la suite avec RDF.

Parallèlement projet SYG par des chercheurs dans le domaine de l'IA. D'abord un projet visant à produire une base de connaissances pour pouvoir constituer des systèmes experts. L'idée de bourrer une base de connaissances du sens commun en questions pour rendre les réponses des machines pertinentes. Au départ financé sur fonds publics et militaires. Depuis devenu projet privé [CYC Corp](). En partie développé à la hussarde, le projet a rencontré plusieurs difficultés.
SYC signifie encyclopédie. L'application première du web de données a donc aussi été une encyclopédie. Intéressant de faire parallèle projet de Wikipédia, et premier projet. Par ailleurs travail de reconceptualisation dont l'application vedette du web sémantique a été une encyclopédie. Permet rapprochement saisissant mais aussi de distinguer l'approche web de données et IA.

CYC distinguait 
Accent sur les inférences. Ce qui change avec web de données, c'est qu'il s'agit non pas d'une base de connaissance mais d'un médium. Publication des données qui change leur statut.
Autre gradation dans la distinction : la définition donnée par chaque projet. Idée de CYC de faire sortir les connaissances factuelles du monde réel. Pour savoir ce qu'est le monde réel aller voir CYC ou la base de la CIA.
L'idée de regrouper le savoir du sens commun nécessaire pour pouvoir lire l'encyclopédie elle-même.

Sur wikipédia questions très différentes, question des sources du savoir, variabilité des sources, question de l'autorité, de la controverse qui fait que ne va pas uniquement rencontrer du factuel sur wikipédia. On peut sans doute faire plus pour rendre sur wikipédia plus évident cette controverse mais en tous les cas une idée qui n'était pas présente dans la base CYC.
Par ailleurs une théorie propre à Wikipédia de traiter les sources. S'appuie sur des sources secondaires. Or quand de nouvelles recherches deviennent des sources primaires et perdent leur nature factuelle au fil du temps.

Ceux qui continuent aujourd'hui d'avoir une approche s'appuyant strictement sur du factuel : c'est Google. Cf. les interviews de ceux qui constituent le Knowledge graph expliquant que tout ce qui n'est pas factuel est supprimé. Alors que notre idée serait plutôt de montrer ce qui n'est pas factuel et faisant controverse.

Manière dont ce stock discrétisé de propositions formalisées, étiquettes indifférentes si pas publié. Approche du web semble porter même approche. Sens défini par les inférences que l'on peut faire sur ces données. Mais dans ce cas les auteurs reconnaissent qu'ils s'agit d'une simplification. Les débats ont été nombreux au W3C pour comprendre comment les URI signifient. Question de la signification sociale qui ne trouve pas de traduction dans les standards, mais qui est une de leur faiblesse reconnue.
On parle de signification sociale pour rendre compte du fait que les URI sont déréférençable c'est à dire qu'elles donnent accès à des contenus. Ces URIs par conséquent donnent accès à des contenus éminemment variables qu'il faut prendre en compte pour leur donner signification. 
[Pat Hayes](http://www.ihmc.us/groups/phayes/) défendait une approche logistique avant d'évoluer considérablement en 2009 avec ce qu'il appelait la blogique (blogic), une logique adaptée au web qui prend en compte l'existence de l'architecture du web, tirant partie des spécificités du web.
Le fait de parler non pas de pointeurs mais d'URI, constitue le principal moteur de l'évolution qui permet de passer de la notion de web sémantique à la notion de web de données. Et ce passage reste encore largement impensé dans les stds du web.


### Conclusion

Claude Humbert ?? expliquait qu'il fallait désormais apprendre en philo à changer les prises (idée que la logique pouvait dire le monde). Ce que voit ici, le web avec tts ses spéci s'interposant entre la logique formelle et la connaissance de tout le jour. C'est par le web que appréhendons les ressources, et que constituons un horizon commun (cf. Wikipédia, consensus, controverse), qui nous permet de faire ce travail d'objectivation en commun.
Ainsi les conceptions en jeu ont considérablement évoluées et le fait que les anciennes questions comme celles du traitement automatique des langues, les réponses apportées et les applications qui demeurent aujourd'hui sont en réalité de nature très différente. Qui aurait imaginé qu'un trombinoscope géant avec web sémantique... pour autant un des scénarios du web sémantique. De la même manière si avait jugé que changer la propriété intellectuelle aurait donné ce que donne Google avec CC. 
Nouveaux stds comme webid
nbx modèles qui restent à imaginer, et nbx scénarios qui n'ont pas encore été pensé dans le monde économique. Ne pas refaire ce qu'a fait dans le domaine de l'ingénierie des connaissances et de l'intelligence artificielle.


### Discussion

Demande d'éclaircissement sur la notion d'"interobjectivisation"
AM : un concept issus de [Joëlle Zask](http://joelle.zask.over-blog.com) (courant pragmatique). idées de faire penser de nouvelles épreuves au savoir scientifique, etc.
Pour moi le rôle que joue Wikipédia à l'échelle du web autre chose qu'une encyclopédie. Sur le web architecture qui permet à tous de publier n'importe quoi sur n'importe quoi. Pas d'opérateur de vérité, le seul opérateur celui de la confiance. Chacun peut dire ce qu'il veut, identifier ce qu'il veut et publier ses propres ressources. Pourquoi tout le monde se connecte à DBpédia, c'est que parle de tout et suit règles de production qui font consensus. Donc à la fois central pour le web.

Lou Burnard
Vous avez dit dans une phrase concernant les efforts malins de Google, vous indiquez qu'une correspondance factuelle et consensuelle.
AM pas une correspondance que je fait moi, une correspondance faite par le projet CYC. Là où n'utilise pas cette notion, c'est que sur wikipédia un procès en cours, pas un 

Emmanuel
DBpédia encore une collection de faits. Faire sortir les connaissances et aussi les controverses qui ont servi à les produire. Au médialab de Sciences-Po des gens qui travaillent sur l'étude des controverses.
Question de la provenance et de la confiance.
Standards du web 


Thibault Grouas
------------------

S'occupe d'une mission intitulée "langue et numérique", articulation  entre les langues et le numérique. Comment mieux prendre en compte le numérique dans la problématique des langues.
DGLFL Délégation générale à la langue française et aux langues de France. 75 langues reconnues comme langues de France (locales, issues de l'immigration, ou du territoire comme le Yiddish ou la langue des signes).

Favoriser la diversité interne
Favoriser la diversité des langues employées au niveau internationale
Umberto Ecco avait dit "la langue de l'Europe c'est la traduction". Dans un monde mondialiser, trouver de nouveaux outils pour rompre les barrières linguistiques. La langue de l'Europe se serait peut-être même le multilinguisme. C'est tout l'enjeu du travail que nous menons sur le web de données.

S'intéressent de manière générale à toutes les technologies classiques de la langue :
 - TAL
 - Web 2.0 (ex : WikiLF pour la terminologie)
Attention particulière accordée à la participation.
Partenaire de Wikipédia France
Travaillent de puis quelques années de manière proche avec eux. Nombreuses langues de France présentes sur Wikipédia dont aimerait également favoriser la visibilité.

Enjeu considérable du web de données pour favoriser le multilinguisme
Possibilité d'envisager des interfaces multilingues et donc favoriser interrogation multilingue. DBpédia, une extraction de Wikipédia.

Exemple requête baroque et diversité des résultats sur mot-clef avec le même sens.
Par ailleurs fournir accès à des ressources francophones dans d'autres langues.

SemanticPedia une convention de partenariat signée au printemps entre INRIA, Ministère de la Culture et Wikipédia. Associer trois acteurs issus d'horizon différents. Expertise propre à chacun des acteurs, en associant ces trois partenaires, mutualiser nos expériences, et usages innovants.
Un précédent avant 2012 qui nous a mis sur la voie, le projet Histoire des arts HDALab qui était une "preuve de concept", visait à démontrer la faisabilité d'une technologie sur un projet de petite taille.
Extraction de wikipédia, mise dans un entrepôt et possibilité de les interroger avec des requêtes SPARQL. Premier projet avec HDALab.
Idée avec DBPédia francophone de mettre en avant langue française. Intérêt fort en matière de structuration, etc. Travail sur le système de gestion d'identifiant pérenne qui n'existe pas avec .org

[Izipédia](http://izipedia.com), un moteur intelligent de réponses aux questions basé sur DBpédia.
Projet hébergé ailleurs que sur les serveurs du ministère de la Culture = plus légers

[Projet Muséophile](http://museophile.fr/apropos?language=fr) : projet proposant la suggestion de musées sur la base de critères personnels. Projet dépourvu de base de données qui utilise uniquement les données disponibles sur le web de données. Produit disponible en 9 langues. Réalisé en 8 mois. Valide l'intérêt d'une approche multilingue sur les concepts du web de données pour les aspects multilingue. Plusieurs lourdeurs identifiées quant aux requêtes Sparql qui impliquaient des unions relativement lourde, implique le besoin de mise en cache pour ce type de projet. Par ailleurs identifié la nécessité de faire travailler les petits musées sur la mise à disposition d'infobox.

[JocondeLab](http://cblog.culture.fr/projet/2013/07/11/joconde-lab) projet qui s'appuie sur la base Joconde, l'une des bases les plus anciennes du ministère. Grande diversité de contenu, 300 000 notices. Alignement des entrées qui décrives ces notices avec DBPédia. Travail avec l'IRI sur ce projet pour proposer un accès à toutes ces images en 14 langues (sortie fin janvier) dont 4 langues de France.
Un projet expérimental qui a pour but de faire évoluer nos dispositifs.
Responsive design, possibilité de changer les langues
Entrée par mots-clefs, entrée cartographique, entrée par artistes
La seule traduction nécessaire concerne l'interface. Possibilité de proposer du multilinguisme sans traduire.
Expérimentation de la contribution pour apporter des folksonomies.

Un programme de sémantisation sur trois ans. Donc plusieurs projets à venir. Une des pistes consiste à travailler sur la parole, langue des signes pour l'intégrer dans un contexte de web de données (à partir de 25 000 vidéos). Un projet sur le wiktionnaire, et sur la controverse.

Voir [lien isni](http://www.isni.org)
Pas de lien isni dans le projet

Lié 5 des thesaurus au service des musées.
Travail de 3 mois à 2,5 personnes, relié 30 000 entrées de thesaurus avec wikipédia.
L'équipe de Laurent Manœuvre qui a fait ce travail.

Quel format pour les thesaurus ?
Pas la norme Skos utilisée (une ontologie pour faire des thesaurus). Ici utilisation de DBPédia pour se greffer sur des existants. Un autre travail en cours qui consiste à traduire en SKOS les thesaurus du ministère.

Discussion
-------------

Existe-t-il des travaux liés au text mining parmi les technologies linguistiques évoquées avec DBpédia et qui viseraient à intensifier la triplification.
AM : oui, beaucoup de pollenisation. Par exemple [DBPédia spotlight](https://github.com/dbpedia-spotlight) qui consiste à extraire des entités nommées à partir de DBPédia pour faire des enrichissements.

JocondeLab qui devrait alimenter des produits interne pour diffuser l'information provenant de ces bases de données.
Insiste beaucoup sur le bilan des projets en invitant les différents participants pour retrouver la diversité de points de vue en cours de projet.

En savoir plus sur le projet concernant la controverse.
AM : Normalement prévu dans le programme sémantisation. Beaucoup de données d'activité qui n'apparaissent pas forcément dans DBPédia et qui sont disponibles sur wikipédia. Pas d'infobox partout, mais par delà des contenus peu accessibles pour lesquels peut utiliser des outils de text mining, mais encore d'autres données disponibles auxquelles voudrait s'attaquer et capter pour comprendre les processus en jeux.

Grouas : Intéressés aussi pour identifier les éléments en œuvre dans la traduction. Identifier les parties ajoutées, retirées, etc. Analyser les différences culturelles qui s'expriment ici, à quels endroits.

AM : Faire travailler des étudiants sur la production d'infobox très intéressant car ce faisant se posent énormément de questions et intéressant à étudier pour comprendre la dynamique de la controverse.
400 000 articles sans lien interlangue, soit 1/3 du corpus. Le travail sur DBPédia a permis de le corriger

Grouas : Intéressant de travailler aussi sur des démarches contributives et collaborations, intégrer les enjeux de corrections de l'information, etc. Outils automatiques de liages qui permettent également d'identifier les trous, et de les compléter.


Loma Hugues, animatrice NEDIMAH
-------------------

Anime un réseau d'experts
Question de la production finale d'un tel réseau, une fois que les experts ont mutualisé sur les méthodes et pour les participer comment partager le résultat de ce travail : cela sera sous la forme d'une ontologie.

Building an ontology of Digital Methods un the Humanities
Revenir un peu en arrière concernant la discussion de ce matin sur l'étendue des digital humanities. Extrapoler sur la définition des DH puis introduire une tentative de formalisation de cette définition sous la forme d'une ontologie actuellement développée dans le cadre de NEDIMAH.

Qu'est-ce que les DH ?
Quand dit que fait des DH nous demande souvent qu'est-ce que les DH. L'utilisation effective de méthodes et d'outils numérique qui introduisent des disruption dans la manière de faire de la recherche traditionnellement.
Une nouvelle manière de travailler et de produire la recherche. Deux éléments clefs : de nouvelles manières de communiquer et d'échanger en développant un espace partagé de travail. Possibilité de faire de la recherche plus rapidement, plus efficacement et de manière collaborative.
Enfin rend possible de traiter des choses que ne pouvait faire précédemment, corpus textuels, etc. mais aussi de traiter de nouvelles questions de recherche à partir des contenus.
DH connaissent un vrai intérêt, trendy, financements. Très bien, mais les DHs quelque chose qui peuvent ajouter de la valeur à ce que nous faisons déjà, à nos recherches, et nos collections patrimoniales.
Un des aspects importants des DHs est qu'il s'agit de collaborer avec des partenaires avec lesquelles n'a pas habituellement l'habitude de travailler et qui introduisent des méthodes de travail différentes.

Diruption
Trois aspects : Digital content, digital methods et digital tools
Le digital content ce avec quoi on travaille, les artefacts numériques avec lesquels travaille. Un élément clef de l'espace de travail des DHs
Méthodes : les scholarly primitives : annoter, représenter, commenter, etc. qui peuvent toutes être réalisées numériquement.
Enfin les outils qui sont les logiciels qui rendent possible ces opérations ou facilitent l'utilisation du matériel numérisé avec des méthodes digitales.

Méthodes qui nous permettent de faire des choses comme l'analyse d'image, analyse quantitative ou qualitative d'images. Points d'intersection des disciplines, ou des méthodes, qui font des DHs un intéressant pour travailler. Para ailleurs des recherche guidées par les données, travaille avec des grands ensemble de données comme jamais pu le faire auparavant.

Pour illustrer ces idées et ces méthodes, voudrait évoquer un certain nombre de projets auxquels participe à la NLW
exemplifier les "methodological commons"

[Welsh Newspapers online](www.welshnewspapers.llgc.org.uk)
Une ressource numérique simple mais qui mobilise beaucoup de connaissances académiques.
Données utilisées pour être comparées avec d'autres données européennes.

[DMS Interoperability : Divitally Enabled Scholarship with Medieval Manuscripts]
Analyse photospectrale que n'aurait pu mettre en œuvre sans ces méthodes.
Méthodes d'annotation de manuscrits.

[The snows of yesteryear : narrating extreme weather]
Consiste à travailler sur des sources sur des événements climatiques majeurs avant l'instrumentation.
Capture du processus de recherche pour travailler sur des sources hétérogènes dans une approche collective.

[The Cult of Saints in Wales : Medieval Welsh-language sources and their transmission]
Travail avec le King's College pour créer une plate-forme d'édition critique où les personnes puissent annoter, comparer, etc.

[Wales1900 launched oct 2013 !]
Un projet de crowdsourcing. Ici encore comment capturer le processus et la manière dont il altère notre manière de faire.

[en développement un projet d'annotation collaborative pour transcription collection large]
Interface de transcription avec manuscrits


Pour tenter de définir ce que peuvent être les DHs, première tentative de Willard McCarty, plusieurs itération pour tenter de saisir la complexité du paysage des DHs et de son paysage.

[methodological commons](http://www.makinghistorypodcast.com/wp-content/uploads/2010/02/methodologicalcommons-simplified.jpg)
Plusieurs tentatives pour formaliser ces méthodes
2003, développement d'une taxonomie des méthodes pour les arts et humanités.
Dévelopé encore en 2007 dans un projet ICT :
http://digital.humanities.ox.ac.uk/methods/ict-methodology.aspx

Mais encore quelquechose d'assez plat
Dans le cadre de NeDiMAH : développement nombreuses méthodes.
Documenter les activités et les pratiques des participants.
Main input ontologies des méthodes digitales en DH

Plusieurs groupes méthodologiques pour aborder les domaines clefs de la méthodologie.
[ICT Methods Taxonomy Working Group](http://www.nedimah.eu/workgroups/development-ict-methods-taxonomy)
que dirige avec nombreuses autres personnes. Décidé de construire une ontologie qui sera l'opportunité de construire à partir de ce qui a été fait précédemment.
Idée de mener ce travail dans le cadre du VCC2 de DARIAH.

!! Objectifs : disposer d'éléments pour évaluer et capturer les projets menés. Une manière à partir de la description des projets de documenter et d'évaluer leur qualité.
Université d'Athènes construira l'ontologie. Nedimah finance.
Incluera des définitions formelles des entités, classification, approches théoriques, techniques de recherche, types d'acteurs et environnements. Aspects critiques des méthodes, et outils ...
Un schéma RDF pour être une ressource ouverte et lisible par la machine.
Souhaite que soit compilant avec les standard, interopérable. Travaille en proximité avec CIDOC-CRM, et le web service devrait permettre au chercheur d'accéder à l'ontologie et de proposer plusieurs méthodes de travail avec elle. Idée de maintient sur le long terme par DARIAH, pour être utile une ontologie qui a besoin d'être vivante et d'être concrètement utilisée.

Discussion : 
Développer des vocabulaires contrôlés devrait faire partie d'un tel travail. Certainement un travail à conduire, mais un élément clef ici consisterait à faire participer la communauté.

Giovanni : Une place ausein de DARIAH pour travailler sur le sujet. Suffit de prendre contact avec lui.


Rodolphe Bailly
--------------

Directeur du système d'information et de la numérisation à la Cité de la musique rattachée au département de la documentation.
Va parler d'un projet intéressant qui nécessite de l'interopérabilité.

[déjà entendu à la journée Athena, comme la présentation sur DBpédia. Est-ce à dire que peu de projet à faire valoir en sciences humaines]

Initialement, un projet de numérisation d'instruments de musique. 2005-2008 lorsque le projet Europeana commençait à émerger. Un projet déposé dans le cadre du programme eContent+ en 2008. Budget total de 3 millions d'euros dont 2 financés par la Communauté européenne. Aujourd'hui projet terminé.
Au démarrage, 11 grandes institutions, aujourd'hui une 20aine de collections.
50 000 objets décrits dans la base. 6 langues utilisées pour décrire les instruments. En plus du multilinguisme les différentes bases de données ont toutes des nouveaux de description différents et des modèles différentes. Encore jusqu'à il y a peu, il n'existait pas de std pour décrire les objets muséaux, ceux-ci étant par définition souvent uniques.

Problématique du projet :
Multiplicité des sources qui provoque des problèmes d'indexation, en outre pour construire des interfaces d'accès modernes à facettes pose de gros problèmes. La solution consiste à homogénéiser les données en utilisant des référentiels.

Cela a été fait sur trois axes dans le projet MIMO :
 - constitution thesaurus nom
 - utilisation nomenclature de noms de lieux (geonames) 
 - construction d'une liste d'autorité des facteurs d'instruments

Thesaurus limité à trois niveaux pour faciliter interface de recherche :
familles d'instruments, grouse et classes.
Une information de contextualisation des termes qu'il convient de conserver à tous les niveaux (celle de la langue où l'on est).
Souvent termes usuels qui n'étaient pas toujours présents dans les 6 langues. Pour des raisons de structure interne, voulu déterminer une langue pivot.

Outil de management des thesaurus.
un responsable par langue de la maintenance du thesaurus. C'est elles qui centralisent les demandes d'ajout. 4 000 termes.

Infrastructure technique qui comprend au centre un agrégateur de données qui reverse dans Européana. Chaque musée chargé d'utiliser le format d'échange LIDO. 
Moissonnage des entrepôts au moyen de OAI-PMH
Passe par une phase d'enrichissement des données, ici que récupère le mot-clef et que fait un lien avec le vocabulaire. Dit qu'enrichit la données car à partir de ce lien fait pour les facteurs, la classe d'instrument et les lieux qu'est capable de proposer du multilinguisme.
Se retrouve ici avec un URI dans la notice LIDO, distribue les données en RDF dans européeana en utilisant skos pour faire passer ces données à l'extérieur.

Exposition en (linked)open data
Toujours pas liés à d'autres référentiels.
http://www.mimo-db.eu/InstrumentsKeywords/2232
qui conduit si humain sur une description, si requête 303 en RDF.

Mimo-DB ouverte à tous, l'interface de recherche dans le projet.
Interface dans europeana où retrouve les URI. Peut ensuite l'utiliser pour fabriquer l'index, pour présenter les résultats dans la langue de consultation, etc. C'est dans ce cadre que travaille avec Europeana.

Projets encours
COST sur les instruments en bois, etc.
Plusieurs perspectives. Projet qui s'est arrêté en 2011. Début 2014, ouverture d'un nouveau site web car s'est aperçu que n'était pas suffisamment visibles (30 millions objets dans europeana) et par ailleurs envie de dépasser le cadre européen. 40% du patrimoine musical européen représente 40 000 instruments, seulement 16% au niveau mondial. Mais un monde fini qui permet de construire des interfaces très propres.
Deux nouvelles langues qui vont être ajoutées au projet avec le musée de la musique de Barcelone à leur charge de traduire le thesaurus en espagnol et en catalon.
Musées africains, et Shanghai ?
Alignement du thesaurus avec DBpedia envisagé.

Interface du nouveau site.
Moteur de recherche
Liste de résultats avec images et thesaurus à gauche proposant des facettes. le même thesaurus est utiliser pour naviguer dans la collection rapidement par choix croisés. Rebonds possibles sur les termes. Pour le moment pas d'ordre de pertinence dans l'affichage des résultats. Des quantités relativement réduite d'objet, avec une telle interface accède assez vite à ce que l'on recherche.


###  Discussion
Katell : Discussion sur le multilinguisme
Y-a-t'il toujours une équivalence ? Si l'instrument de musique n'existe pas dans la langue concernée ? un trou ?
R : on a décidé de tout ramener à une langue pivot ce qui implique que chaque instrument soit traduit dans une langue. Comme parle d'objets, doit bien pouvoir généralement les désigner.
Katell : si africains rejoignent, risque d'avoir ce problème.
R : L'idée, si le terme n'existe pas dans la langue de réutiliser le terme pivot qui est a priori celui le plus souvent utilisé pour désigner l'instrument. 
Cas où les référentiels commencent à compléter les langues par nécessité !

Archimède pour la gestion de contenus. Produit de gestion de thesaurus modernisé pour avec eux pour travailler sur la gestion du thesaurus et le travail à distance.
Demande sur l'utilisation d'Eurovoc ?

Question, pourquoi ne pas être parti de DBpédia pour tout traduire ?
R : traduction après les deux premières langues qui a été assez rapide. Il n'est pas certain que cela aille plus vite avec DBpédia pourrait accélérer les choses mais pourrait aussi donner lieu à discussion sans assurance de succès. Dans le cas présent évidemment où ne dispose que de 4 000 termes, et pour lesquelles beaucoup de termes très spécifiques sans traduction.

Question : Les musées ont des objets auxquels donnent des noms, vous avez créé un cadre pour gérer des dénominations dans toutes les langues. Est-ce qu'à un moment vous avez pensé à un cadre pour décrire les propriétés des objets afin de pouvoir préempter les nouveaux objets que créée tous les jours encore.
R : Reste dans le domaine du patrimoine des musées. S'il y a des synthétiseur au musée de la musique, pas vraiment l'IRCAM. Je n'ai pas montré de fiches descriptives d'instruments de musique de la Cité, certaines assez fournies avec des caractéristiques techniques notamment concernant la manière de produire le son. Ici concentré sur la dénomination dans l'idée de produire un instrument de recherche pour le grand public.



Isabelle Donze : GeoEthno : retour d'expérience sur le développement d’un thesaurus de noms géographiques
---------------------

http://www.mae.u-paris10.fr/dbtw-wpd/bibliotheque/ObjetGeoEthno/ontologies/ObjetGeoEthno___-1067383778.html

UMR Nanterre
SIGB Koha à la bibliothèque d'anthropologie.
40 000 notices, 180 000 notices réseau ethno
Consortium archives des ethnologues

Notices enrichies par des mots-clefs en bleu
Pas d'utilisation de Rameau
Initialement pas de thesaurus ou d'outil de gestion des vocabulaires. A donc développé un outil de gestion des vocabulaires.

Application lancée en 2001, bénéfice de l'expertise d'un ingénieur géologue spécialiste de SGML, aide dans la crétion d'une DTD. Acquisition en 2003 d'une base de données documentaire BDText
Interface interrogation web
2006 Export réseau Ethno AGH = développement d'un export
2007 participation à la base Termscience de l'inist
2009 alimentation de la base Télémeta du CREM une base qui gère des archives sonores
2011-2012 des changements dans la normalisation des thesaurus = adaptation pour prendre en compte ces nouvelles normes ISO. Permis export dans la plateforme Isidore de la TGIR huma-num.
Un processus très artisanal car seule à gérer le thesaurus.

### Les contextes d'utilisation du référentiel
Une application déjà ancienne. Changement des contextes d'utilisation.
2001 indexation manuelle des catalogues de bibliothèques d'ethnologies
Ensuite apparu un nouveau contexte qui est celui des données de la recherche, le LESC s'est spécialisé dans le domaine des archives d'ethnologues. Par ailleurs des archives sonores d'ethnomusicologie. Des documents très divers et hétérogènes allant des documents papiers aux enregistrements. Il a donc fallu voir s'il était possible d'adapter ce thesaurus pour prendre en compte des données de terrain (microthesaurus géographiques, etc.).
Le contexte n° 3 celui de la plateforme Isidore pour faire de l'indexation automatique sur le texte intégral de documents.

### Périmètre et couverture du référentiel
Il s'agit de toponymes dans le domaine de l'ethnologie.
Ni un SIG avec cartographie, ni une base de données officielles. Il s'agit d'un langage documentaire qui permet d'indexer des documents et parfois les descripteurs retenus ne sont pas la forme officiellement retenue.
Géographie physique (fleuves, montagnes), humaine (villes, sites culturels, etc.), historique

### La normalisation
Au départ parti dans un univers SGML. Environnement XML natif (base de données XML native). Donc dans un univers de normalisation conforme à ces formats. Normes ISO SGML et XML. Normes Codes de la représentation des noms de pays et de leurs subdivision ISO 3166-1:1997 et ISO-639-2
Normes thésaurus et interopérabilité
Unicode

Std du W3C
XML et HTML
XSL
RDF/SKOS

### Le système référentiel

Le référentiel est constamment lié à la base de donnée avec laquelle il travaille. Soit directement intégré dans la base de ondées (c'est le cas pour les collègues archéologues avec Frantiq et Pactols). Ici un thesaurus autonome qui est déconnecté de Koha.

Trois grandes parties
Une partie production, accès et communication.
DTD SGML, parseur pour contrôle qualité (analyse syntaxique) --> produit un fichier XML valide qui sert de format pivot.
L'application principale est l'accès web, DB/text l'application de base de données documentaire, format propriétaire. Base qui n'est pas en XML, utilisation de XSL.
Partie communication : systèmes externes : XML/TMF, Télémeta, Isidore


### Chaîne de traitement
Utilisation de BaseX pour l'analyse de fichier
Interfaces DB/text
Communication SKOS en XML

### Aperçu des bases externes

#### Base termscience de l'INIST
Une fusion d'arborescence, à titre d'expérimentation il y a quelques années.
Vocabulaires inist, et géoethno. 

Telemeta
Archves sonores, 25 000 enregistrements d'ethnomusicologie.
Sonogrammes. Métadonnées très riches dont un champ géographique. Par ailleurs croisé avec génonames pour la géolocalisation.
Plateforme Isidore d'humanum. Moissonne des ressources en texte intégral dans les grands réservoirs.

### La gestion des référentiels

#### Les mises à jour du référentiel
annuellement, mises à jour événementielles en fonction de l'actualité.
Suivi de documentation officielle, newsletter ONU, création d'un nouveau pays comme le Soudan du Sud, réduction échelon administratif en Grèce.
Question des translittérations.

Enrichissement au fil de l'eau
Mises à jour technologiques "changement de version", normalisation, modélisation pour s'adapter aux évolutions techniques ou de normalisation.

Notions de statut, datation, versionning qu'introduit peu à peu dans le système


### La structuration des données du référentiels
Un thesaurus qui est une arborescence qui va permettre de créer des hiérarchies. Thesaurus qui s'aligne sur la nomenclature internationale qui regroupe les régions par grandes régions. Maintenu par organisme internationale = neutralité et bonne couverture, normalisée, bilingue et alignée sur ISO codes pays ISO 3166-1. Entrées niveau pays pour certains DOM-TOM qui augmente leur visibilité, plus facile à manier pour l'ethno intéressant.
inconvénients : éparpillement des DOM-TOM, zones de découpages (macrorégions). Selon disciplines pas les mêmes regroupements.

Représentation carto des niveaux pays (niveau 1)
Subdivisions de pays en bleu en revanche (hawaï, île de Pâques)

### Les relations dans le thesaurus
Plusieurs types de relations possibles même si primaires par rapport à une ontologie

Relations hiérarchiques (hiérarchiques et équivalence)
Niveau Pays : relations associatives...

### Le multilinguisme
Nouvelle norme ISO 25964 orientée multilinguisme
Descripteur qui devient un terme préférentiel
1 concept, des termes préférentiels dans différentes langues et des termes alternatifs
Synonymes, termes non retenus, etc.

Bascule dans Isidore
Multilinguisme
Aspects historiques de la langue.
Un système de graphe, multiplication des liens et des relations qui compliquent le système.

Typologie des incohérence lors de l'indexation automatique
Nom géo --> Noms de personnes
Nom géo --> Noms d'animaux : Rennes, Tigre, Wolf, Moustique, etc.
Nom géo --> Mots vides: Una, Our, Die, Yet, Lot, Cher, Main [pb multilinguisme]
Nom géo --> Noms communs : Colonia, Trente, Sens, Tal, Unité, Entrance, Intérieur, Réunion, Islande, Glorieuse
Relations indirectes : par l'intermédiaire des traductions :
Points cardinal : Western, Eastern Beaucoup de noms de région avec des noms d'orientation. Parasitage de l'affichage.
Ex Danube, Tunus (tr), tunes (en), thon (fr) au travers de la chaîne de traduction.


Discussion
--------------

Querelle des universaux, une vieille controverse
Dans les différents exposés donne l'impression d'être en plein nominaliste. Et peut-être laissé de côté avec les ontologies lourdes le réel. Est-ce une tendance forte, y-a-t'il un rééquilibrage possible ?

AM : En effet dans le cadre du web sémantique et du web de données, tendance à faire des ontologies légères, car les vocabulaires mis en avant ou les ontologies qui ont pour le moment fonctionné, de petits vocabulaires qui ont bien fonctionné.
En fait ontologies lourdes pas forcément immédiatement traduits dans le web sémantique par ailleurs, pour publier ces données souvent eu besoin de petites ontologies. Même si à l'origine les scénarios d'application du web sémantique dans le domaine médicale, des ontologies lourdes. Certainement une forme de nominaliste en partie combattu par les personnes qui font des grosses ontologies et qui sont eux des réalistes.
Mais en effet ce débat là existe. François Rastier qui dit que le nominalisme a un effet abrasif sur l'ontologie.

Lou : Pour reprendre ces termes, je dois dire que je suis très content d'être nominaliste en disant que j'appelle telle chose avec telle étiquette. Au moins sait ce que suis en train de faire. Il n'y a rien de réel sauf les noms pour mois.

? Rmq à la journée ABES. Modèle de description bibliographique qui sera bientôt mise en place. Peut très bien avoir description DC de type nominaliste et une description professionnelle par alignements d'ontologies.

Lou : Pour moi le Dublin Core n'est pas utile car il comporte trop d'ambiguité; Que signifie DC corverage ?

Aurélien : Lorsque vous avez commencé votre présentation en parlant des DH vous avez dit qu'elles étaient disruptives, nous cherchons tous à construire ?

Laura : aurait pu dire destructive. Mais idée de rompre ou changer le continuum.
Par certains aspects partie du continuum et le succès des DH fait que plus nécessaire aujourd'hui car ce que les gens font maintenant. Mais pour arriver à cela, besoin de disruption pour que gens quittent leurs habitus, et acquièrent de nouvelles litteracies.
Par exemple partenariat avec les sciences de l'informatique. Demander il y a quelques année de collaborer avec un informaticien pour une base de données pas si évident.

Sans doute un grand nombre de débat, surtout aux EU, au sujet de la formation des étudiants selon la carrière que nous voulons qu'ils aient. Les étudiants doivent-ils par exemple apprendre à coder ? en tous les cas besoin de maîtriser le vocabulaire.

Aurélien : Souvent discussion que n'a pas, qu'est-ce qui n'a pas fonctionné, ce qui ne fonctionne pas. 

Lou : Si j'ose, il me semble qu'il y avait une partie manquante, c'est-à-dire le point de vue des gens qui font ce travail depuis des siècles, c'est à dire celui des bibliothécaires et de la documentation.

Aurélien : nous ne voulions pas choisir un seul terme.

Emmanuel : Très en retard
Bien sûr Isidore, etc. Manque d'investissement massif sur le sujet. Du point de vue disciplinaire définition de catégories, etc.

Alexandre : Bizzarement, ce que j'ai trouvé dans le domaine de la culture, plutôt une continuité. Fait une preuve de concept, puis des projets autour. N'a pas vu cette continuité dans le domaine de la recherche et de l'éducation. Plutôt le sentiment que manque de définition d'une politique à long terme. Sans doute plus matière dans le domaine de la culture à expérimenter car une vitrine. Mais le vrai parent pauvre aujourd'hui c'est la recherche et pas entendu de discours sur l'open data.

LESC : bien sûr dans les IST des descripteurs, mais avec SUdoc beaucoup désastre. Moins nécessaire d'avoir des vocabulaires. Monde de la culture une tradition d'exposer et de documenter ses produits, donc plus de choses disponibles, le monde de la recherche a sans doute moins ces habitudes en tous les cas en sciences humaines en particulier pour les données de la recherche hormis dans l'archéologie et l'ethno.

Alexandre : Pb open archive, etc. Si les programmes web sémantiques avancent dans le domaine de la culture, que plus facile de rendre accessible des métadonnées et des images que des données.
Joconde une base de données de métadonnées d'objets, d'où l'intérêt.

? Bib indexation big num Cujas \ Confrontés même problématique que vous, au départ mots libres mais allait dans tous les sens, or des facettes (plateforme XTF), a donc dû coordonné l'indexation.
Rameau existe, en open data, possible de faire des propositions, alignés sur le LCSH. Ne communique sans doute pas suffisamment ensemble entre bibliothèques et universités. 
Préconisé utilisation de Rameau pour permettre une navigation taxinomique. Corpus qui a évolué en fonction des stds du web, de l'évolution de Rameau et de son adaptation (analyse par chapitre).
Partis de qqch allant dans tous les sens, et fait un effort de normalisation.

L'ANR se préoccupe peu de ce qu'elle produit. Mais commence à aller dans la bonne direction. Une question d'éducation. Réussir à ce que l'information de base soit donnée pour aller chercher des modèles.

Lou : Très frappé par la notion de ressource. Cette idée vient en fait des chercheurs et des scientifiques. Pas créées par les bibliothécaires et le archivistes, ceux qui sont responsables de la création de nouvelles ressources numérisées doivent bien penser à leur utilisation optimale : implique balisage de manière claire et transparente de manière à ce que des moteurs puissent en profiter.






Pourquoi besoin standalone guides de paris
séparer édition / annotation / bases de données
Délocaliser de l'édition ce qui est amené à changer.
Possibilité de prendre en charge plusieurs assertions sur un projet.








