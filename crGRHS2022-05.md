# CR École d’été du GRHS, mai 2022

GRHS Groupe de recherche créé en 2013 pour fédérer les chercheurs et les étudiants travaillant sur la France moderne. Au cours du temps, plusieurs projets ont permis de nourrir l’activité du centre.

L’École d’été du GRHS et du Pireh initiée en 2016, une des activités phares du GRHS. Cette semaine, une opportunité exceptionnelle car des collègues vont vous soutenir et vous accompagner dans une formation intensive. Collègues exceptionnels. Ne vous privez pas de profiter des chercheurs qui sont là avec vous pour vous former. Les éditions précédentes nous ont démontré que les particiapnts qui sortent de la formation, en reviennent réjouis.

## Tour de table

Possible de travailler sur vos propres jeux de données.

Emmanuel Château-Dutier, professeur en muséologie numérique. Je suis historien de l’architecture et je travaille sur l’histoire de l’administration de l’architecture en France.

Formé aux HN à l’École nationale des chartes, j’ai participé à plusieurs projets de constitution de corpus en XML-TEI. 

Engagé sur plusieurs projets de recherche dont deux pourraient faire l’objet d’une exploitation textométrique ou d’analyse du discours

- Analyse du discours architectural : analyse des pvs du Conseil des bâtiments civils
- rapports d’experts du bâtiments à Paris 1690-1790 (ANR + Mission de recherche Justice et droit)

Quelques exp TXM, divers logiciels, programmation.

Remercier de m’avoir accepté dans cette école d’été.

- e
- Gaëtan
- Julien Deriny
- Damien Mayaffre, travaux qui s’intéressent aux observables statistiques. Politologue de vocation. HDR à vocation très méthodologique sur les outils que l’on peut utiliser pour objectiver nos résultats.
- Mathieu Langevin, thèse doctorat, révolution Fr et discours politiques notamment esthétique. Central dans ma thèse.
- Myriam, Mtr sociabilité alimentaires au prisme du genre à l’époque médiévale.
- Édith-Anne, UdeM. Histoire des femmes africaines, au Togo.
- Marie-Lise, postdoc U libre Bruxelles, financée par le CRSH. Spécialisée en histoire coloniale française tardive, post 45. Défense intérêts coloniaux à l’ONU. Question du discours.
- Hugo Trépanier, Mtr histoire et HN. Correspondance du roi Charles VIII, relation entre le roi et ses villes. Épistolaire politique. Outil de pouvoir et de représentation de la monarchie. Magnifique corpus. Lexico 3. AFC.
- Maude Grimenar, Bac Sherbrooke, sept en Mtr Sophie Abelard ? Hôpital des enfants-trouvés à partir de la presse parisienne.
- Isabelle Bruneau, Sherbrooke, lexicométrie. Voir évolution développementale des leaders au niveau du langage. Recherche/action

---

Benjamin Deruelle

On a coutume de dire que les sources sont le pain de l’historien.

Souvent les textes, premier contact avec les éléments du passé, au point que l’on réduit parfois son travail à faire parler les sources. Contexte et critique de source.

L’étude du document en tant qu’objet d’étude apporte énormément de choses à la connaissance historique. Analyse textuelle amène à reconsidérer cette division.

Depuis les années 60/70 avec le renouvellement important des méthodes en histoire, la lexicométrie s’est fait une place en histoire. Il faut bien être conscient que se lancer dans une analyse textuelle par ordinateur, c’est adopter un positionnement historiographique. Il est donc essentiel d’en comprendre l’histoire et les enjeux.

L’activité de l’historien suppose pour commencer un rapport critique à l’écrit qui n’est pas seulement lié à l’authentification des documents ou à la critique historique héritée de l’école positiviste (Seignobos). Elle est aussi liée à l’étude d’une langue historique qui nous échappe en partie. À ce titre, l’introduction des méthodes et techniques issues de la linguistique a véritablement renouvellé l’approche historique. Une réflexion héritée des années 70 qui a pâti de la critique du structuralisme et des approches quantitatives. C’est particulièrement le cas en histoire par rapport à la sociologie, etc.

Or, le paradoxe, est que depuis les années 2000 dispose d’une masse de textes considérables. Il est possible de récupérer énormément de textes mais pose la question de l’homogénéité des documents dont on dispose et de la pertinence de les rassembler entre eux. Donc la production des textes pas seulement une question économique, mais aussi une question de recherche.

Avant de se lancer dans la textométrie, avoir une idée précise du domaine de la lexicométrie et des rapports entre l’histoire et la linguistique.

Quelques pré-requis en guise d’introduction

On pourrait dire que la lexicométrie est une méthode de comparaison des textes, réunis en corpus. Cette comparaison se fait sur la base d’un comptage des unités de sens : La forme, le lemme, le groupe de formes. Une comparaison qui se fait sur la base de différentes échelles que l’on va appeler les parties, ce qui permet de multiplier les prismes d’analyse.

Marianne Polo de Baulieu, dans n° de Histoire et Mesure, Panorama de la lexicométrie. 1987. Elle a à l’époque procédé par enquête. Rédige une définition et propose un panorama des usages de la lexicométrie.

> Ensemble de **méthodes de description** des textes fondées sur des **indicateurs statistiques**. La description des textes par leur caractère mesurable conduit à utiliser des **modèles statistiques** qui ne sont **ni linguistiques ni explicatifs** [...], la spécificité de la lexicométrie est de **s’appuyer sur les indexes et les tableaux obtenus à partir du texte lui-même** [...].

D’abord la lexicométrie est un ensemble de méthodes de descriptions. Ces indicateurs ne sont ni interprétatifs, ni explicatifs, c’est le chercheur qui fait l’interprétation.

Constitution des catégories d’analyse, regarder les textes du corpus non pas avec des catégories pré-définies mais créées spécifiquement pour le corpus à partir du corpus lui-même.

L’intérêt de l’approche textométrique est donc qu’elle contribue à se défaire de l’idée que les textes auraient un sens qu’il faudrait dévoiler. Ne remplace pas l’historien et l’interprétation mais fournit une aide et des outils à l’historien pour l’interprétation.

Lexicologie, étude des signification des unités textuelles.

Lexicographie, écriture des entrées de dictionnaires

Chercheurs qui privilégies les vocables de textométrie ou logométrie. Ce faisant suggère volonté de dépasser le traitement des formes graphiques pour essayer de comprendre les textes dans toutes leurs dimensions. Notamment des dimensions plus profonde du discours, structure, formes verbales, temps, etc. Accéder au sens par d’autres échelles que le sens des mots.

Ces évolutions reflètes également une évolution de la discipline. Au début lexicométrie. Mais questions ont changé en même temps que l’évolution des logiciels. Rapport dialectique qui permet de comprendre qu’est passé de la lexicométrie, à la logométrie, ou la textométrie.

Pourquoi compter les mots et adopter le prisme de l’histoire quantitative ?

La première chose que nous permet de faire la linguistique quantitative, c’est d’abord d’identifier, caractériser, hiérarchiser et périodiser. Identifier vocabulaire significatif du corpus, caractériser le vocabulaire d’une époque. Fournir des hiérarchies non pas basées sur des jugements de savoir mais des constats opérés sur le texte.

Fourni des méthodes pour éclairer les textes, lever les ambiguïté des mots, des énoncés, mieux comprendre le caractère construit du discours. Donc identifer des stratégies... et la construction du discours.

Peut utiliser

- comprendre idéologie
- étudier les représentations d’individus ou de groupes sociaux

Indicateurs qui sont autant d’aides à l’interprétation des textes.  Outils aussi pour vérifier nos hypothèses ou nos interprétation. Caractère heuristique égaelement de la statistique qui révèle des phénomènes ou des pistes que le chercheur peut choisir d’étudier ou de laisser de côté. En ce sens que la lexicométrie, une sorte de couche suplémentaire entre le chercheur et sa documentation.

L’automatisation des tâches, la croissance de la puissance de calcul des machines permet d’étudier des masses de documents considérables.

La première étape d’une analyse lexicale est l’établissement du *dictionnaire des formes* appelé également *inventaire distributionnel*. Cet inventaire propose le décompte de l’utilisation des formes dans un texte. Cette simple opération permet de hiérarchiser l’usage des formes dans le corpus.

Parfois des colonnes sup : nombre d’occurence, caractère des formes.

Cette liste est le résultat d’un processus que l’on appelle la *segmentation*. La segmentation est le processus mis en œuvre par le logiciel qui consiste à distinguer et compter toutes les formes du corpus. Le logiciel identie des *caractères délimiteurs* qui séparent ces formes. Par ex. l’espace, les signes de ponctuations, qui permettent de distinguer la chaîne de caractère 1 de la chaîne de caractère 2.

Dès lors possible d’avoir accès au corpus de manière délinéarisé. Cette approche est fondée sur un useage de la statistique qui est un usage émergeantiste. Il s’agit de faire émerger des phénomènes qu’il s’agira d’interpéter.

La *forme* est la forme graphique indépendamment de son énonciation. La chaîne de caractère.

Ensuite *occurence* ou *fréquence*, c’est d’abord le nombre d’utilisation totale des formes dans le corpus. Exemple tableau Père Duchesne. Mais aussi le nombre d’utilisation d’une forme particulière dans le corpus.

En statistique quand on parle de fréquence on parle le plus souvent d’une fréquence relative. On calcule la place d’un élément par rapport à la totalité des éléments du corpus. Ici, on donne des chiffres qui sont des fréquences absolues. Il s’agit de ce qui est réalisé dans le corpus.

Il faut donc bien faire attention dès à présent au fait que votre logiciel distingue des chaînes de caractère indépendamment de leur signification. Il va donc rassembler des formes qui sont homographes mais qui ont un sens différent. Ici du rassemblement que l’on pourrait vouloir éviter. Ex.

À l’inverse les logiciels vont séparer des formes hétérographes mais qui ont le même sens. Ex. c’est le cas des verbes. Là le problème est inverse, on a de la dispertion. Modifie les hiérarchies.

Que peut-on faire avec un tel dictionnaire distributionnel ?

La première chose que ce dictionnaire permet de faire, c’est de repérer des erreurs de segmentation. Parfois certains logiciels ne reconnaissent pas les apostrophes droites, etc. Permet de resegmenter le corpus.

Identifier des manques dans les caractères délimiteurs. Soit les supprime soit les remplace.

Identifier des caractères inatendus. Lorsque manipule des fichiers textes, différents formats de caractères. Coquilles, fautes, etc. issues de la production des corpus.

La lecture du dictionnaire permet aussi de comparer et d’étudier l’usage des formes. Repérer des usages attendus ou inatendus. Pose la question du pourquoi qui permet d’entrer dans un processus de réflexion à partir d’un simple comptage.

Ce dictionnaire va pouvoir être réalisé à l’échelle du corpus mais aussi de différentes parties du corpus. Alors peut se rendre compte que les fréquences sont différentes et entrer dans le processus du pourquoi.

Intuitivement, vous allez également commencer à avoir des idées de catégories ou de thématiques d’analyse. Le dictionnaire permet de construire des catégories d’analyse.

Révéler des vocables, l’importance de signifiants.

Attention à bien garder un regard critique sur la statistique. La lexicométrie peut parfois engendrer des regards biaisé. Exemple affaire Corneille/Molière. Mesure sur la distance entre les textes à partir des travaux de Cyril et Labbé, certains auteurs ont voulu confirmer une hypothèse née au début du 19e siècle que Corneille aurait écrit une partie des textes de Molière. Sorte de théorie du complot destinée à cacher la production critique de Corneille. À l’appui de cette thèse, ajoutaient des faits historiques avérés, pas conservé archives de Molières. Analogies linguistiques, parfois collaboré. À partir de la statistique... en utilisant Hyperbase.

D’autres chercheurs ont contesté cette théorie dont Etienne Brunet papa de Hyperbase, travaux sur la distance des textes, et la métrique. Ont démonté cette théorie. Trouve les termes du débat sur un site de Paris IV. http://www.moliere-corneille.paris-sorbonne.fr

Ce qui est en jeu ici pas le logiciel, mais l’usage des chiffres et leur interprétation. Si ne connaît pas bien le contexte dans lequel les auteurs ont écrit. Peut nous amener à dire des bétise.

Le problème que les gens qui considèrent que l’histoire quantitative ou ces approches sont dépassées, c’est qu’ils prennent ces exemples pour dire regardez ce que vous faites sans tenir compte de l’ensemble du débat historiographique.

La différence ici, qu’à partir des même données, plusieurs interprétations différentes. Reproductibilité. Tout le monde peut reprendre l’expérience.

Considérer la statistique comme heuristique plutôt que probatoire. Alors pas de pb, si croit que le chiffre peut prouver, risqué car peut projeter ses hypothèses, etc. Si considère seulement que la statistique fait remonter des questions alors gagné.

Il faut simplement se servir de la statistique comme interrogation plutôt que comme preuve.

Est-ce que tendance à aller vers la statistique pour objectivité pas issu de la critique de la subvectivité. On a aussi des exemples de pb avec le qualitatif. L’analyse quantitative n’est pas exempte de subjectivité. Arrive avec des hypothèses construites. Le chiffre en lui-même n’est rien, tout comme l’archive n’est rien si n’arrive pas avec des questions.

Bien sûr les statistiques textuelles ont fait leur irruption en histoire pour essayer de répondre à la question de la subjectivité de l’analyse en histoire.

L’analyse lexicométrique, que peut-elle apporter à l’historien qui pose une question ou une problématique ? Question de la constitution du corpus. Travaille sur un corpus que nous avons construit pour répondre à nos intérêts de recherche. Simplement ici, dix historiens qui interrogent le même corpus auront ici toujours le même dictionnaire. Ici quelque chose de commun à partir duquel pourra discuter.

On est dans le contexte de protocoles qui nous obligent à expliciter nos choix. Les choix sont explicites, cad qu’il y a un modèle d’analyse explicite.Si fait une régression ou ... deux modèles différents mais un choix. Pratique du numérique qui repousse le moment de l’interprétation. Très fort car un outil heuristique, mais la probation en histoire est un processus scientifique qui réthorique. Simplement ici, on sait dans quelle dimension on discute.  Sans analyse textométrique le débat concernant Molière aurait été sans fin. Sans doute la force de la méthodologie.

## Quelques éléments de linguistique pour les historiens

Pas inutile d’observer les grands renouvellement de la linguistique pour comprendre comment la lexicométrie a fait son irruption en histoire. C’est le renouvellement profond de la linguistique au XIXe siècle qui a amené intérêt sur ces méthodes pour améliorer le travail de l’historien.

Linguistique étude des langues dans toutes ses dimensions. Oral ou écrit comme étude de la transmission du langage.

Jusqu’au 19e siècle les langues ont beaucoup été considérées au prisme de leur histoire. Idée d’une origine commune qu’il fallait retrouvée. Théorie évolutionniste héritée des lumières. Humbolt, Schleicher, etc. Dendrogramme que retrouve dans la théorie de l’évolution.

À la fin du 19e siècle, Ferdinand de Saussure qui aujourd’hui est considéré comme le père de la linguistique structurale et tournant de la linguistique (alors que d’autres chercheurs au même moment mêmes approches). Dans son cours sur la linguistique générale, rompt avec cette théorie évolutionniste pour essayer de comprendre comment une langue se structure. Va chercher des modèles abstraits plutôt que de rechercher la genèse.

Grande force de S de proposer une théorie générale qui permet de distinguer des ... différentie aisni langage (moyen d’expression) de la langue (ensemble des signes à disposition pour parler)...

Ce qui l’amène à considérer la langue et le langage comme construction sociale (là qu’intéresse les historiens). La langue est l’association d’un signifié et d’un signifiant (l’unité de sens). Pour comprendre le langage doit donc étudier à la fois les deux entités. Cela l’amène notamment à considérer l’enchaînement des signifiants comme une séquence temporelle. Signifiant 1, 2, et 3 et que ne peut comprendre le 2 qu’en rapport au 1. C’est seulement dans cette chaîne temporelle que le signifiant peut être compris. Dans son usage.

Le signifiant seul ne suffit pas à donner le sens du texte. S montre que s’insère dans un système. 

> Quand on dit qu’elles correspondent à des concepts, on sous-entend que ceux-ci sont purement différentiels, définis non pas positivement par leur contenu, mais négativement par leurs rapports avec les autres termes du système. Leur plus exacte caractéristique est d’être ce que les autres ne sont pas. » (p. 162 Cours de linguistique structurale)

Il faut donc étudier l’ensemble du système et pas seulement une forme. Comprendre la forme par ce qu’elle est et ce qu’elle n’est pas pour bien comprendre le sens. Ici le principe de différentiation. Le sens pas lantent, mais émerge du reste du système.

Compréhension dans le rapport syntagamtique (dans le temps) mais aussi dans le rapport paradigmatique (associatif). Ce dernier permet de comprendre le sens à partir de toutes les formes qui auraient pu être utilisées à sa place. ex. Enseignement, famille de signification, classe de mots, etc. Ce rapport là avec tous les autres mots qui auraient pu prendre la place d’enseignement qui est le rapport paradigme.

Aller-retour entre le rapport syntagmatique et paradigmatique qui permet de comprendre le mot.

Exemple Canard. Ne sait pas ce dont parle. Pour comprendre doit considérer ensemble des mots qu’aurait pu utiliser. Un nom, matériel ou imatériel. Si matériel, animé ou inanimé. Si animé, mâle oiseau alors palmipède. Si inanimé, comestible ou pas, si non comestible un journal, si comestible oiseau ou pas = sucre ou viande.

Avoir à l’esprit cet enjeu car une grande partie du travail du lexicomètre c’est jouer sur ces questions. Trouver des formes et ensuite les désambiguiser pour les analyser.

Permet d’introduire un déplacement dans l’étude des langues de la diachronie vers la syncrhonie. Comprendre que 

Pour S la langue ne peut pas être étudiée dans la diachronie. C’est le système de la langue qui doit être étudié. Pose problème pour l’historien pour qui le problème c’est la diachronie, le temps.

**J.-Ph. Genet**, **P. Lafon**, « Des chiffres et des lettres : quelques pistes pour l’historien », *Histoire et mesure*, 18 (2003), p. 216.

> puisque chaque élément linguistique, chaque signe, avait une valeur qui correspondait précisément à sa fonction et à sa place **dans le système de la langue à un moment donné**, il était donc **impossible d’établir la valeur d’un signe** quelconque **sans comprendre l’ensemble du système**. Du coup, la construction du **discours historique**, à partir d’une structuration le plus souvent **diachronique** des citations, le rendait hautement **suspect**.

Critique faite qu’il faudrait comprendre au préalable le système entièrement avant de pouvoir étudier un texte.

La lexicométrie n’est pas adaptée pour étudier le système langue, ce n’est pas sa fonction. Elle est adaptée pour faire des comparaisons sur un corpus construit que l’on va décrire et dans lequel va repérer les différences.

Le texte pourrait être étudié pour lui-même et en lui-même. Ce que va étudier c’est la norme du corpus. Chaque corpus constitue sa propre norme.

Pour comparer doit avoir des points d’approches, des éléments de comparaison entre les texte : c’est ça le corpus. Que faut-il faire pour ne pas comparer des torchons et des serviettes.

S fait la différence entre la langue et le texte et coupe l’étude de la langue de tous les éléments de contexte et sociologique. Mais permet par contraste de comparer la langue comme étude de contexte sociaux.

Dans de nombreux cas entre Québec et France, utilise des signifiés différents pour signifier des signifiants différents. Achaler et ennuyé, etc. Quelque chose que l’on peut donc appliquer pour étudier des groupes sociaux différents.

Lexiques spécialisés entre la ville et la campagne pour distinguer blé et fouin.

L’étude de Régine Robin, thèse sur les Cahier de doléance de la Révolution qui a permis de montrer comment certains vocables étaient associés à certaines catégories de population, mais aussi comment certains groupes sociaux capables de se fondre dans le vocabulaire du tiers état pour protéger ses privilèges. Exemple droits féodaux, ou privilèges. Nobles utilisant le mot propriété. Usage variés des vocables. Groupe peut se fondre dans le vocabulaire d’un autre.

La démarche de l’historien est différente du linguiste. Différence entre norme et problématique. On ne construit pas un corpus pour connaître une langue mais répondre à une question. Ce qui intéresse pas la langue mais le discours en tant qu’émission de signes, en tant que produit, qu’intentionnalité.

**Régine Robin**, *La société française en 1789 : Semur-en-Auxois,* Paris, Plon, 1970.

> **L. Guespin**, « Problématique des travaux sur le discours politique », *Langages*, 23 (1971), p. 10. « Le mot d’**énoncé** et celui de **discours** tendent à s’organiser en une opposition : l’énoncé, c’est la suite des phrases émises entre deux blancs sémantiques, deux arrêts de la communication ; le discours, c’est l’énoncé considéré du point de vue du mécanisme discursif qui le conditionne. Ainsi un **regard** jeté sur un texte du point de vue de sa structuration “en langue” en fait un énoncé ; une étude linguistique des conditions de production de ce texte en fera un discours »

L’historien lui porte attention aux règles et aux contraintes qui portent sur le discours. Varie selon les conditions d’énonciation, etc.

Cf. Cuminal, et al. Analyse d’un discours d’extrême droite, 1997.

Auteures cherchent à montrer à quel point les conditions d’énonciations avaient influencé les thèmes et les formes du discours. On ainsi éclairé les grandes orientations idéologiques mais aussi les stratégies énonciatives. Au travers d’une étude quantitative qu’ont pu identifier des thèmes dominants mais aussi des stratégies énonciatives comme l‘évidence contre la démonstration. Vérité reconnue par tous qui tient du bon sens, ou encore le déplacement. Ensemble de mécanismes qui font que le discours du FN a un écho dans la population.

Une étude qui a aussi contribué à montrer comment le FN avait réussi à se banaliser.

## De la linguistique à l’histoire

L’apparition de la linguistique et la linguistique quantitative dans la discipline historique s’est faite à partir des années 30.

Avoir trois noms en tête :

L’étude de la fréquence des formes de Geoges K. Zipf qui a débuté une étude statistique des occurences des mots dans différentes langues. Un des premier à faire une étude lexicométrice centrée sur la fréquence des formes. Étude lexicométrique d’Ulysse centrée sur la comparaison statistique de la fréquence des formes, de leur réparatition. Le premier à avoir proposé un profil des formes et de leur comparaison.

Premier jalon dans cette linguistique quantitative.

Zellig Harris lui un des pères de la linguistique distributionnaliste. Il propose de comprendre les formes dans leur distribution à l’intérieur d’une phrase ou d’un texte. Comprendre les formes dans leur environnement et leur contexte. Une linguistique d’énoncé. Un de ses apports comprendre que pas de sens a priori. Il fonde son analyse sur des critères de grammaticabilité. Logométrie. Influence sur la linguistique de discours, et linguistique de corpus.

Dernier nom, Noam Chomsky, père de la grammaire générative. Son idée est de considérer que dans une langue il y a un nombre fermé d’énoncés possibles qui pourraient potentiellement être décompté. Permet l’introduction de l’approche probabiliste dans la linguistique. Ex. iphone et proposition de mots. Chaînes de Markov, enchaînements redondants qui de manière probabiliste reviennent plus que d’autres. A ouvert la voix au TAL et la traduction automatique.

Trois piliers sur lesquels repose encore l’usage de la textométrie en histoire. Intérêt pour la linguistique que retrouve dès Lucien Fevre. Mais jamais remis en cause jusqu’aux années 70, l’approche subjective des textes par les historiens. Approche des textes, intuitive ou inductive, puis hypothético-déductive. On rassemble un corpus pour répondre à la problématique, on repère les citations pour aller dans un sens ou un autre. Intuitivement construit un discours ou une démonstration pour appuyer une thèse. Dans ce cas là la validation des résultats repose sur un contrat de confiance entre le lecteur et le locuteur.

Ce sont les limites de ces approches intuitives de l’histoire et le manque de réflexion sur les pratiques qui ont amené les historiens à s’emparer de la linguistique et de la linguistique quantitative.

Parmi ces historiens deux personnes particulièrement cruciales.

Régine Robin, Histoire et linguistique où reprend les postulats de sa thèse et les systèématise pour chercher à fonder une nouvelle discipline. Remet en cause de manière très véhémente la manière dont travaille les historiens. 1973

Antoine Prost, Vocabulaire des proclamations électorales. Même démarche à la même époque. 1974

Dénoncent l’objectivité de l’analyse historique. Critique de trois postulat :

- transparence du contenu
- permanence du sens et de la sensibilité qui suggère que l’historien peut directement entrer dans le sens
- capacité du chercheur à embrasser tous les phénomènes avec une approche linéaire

Absence de protocole de constitution et d’exploitation du corpus. L’historien a une pratique impressionniste. Trouve dans l’analyse linguistique quantitative un processus pour maîtriser le processus d’interprétation. Donner à l‘historien des outils et des méthodes complémentaires pour confirmer ses impression, mais aussi les nuancer ou ouvrir des pistes de réflexions auxquelles n’aurait pas pensé.

Construction des catégorie d’analyse à partir d’indicateurs solides à partir desquels peut construire l’interprétation.

Régine Robin emploie des mots très durs à l’égard des historiens : "misère méthodologique" de l’historien face au texte. "lecture sauvage" des historiens. Foucault dit que l’historien fait une synthèse subjective.

Régine Robin exhilée au Canada, carière à l’UQAM. Décédée il y a deux ans.

Idée d’introduire une couche d’informatique entre l’analyse et l’interprétation.

St Cloud, base corpus language Étienne Brunet à Nice. Des projets dans d’autres universités. Jean-Philippe Genet à Paris I. ENS St Cloud devenue ensuite ENS Lyon a continué. Tournier à ENS Lyon. Pêcheux, etc.

À paris le LISH laboratoire informatique pour les Sciences humaines. Utilisation des ordinateur la nuit. Petit groupe que se retrouvait pour travailler. Dynamique collective qui disparaît avec l’émergence de la micro-informatique. Un phénomène socio-technique. Analyse factorielle popularisée par Bourdieu. Retour anti-structuraliste très violent qui fait que rejette la méthode avec le personnage. Dans les années 80 et 90 faire une thèse avec un ordinateur, vraiment une héresie. Ensemble de phénomènes qui impliquent un retrait. Histoire et mesure 84-86. Nombreux numéros sur la linguistique. Puis dimension informatique gommée. Des générations d’historiens qui se remettent au travail.

Changements de paradigmes en histoire, changements critiques, et les nouveaux contexte de production.

Micro-histoire qui casse tout. 

Aussi une crise dans l’histoire quantitative anglo-saxone et américaine dans la fin des années 80. Pb méthode, demande aux étudiants de faire du neuf. Mais aussi les spécialistes devenus très performants en mathématique appliquée, rend l’accès très difficile. 

Chutte des publications quantitatives en histoire à la fin des années 80. Corpus grandissent mais prise en charge longue. Élément explicatif.

À partir des années 80, un paradoxe entre les progrès exponentiels de l’informatique et le fait que les historiens se sont éloignés de l’informatique.

Quelques facteurs explicatifs

- crise du structuralisme, rejet du quantitatif
  Si ne peut pas reconstituer le passé, alors à quoi bon compter ? Foucault. 
- une dissolution de l’identité et de la discipline historique
  crainte qu’en allant chercher des méthodes ailleurs fait de vous quelque chose qui n’est plus tout à fait un historien. Recherches en linguistiques sont aussi allées vers la grammatilation du discours mais pour un historien complique de travailler sur les déterminants, pas outillés pour cela.
- opposition entre quantitatif et qualitafif
  rejet plus ou moins fort des méthodes quantitatives. Vrai que l’usage de la lexicométrie impose de maîtriser des notions complexes mais aussi des notions spécifiques. Mais obstacle bien plus méthodologique que scientifique.
- Scientisme et déterminisme
- Des logiciels limités
- L’investissement méthodologique
- Une méthode tautologique ?

Important de comprendre les outisl que l’on a va utiliser. Pour comprendre un résultat avoir conscience des outils.

Confusion entre les faiblesses de certaines études et les limites réelles de la méthode. 

Il y a toutefois des critiques qui méritent d’être réfléchies. Pour les historiens qui se lancent dans cette question, Pêcheux produire une théorie globale du discours de manière historique. Taxé de scientisme.

Premières approches à partir de mots pivots. Formes qui nous intéressaient puis écrivait livre à partir corpus de phrases qui nous intéressait. Ambiguïté entre le fait de vouloir mettre au point une méthode qui réduit la subjectivité et la sélection opérée. Toutefois à l’époque limites des logiciels utilisés, fiches perforées. Bien évidemment ne pouvait à l’époque faire la même chose.

Logiciels qui demeurent limités. Certain nombre de choses que ne peut pas dépasser. L’ordinateur qui ne permet pas d’embrasser toute la dimension du langage : ironie, périphrase, etc. En revanche repère des expressions figées, qui pemrettent de dépasser cette limite.

Réfléchir aussi au postulat que la redondance fait l’importance. Part du pratique que va hiérarchiser en fonction de la quantité. Mais bien être conscient que parfois un seul mot blaisse plus qu’un discours. Établir les fréquences, mais aussi les hapax qui peuvent avoir une importance démeusurée (basculer une élection, déclancher une guerre).

Bien être attentif à l’investissement méthodologique. Il y a un véritable coût, pour l’acquisition du corpus, l’apprentissage des méthodes, etc. Une question d’interdisciplinarité pour prendre à bras le corps un champ sécant entre l’histoire et la linguistique.

Critique qui revient souvent celle de la tautologie. Critique récurrente de l’analyse du discours que travail lourd qui produit des résultats insignifiants car quelque chose qu’aurait pu voir avec lecture cursive.

**Antoine Prost**, « Les mots », dans Réné Rémond (éd.), *Pour une histoire politique*, Paris, Seuil, 1988, p. 134.

> Cette contestation radicale ne s’exprimer guère à haute voix, mais la plupart des historiens la partagent, et on ne les ébranle guère en plaidant la valeur démonstrative des méthodes linguistiques. Pour eux, en effet, la linguistique permet de prouver des évidences, et c’est perdre son temps que de le consacrer à de tels “gadgets” méthodologiques. Il y a là un préjugé d’autant plus tenace que, pour le dissiper, il faudrait lire des travaux empruntant aux méthodes linguistiques, et que la technicité de ces travaux dissuade trop souvent de les lire.

L’apport minimal de ces méthodes là, de démontrer. En cela un apport. Logique d’avocat contre logique d’historien.

Paradoxe d’autant plus grand sur la possibilité d’utiliser cette méthode, que 70 ans de recherche et d’historiographie que dispose de méthode et de procédure.

- Définition du sujet, élaboration des problématiques et établissement du corpus
- préparation des textes (acquisition, préparation du corpus, choix de la partition, réflexion sur les unités et les échelles de l‘analyse)
- exploration statistique du corpus
- interprétation

Le corpus pour maîtriser le processus interprétatif

Est-ce que l’on constitue le corpus pour disposer d’une norme de la langue ou pour un objet heuristique.

Trésor de la langue française (débuté en 1952, première publication dictionnaire 1997). 416 ouvrages du XIXe siècle et de 586 ouvrages du XXe siècle, recense environ 70 millions d’occurences et 71 415 formes

= une norme de langue

Alors que pour l’historien l’enjeu est de produire un espace de comparaison raisonné des textes réunis en un univers clos selon des règles strictes.

Ce corpus est un construit, et il s’agit d’un monde clot.

Peut adopter définition du corpus de Rastier

**François Rastier**, « Enjeux épistémologiques de la linguistique de corpus », Texto, 2004 (En ligne)

> Un corpus est un regroupement structuré de textes intégraux, documentés, éventuellement enrichis par des étiquetages et rassemblés : (I) de manière théorique réflexive en tenant compte des discours et des genres, et (II) de manière pratique en vue d’une gamme d’applications.

- Corpus d’énoncés
- Corpus intégraux

On constitue donc le corpus en fonction de règles et ensuite de problématiques.

**Antoine Prost, « Les mots », art. cité, p. 279.**

> Nous avons vu que le corpus devait présenter trois caractères : être **contrastif**, pour permettre des comparaisons ; être **diachronique**, c’est-à-dire s’échelonner dans le temps, pour permettre de repérer des continuités et tournants ; enfin être constitué, sinon de textes d’organisation, du moins de **textes significatifs**, assignables à des situations de communication déterminées.

- Contrastivité permette des comparaions
  Les textes doivent être suffisamment différents les uns des autres pour que des textes ressortent des comparaisons
- Diachronicité, s’échelonner dans le temps pour permettre de repérer des continuités et tournants
  Règle de l’historien. Diachronicité une déclinaison de la contrastivité.
  Diachronicité qui dépend du sujet d’étude. Le paradigme de l’historien les ruptures et les évolutions.
- Représentativité
  En quoi les textes que vous réunissez sont-ils représentatifs de ce que vous étudiez.
- Réflexivité
  Ajouté par Damon Mayaffre. Un corpus peut s’éclairer de l’intérieur. Un monde clos. Si réunit les discours de Chirac, peut étudier les discours de Chirac en étudiant l’ensemble des discours de Chirac. Mais peut aussi être éclairé de l’extérieur en étudiant des discours suffisamment semblables pour l’éclairer de l’extérieur avec d’autres sous-corpus qui lui sont comparables. Permet de voir ce qui est spécifique de Chirac par rapport à d’autres concurrents.
  Deux façons d’éclairer le corpus, de l’extérieur ou de l’intérieur.
  Plus ou moins facile à appliquer.

Réflexivité aboutissement naturel de cette conception du corpus clot sur lui-même avec son idée d’endogénéité. L’idée c’est que plus votre corpus rassemble les ressources interprétatives de chaque texte. Si chaque texte s’éclairent les uns les autres, ou en quelque sorte sont en dialogue. Si vous avez fait cet effort de la construction du corpus qui préssent cette dimension interprétative après très confortable de travailler de manière homogène.



Ex. Matthieu : Archives parlementaires, saisie à Cambridge. Peut employable. Dans le texte comparaisons entre les différents locuteurs. Dédouble le pb. 

Pb Acquisition et de préparation du corpus.

Aujourd’hui trouve énormément de choses sur le web. Ressource dans lequel va pouvoir puiser pour constituer des corpus. Mais toutes les périodes ou toutes les thématiques ne sont pas traitées de la même manière.

Parfois possible OCR, et correction manuelle.

Préparer le corpus :

- Relecture
- Partionnement (texte, auteur, date...)
- Balises Lexico3
- Balise Hyperbase

Prendre garde pour les textes MA ou époque moderne, travail sur des textes homogénéisés avec appareil critique et savant. Modification de la langue qui s’opère. 19e siècle parfois très mal édités, censure, etc.



Jean-Michel Adam résume en disant que tout corpus doit être précédé d’un moment philologique. Que transcrit-on ? D’où vient le travail ? Quelle échelle ? etc.

Relecture fondamentale. Car permet de développer des intuitions, des questions de recherche, etc. Textométrie une aide supplémentaire. Indiquer à votre logiciel quelles sont les parties de votre corpus.

Utilisation de balises.

Quelles sont les unités d’analyse.

Le mot et la forme. 

- Formes hétérographes (verbes, formes verbales composées, adjectifs, formes figées, langues anciennes ou à déclinaison, voire synonymes).
- formes homographes (homonymes)
  Ex. pouvoir verbe ou substantif
  Première conclusion de thèse de Damon, gauche entre guerre surutilise parti. Mais avec la forme graphique comptait aussi je suis parti.
  Un mot sur trois est homographe dans un texte : la une du journal, une voiture. La la note ou le déterminant. Sale ou saler, etc.

Pour contrevenir à ces différents problèmes possible d’appliquer un certain traitemeent. Quand un corpus toujours garder acquisition première. Mais possible de regrouper et uniformiser pour lutter contre la dispertion des formes.

De la même manière on peut dégrouper. Manière de berner le logiciel pour avoir les catégories que veut voir apparaître.

La lemmatisation
L’opération qui consiste à réduire les formes à leur entrée dans le dictionnaire. Ex. Tous les verbes à l’infinitifs. Noms communs réduits au masculin.
Plusieurs outils : Tree Tagger, Cordia, etc.

- le regroupement et l’uniformisation contre la dispersion
- le dégroupement pour distinguer (*pouvoir, pouvoir)
- la lemmatisation

Il y a eu beaucoup de débat dans les années 80 sur le fait qu’il faille ou pas lemmatiser. Certains pour car change l’ordre dans le dictionnaire, permet de mieux voir des phénomènes qui pourraient être dispersés. Les opposants considèrent que perd trop d’information. La liberte et les libertés, pas la même chose.

Les partisants de la non lemmatisation considéraient que pas de norme de lemmatisation. 

Enfin, temps nécessaire pour lemmatiser un texte.

Étienne Brunet a clos le débat avec un article : Qui lemmatise, qui dilemme attise. 
https://hal.archives-ouvertes.fr/hal-01575442

Une fois que le texte lemmatisé, quasiment plus compréhensible. Ex Corneille.

Autre manière, travailler en fonction des racines, des notions et des étiquetages. Regroupe des formes ensemble car constitue des notions. Enfin étiquetage morphologique-syntaxique qui permet d’aller là à l’étape de la logométrie en allant au niveau des fonctions syntaxiques.

Idéologie ne transpire pas toujours du lexique. Parfois d’un code grammatical. Ce qui différentie la droite de la gauche. Un temps verbal, le discours de la droite presque toujours au passé-composé. Incapacité à se projeté, énoncer le discours au futur. Retour en arrière, terre qui ne ment pas.

Une démarche qui peut aussi être itérative.

Quand ente dans la lexicométrie, entre de plein pied dans un domaine de recherche linguistique. Se fait aussi héritier d’un contexte historiographique complexe. Corpus qui détermine les résultats et devient monde clos à partir duquel va mener les analyses.

André Salem en héritier de St Cloud était un tenant du respect de la forme graphique. Ne pas transformer le texte, à l’informatitien de produire les fonctionnalités pour faire les regroupement. Ne pas prendre le risque de travestir le texte. 

TGN belle réposne à la André Salem. Contrôle ce que rassemble.

Lemmatisation par touche, repérer les mots dans le dictionnaire et les rassembler.

En linguistique Analyse du discours et Analyse automatique du discours. Dès les années 70 opposition entre analyse du discours et automatisation de l’analyse du discours (Pêcheux). Pêcheux a ensuite changé de perspective en introduisant les notions de Foucault, Barthes, etc. en considérant les possibles du discours. Analysé le contextuel.

Toujours eu aussi une approche non quantitatif. Kentin Skinner. Guilhaumou qui a un moment quitté le quantitatif. Sans renier mais arrêté. Skinner, sur le discours politique de la Renaissance, mais uniquement du qualitatif. École qui a plus prospéré dans les années 80 et 90. Mais plus suspect.

Des gens rapidement sensibles à la matérialité du discours. Des tokens qui sont opérables. Matérialisme textuel comme point de départ. Maingueneau au début comme ça. Dans les années 80, 90, prise en compte du caractère limité de ces approches en matière de pragmatique, de figure de style, d’énonciation même. Contre-courant qui a soulevé la complexité du discours, et notamment son aspect très subjectif jugé inopérable par l’ordinateur.

Pense qu’il y a des marques et des traces formelles de l’énonciation.

Ruth Amossy, courant de la nouvelle Rhétorique qui s’interroge sur le fait que l’oridnateur puisse repérer la métaphore. Courant sur le dialogisme et l‘intertextualité. Ensemble de questions versées contre le traitement lexicométrique. Bras de fer entre les qualitativistes et les quantitativistes. 

Toujours retour au texte, dans cette approche que retourne au texte.

Pour préciser les choses, ici ne travaille pas sur une esthétique de la réception. Ne regarde pas comment le discours est reçu par l’auditoire, comment le lecteur est séduit, etc. Ne permet pas de le faire. Considérer le texte avec ses limites.

Quid des outils qui permettent de standardiser la détection des figures de style. Oui avec l’analyse factorielle des correspondance et travail sur les co-occurence peut identifier des isotopie, ou regroupements co-occurents avec de mêmes mots. Bâteau et arrive, capitaine, mer, vague, dont peu conclure qu’un peu le même sens. Alors dans ce cadre là, si capitaine plus avec navire, etc. alors sans doute un usage métaphorique.

Première définition lexicométrie, logométrie, textométrie. Repérage statistique des unités, mais tous les penseurs de ce domaine viennent de l’école anglo-saxonne contextualisante qui considère que le mot ne peut que se comprendre dans le contexte. Tous les logiciels permettent ainsi le retour au texte qui fait partie du cahier des charges des informaticiens qui ont implémenté la lexicométrie.

Clivages qui pouvaient exister entre quantitativistes et qualitativistes en partie esquivés. Sur certains logiciels, il existe aussi des métalangages pour chercher des alitérations, etc.

Peut-on créer une notion à partir des procédés narratifs ? Par exemple sur la manière dont l’emphase est mise en avant dans le texte. Si repère des formes qui selon vous sont significatives de l’emphase, alors vous pouvez les annoter pour ensuite pouvoir les déceler de manière systématique. Simplement le logiciel ne les détecte pas directement.

Ici touche à des problèmes épistémologiques cruciaux. Différence entre l’itération nécessaire et bien traitée dans le cercle herméneutique où des informations réinvesties dans le corpus et le danger de la circularité qui fait réintroduire des choses que sait déjà. Prendre le risque de retrouver ce que l’on cherche.

Toujours commencer en considérant que ne sait pas ce que l’on cherche. Ici qu’une forme de sérendipité du savoir.

Les vraies vertues de la méthode, c’est véritablement retourner la méthode hypothéco-déductive qui dominait les SHS. Comme on le sait on confirme toujours un peu l’hypothèse que l’on avait au départ (90% cas). Double risque, toujours trouver ce que l’on cherche et aussi ne pas voir ce que l’on ne cherche pas. Risque de déformation et d’aveuglement. Vraie prétention, retourner cette méthode là et s’abandonner à une méthode beaucoup plus à une méthode émergeantiste.

Sur la tautologie, souvent nous dit que ne fait que confirmer ce que sait toujours. Alors commencer par demander quel mot le plus aimé de Macron. Laisser le corpus nous interroger là où précédemment interogeait le corpus.

Pourquoi Macron adore le préfixe "re-". Réformer ? mais en fait plus subtile que ça. La statistique nous fait remonter une interrogation. Utilisation de mots de gauche pour pré-fixer des mots de gauche avec des mots de droite : Re-naissance ! Pas instaurer 5e république mais restaurer la 5e.



### Divers perso

Dans historique pas question pragmatique / forme

Discours, approche thématique 

Question segmentation du corpus, dimension comparative

## Après-midi

Aujourd’hui voir comment on progresse pas-à-pas pour monter des corpus et les questionner.

On va présenter deux logiciels, le premier est TXM, une plateforme développée à l’ENS Lyon il y a une dizaine d’année. Verra que comme c’est un logiciel très orienté vers des questions de laboratoires, identifiera des manques en même temps une orientation très forte sur le TAL qui permet de progresser rapidement sur ces questions. En revanche, pas de graphique de distribution de vocabulaire. Loi de Zipf Paretto.

L’idée est de pouvoir connaître les différents outils et de savoir les utiliser en fonction des phases de recherche. Bien sûr, il existe de très nombreux logiciels, pas besoin de tous les utiliser.

Quand vous avez un corpus, vous devez le structurer pour que le logiciel trouve des prises dans le corpus. On verra donc comment structurer son corpus et comment cela s’applique dans les différents logiciels.

Tableau des logiciels

DTm.vic pour aller au-delà de l’analyse factorielle. Logiciel fort pour la sémiométrie, représentation en graphe. Analyse multidimentionnelle.

Hyperbase qui présente l’intérêt d’être multi-plateforme et utilisable en ligne.

Iramuteq Python avec modules R. Intéressant pour rechercher des univers lexicaux. Mais possible aussi de mettre en œuvre méthode Rainert.

Lexico, pas multi-plateforme. Exige un émulateur sur Mac. Mais le logiciel a l’intérêt de présenter un certain nombre de méthodes qui peuvent être intéressantes pour vous. Pensée d’André Salem et logique de Paris III. Pas les mêmes analyses factorielles dans Lexico 3 et Iramuteq. Objectif moins de classer le vocabulaire que de classer des textes. Par contre, notion de segments répétés et carte de spécificités que ne retrouve pas à ce point ailleurs. Permet d’explorer la géographie du vocabulaire. Logiciel très spécifique à des modes labos et très orientés sur des analyses de chercheurs.

Peut etre utilisé en complément du logiciel le Trameur de Paris III qui met la focale sur le calcul de trame et de coocurence. Quand maîtrise Lexico, facile de passer à Le Tameur.

Module R TeMIS programmé en R qui permet de maîtriser l’ensemble des méthodes. Donne une interface graphique qui permet d’utiliser facilement. Un coût d’entrée avec le langage R. Mais coût quand travaille sur des très gros corpus avec R. Sur des volumes de données important ou des matrices lexicales importantes difficile. 

Ne pas oublier que votre population va être une population de mots.

TXM logiciel dévelopé ENS Lyon, multiplateforme Java et R, open source. Présente un certain nombre de fonctionnalités que vous allez découvrir avec nous. On explorera certaines fonctionnalités. CQL pour l’interrogation corpus. Étiquetage morpho-syntaxique, coocurences binaires.



D’autres logiciels peuvent être utilisés en textométrie. Mais ici, sélection de logiciels qui paratgent une même philosophie.

NVivo logiciel de codage qualitatif. etc.

Si vous décidez d’étudier l’enchaînement du langage ne le fera pas avec ces logiciels.

Se méfier de logiciels métiers, parfois développés pour des modèles métiers. Mais pas possible de justifier la mathématique et discuter des résultats.

Pincemin, Bénédicte. 2018. « Sept logiciels de textométrie », 12. Pincemin, Bénédicte. 2018. « Sept logiciels de textométrie », 12. https://halshs.archives-ouvertes.fr/halshs-01843695.

https://fr.wikipedia.org/wiki/Analyse_de_donn%C3%A9es_textuelles

Chaque logiciel pour fonctionner exige des formats pour manger du texte. Il peut s’agir du XML ou du TXT. Le format XML est le plus élaboré des formats disponibles pour traiter du texte.

TXM pensé dans l’écosystème de l’open data et de l’open science : Lit du XML et de la TEI. Ne pas se priver des métadonnées qui pourront être utiles par la suite.

- Importer, sélectionner XML/x + CSV ou TEI
- charger le texte
- pour travailler à l’échelle morphosyntaxique ou du lemme en utilisant Tree Tagger cocher "Annoter le corpus"

Si erreur l’annotation treetagger n’est pas possible... Aller dans Edition > Préférences, TXM > Avancées > TAL > TreeTagger

Ici des chemins

Clic droit et propriétés. Nombre d’occurences.

Word, Lemmes, pos, etc. n l’identifiant de chaque forme. Cet identifiant est ordinal. Important car une opération fondamentale. On va délinéarisé. Une opération fondamentale qui permet de compter. Mais dans le retour qualitatif, retour au texte, va devoir faire une relinéarisation. Pour cela que besoin de cet identifiant, important de savoir où est positionné tel ou tel mot.

Propriétés des unités lexicales.

Lexique, tableau à plat qui fournit la liste d’occurence.

Possible aussi de travailler à l’échelle du Lemme ou à l’échelle morpho-syntaxique. Bouton éditer.

Premières formes, les *mots outils*. Sans eux ne peut pas articuler le language car pas de sens. D’autant plus qu’ici on est sur un dictionnaire de l’ensemble du corpus. On n’est pas encore dans l’analyse contrastive. Souligne qu’à ce niveau là sans contraste, le comptage des mots n’enfonce que des portes ouvertes sur l’analyse de la langue française. Ainsi pas possible de faire du discours. Ceci dit, même ici pourrait identifier certaines tendance. Sait que si la plus fréquent tendance conceptuelle. Avoir et être  toujours les deux verbes les plus utilisés car des verbes, mais aussi des auxiliaires. On peut donc commencer à faire de l’analyse mais pas encore de l’analyse contrastive.

Souligne aussi combien on a besoin de statistiques et pas de fréquences absolues. Quand fait un dictionnaire voit le nombre de mots. Mais que peut-on en faire ? C’est dans la statistique constractive que va pouvoir faire quelque chose.

Ici retrouve dans la répartition la loi de Zipf. Formes les plus fréquentes très fréquentes. Mais à la moitié de plus en plus de formes de moins en moins fréquentes. Jusqu’aux hapax.

Intérêt de la lemmatisation de regrouper les différentes formes fléchées pour réduire la dispersion.

Sur une échelle régulière quelques formes très importante, etc. Sur une échelle de Pareto la représentation va tendre à la diminution sous la forme d’une droite.

Dans la distribution click droit sur la forme et Envoyer vers la progression. Voit le stock de mot s’accumuler au fur et à mesure du corpus. Bien mais problématique. Car le corpus n’est pas forcément chronologique. En revanche si explore un roman super. Car va pouvoir voir quand est-ce que l’on a des sauts qualitatifs. Permet de comprendre l’imposition d‘une forme dans un espace. Un outil ici clairement dédié à des corpus diachroniques.

Il se peut qu’un mot ou un groupe de mot vous intéresse en particulier. Possible de revenir aux différentes propriétés lexicales attachées à vos formes. Ici l’index qui permet dans une barre de recherche de saisir n’importe quelle forme graphique.

`[word="France"]`

Peut aussi travailler au niveau du lemme. Se souvenir que TreeTagger a travaillé. Alors fournir la donnée entre crochets carrés, le nom de la propriété frlemma

`[frlemma="vouloir"]`vous donne toutes les flexions du lemme. Un outil qui peut être pratique pour voir quels sont les verbes les plus souvent conjugés au futur, etc.

Il est également possible de chercher tous les mots qui finissent en isse en utilisant des regex.

`.*isme`

Ici ce sont des requêtes CQL que l’on peut perfectionner pour rechercher des effets d’assonnance ou d’allitération.

Ces logiciels de lexicométrie sont le fruit de 40 ans de recherche. Retenir qu’il y a évidemment un coût d’entrée sur le logiciel. Ici dans le format du corpus qui doit être XML. Ensuite au niveau des requêtes pour poser une question sur ce texte. Un langage qui doit pouvoir être manipuler.

CQL Contextual Query Language ou Common Query Language https://en.wikipedia.org/wiki/Contextual_Query_Language

https://www.loc.gov/standards/sru/cql/

Possible de chercher tous les verbes qui précèdent France.

`[word="France"]`

`[word=".*"] [word="France"]`

Parfois va s’appercevoir que le locuteur caractériser par motif. Transformation plus n’importe quel adjectif. Motif qui devient très caractéristique du locuteur Macron. Risque de projection en l’abordant par la requête ciblée. Mais important de voir que traitement statistique de deuxième génération qui s’intéresse au contexte.

Envoyer vers ma concordance permet de voir suite de formes, ou segment répété. Peut voir le qui.

## Hyperbase

http://hyperbase.unice.fr/hyperbase/

Plusieurs laboratoires en France qui partage la même philosophie. À un moment un laboratoire canadien. Esprit le même exploitation systématique du texte par les requêtes et exploitation statistique du corpus en observant les saillances.

Un premier hyperbase créé avec Bezecri dans les années 80s. Recrutement d’un jeune ingénieur à Nice qui a essayé de rénover le logiciel qui était un vieux logiciel, pas d’expressions régulières.

Logiciel rajeuni qui n’a pas encore toutes ces performances mais plus facile d’utilisation.

http://hyperbase.unice.fr/hyperbase/

- créer une nouvelle base (la plus intuitive possible, autant de fichiers que de txt à traiter. Au format TXT)
- base existante
- bibliothèque

On donne à hyperbase ces 5 fichiers et va faire contraster les occurences. Le pari c’est de donner un format très basique comme .txt mais avec le risque de ne pas pouvoir traiter la richessse de XML.

Il y a aussi des bases existantes, un logiciel qui se veut ouvert avec des utilisateurs qui veulent bien mettre à disposition leurs contenus de manière publique.

Plus d’1Milliards 500 Millions de mots traités depuis 2016. Un peu toutes les langues mais surtout du français.

Pari de la simplicité, copié sur google. Interface qui propose une barre de requête comme le moteur de recherche. Il est possible de rechercher un mot.

Immédiatement arrive sur un concordancier qui propose le mot dans son contexte. Tous ces logiciels font un effort de recontextualiser, et de retourner au texte. L’idée c’est que le sens naît du contexte. Le contexte minimaliste ici c’est une *concordance* quelque chose fait depuis l’analyse de la Bible. Il y a beaucoup d’écrits théoriques sur ces questions, cf. Pincement le KWIC Keyword in context, etc.

Il est également possible de rechercher un lemme.

`LEM:penser` un véritable langage de requête.

La statistique a l’inconvénient d’exploser et de tokeniser un texte. Face à cette nécessaire explosion (délinéarisation) du texte, un contre-effort de contextualisation est nécessaire.

Toujours plus de contextualisation. On peut ici renvoyer à des terminologies herméneutique, c’est toujours le global qui permet de comprendre le local. C’est avec l’ensemble du contexte que l’on va comprendre le sens d’un mot. C’est tout le texte qui va permettre de comprendre le sens de la localité. "Un chat dans la gorge" ne comprend qu’avec le contexte pas en additionnant les mots.

Possibilité de retourner, de manière systématique, au texte et au contexte.

Quelques éléments de statistiques.

Jusqu’à présent on a surtout regardé des fréquences absolues et peu de contraste.

Reprenons le mot amour ou France. En partant d’une requête qualitative, j’ai envie de regarder la distribution, ou la macro-distribution du mot France dans mon corpus. On s’accorde sur le fait que nous avons un corpus contrastif. 

Ici je veux voir si cette unité linguistique se distribue équitablement à travers le corpus. Ici possible en un clic avec la notion de distribution.

Distribution qui comme on le présent est beaucoup utilisée par la droite et moins par la gauche. Tout de suite, c’est cela l’outil de base de la lexicographie originelle, il s’agit de voir qui utilise quoi et qui surutilise quoi par rapport à la norme du corpus.

Ici avec histogrammes positifs ou négatifs suggère qu’il y aurait une équidistribution des emplois du mot France. Or, ici s’aperçoit que certains ont surutilisé, d’autres sous-utilisé.

L’idée c’est effectivement qu’il existe une norme endogène, dans le corpus. En gros nous avons 10 locutuers, 100 fois le mot France. L’équidistribution voudrait qu’ils en aient utilisé 10 chacun. Si différence peut estimer que surutilisé ou sous-utilisé le mot face à la norme ou l’endogénéité du sens.

Il est possible de faire des recherches en multipliant les mots et voir le profil lexical de chacun.

Ici, nous sommes dans une posture très supervisée qui n’est pas la posture idéale dans le textométrie que nous défendons qui est que l’on explore à partir des requêtes très dirigées. J’ai choisi quatre mots que je juge pertinents. Pourquoi ces mots là ? Mais cela fait partie de ces logiciels que de pouvoir faire ces explorations très supervisées commandées par nos hypothèses de travail.

Il est aussi possible de faire des requêtes à partir des parties du discours.

Obtenir, non pas les quatre mots, mais tous les mots. Ici le même travail sur tout le contenu nominal. Le logiciel a étiqueté le corpus au moment de la création de la base (chez nous grâce à Spicy) alors devient possible d’avoir des requêtes au niveau linguistique : qui utilise des noms, qui utilise des verbes.

Voir la structure nominal d’un discours : savoir si dialogique, ou descriptif. Alors regarder les verbes utilisés. Montré que bascule des années 80 d’un discours nominale vers un discours verbal. Devient de plus en plus fatique : faîte moi confiance. De Gaule, le Peuple de France détient la souveraineté nationale. Déterminant, adjectif. À partir de Mitterand des phrases plus verbales : je vous dis et je vous répête ce que je vous ai déjà dit. La communication politique a profondément changé.

Performativité forte dans le discours politique.

`NOUN`

Graphique représenté par un indice de spécificité, mais en cliquant sur l’engrenage peut changer sa présentation.

Exemple, les 10 plus fréquents.

Ici on arrive sur les spécificités. Outil de base. D’un point de vue visuel, sur le graphique commence à devenir illisible. Si monte à 100 complètement illisible. Ici considère que quand trop de données passe en liste pour lisibilité.

200 plus fréquents, bascule sur une représentation factorielle. Un saut risqué. Mais qui permet de voir le vocabulaire préféré des uns et des autres, ici en contraste. AF qui comme la spécificité demande beaucoup de précautions méthodologiques mais qui là s’appopse aux autres.

Aime bien les mots marché, accord, idée, programme.

Macron qui aime parle de ministre, investissement, etc.

Sur Hyperbase web possible de changer les axes et faire tournoyer le nuage. À condition de comprendre comment lit ce type de tableau.

Co-occurence que va regarder en deux clics.

`souveraineté`

Comme dans tout concordancier, peut trier par fréquence.

Théorisé en 30 ans l’idée que le sens naissait du contexte et que le contexte peut se calculer. Ici remobilise la statistique en considérant que ce contexte on va le modéliser. L’idée est que le sens d’un mot est la somme de ses co-occurences. Considère que sens du mot va dépendre du sens.

Exemple va penser à des coocurents : lait, étable, fromage et veau. Le calcul des co-occurence permet de toucher au sens.

Topic modeling une vectorisation des profils concurrentiels. On a regardé terme à terme, comme la fonction correlat que va montré. Mais idée qu’en effet la somme de toutes ces co-occurences, chaque mot ayant des fréquentations distributionnelles avec les autres, en les aditionnant possible de les représenter sommets avec ses concurrents et permet de repérer les thématiques du discours à cause que dans la même direction trouvera ensemble mots convergents, alors que dans une autre direction mots qui tirent dans la mêem direction. Mais pour le faire aller dans le profil co-occurentiel. Mais faut-il encore voir qui fréquente qui.

Fonction thème pour voir comment peut représenter ce genre de choses.

Toujours distinguer le calcul de la représentation. Mais ici, comprendre que la grosseur des traits indique la force du lien concurrentiel.

Ici de mots qui sont peut pertinent comme "est" car souveraineté souvent définie. Va pouvoir paramétrer avec la roue pour faire apparaître tous les noms. 

Voit par la force des liens les mots particulièrement co-occurents de la souveraineté. Européenne plus que française. Moins la souveraineté de l’état nation France que européenne. Nombreux adjectifs. Intéressant quand se souvient que pour Roland Barthes l’adjectif, la mort du concept. Le vit dans sa substance idéologique. Les attributs de la souveraineté sont très déclinés.

Ici on travaille sur le corpus non partitionné. Mais on peut également avoir envie de regarder ce même mot, chez l’un et chez l’autre. Choix du corpus en haut à droite.

exemple chez Zemmour associé à la question des frontières, de la puissance, de la reconquête. Les échelles dans l’outil, un certain seuil de co-occurence et d’attraction. En modiiant le seuil, fait apparaître d’autres mots comme identité, etc. Même requête chez Macron, différent. Climatique, industrielle, énergétique, populaire, progrès, question de transition, etc.

Le même mot permet de faire une histoire des concepts par le biais objectivé de la co-occurence par deux locuteurs qui font semblant de s’entendre sur le signifiant et quand regarde le contexte de signification s’aperçoit que ne l’utilise pas du tout dans le même contexte.

A caché deusième barrette qui permet de faire des co-occurences dans le sens de ??? pour aller plus loin dans le rafinement terminologique. Souveraineté devient co-occurent de liberté et de bataille.

A essayé de le représenter sous la forme de nuage de mots. Fausse démagogie car tout le monde pratqiue els nuages de mots. Mais ici la taille des mots, 30 ans de travail sur la co-occurence, ici proportionnelle à l’indice d’attraction. Ici encore peut jouer sur la réglette pour faire varier le seuil.

Bataille = pair attraction souveraineté bataille qui devient attracteur d’aurtes mots.

Si vous trouvez trop raide de dire que la somme des mots co-occurent donne le sens, au moins visualise univers sémantique du mot qui peut être subjective. Ici du grain à moudre dans l’interprétation, toujours avec l’idée interprétative.

Si regarde ailleurs, macron, référence à la monarchie, parlement, etc.

Pour avoir beaucoup étudié le discours politique, il y a une sorte de malice du discours qui fait que peut être dans le temps très court d’une campagne, les candidats politiques sont contraints à un stock de termes limités. Il y a donc une sorte de consensus sur les signifiants et les candidats vont devoir essayer de donner forme à ces signifiants par des mots co-occurents. 

Dernière campagne deux candidats qui adoraient le mot classe. Mais quand regardait le travail sur les co-occurents, pas du tout les mêmes. Hamon classe, primaire, etc. Mélanchon populaire, etc. Mais ici force un peu le trait car linguistiquement pas le même sens.

Faire attention aux nuages de mots, car un algorithme qui cherche à optimiser l’espace en plaçant les mots. Il s’agit donc d’une représentation graphique qui peut très vite induire du sens. Autre risque dans ce type de représentation comme les couleurs qui n’ont aucun sens. Danger classique en lexicométrie de la surinterprétation des résultats.

Regarder le tableau où duement chiffré. Indice de coocurence.

Module IA, et fonctions dites de lecture qui permettent non seulement de lire le texte en entier, mais aussi classement par indice de spécificité d’un locuteur donné classées par ordre hiérarchique.

Passe du très supervisé au moyen de faire ressortir les saillances. Pourquoi donc parle-t-il de la vendée. Pourquoi la droite de l’entre-guerre utilise le mot a et le passé composé. Pourquoi Macron utilise "re"...

Quand dit que pas probatoire heuristique, pour autant très fort car va s’obliger à explorer toutes ces choses là.

Auxiliaire évidemment dans tous les discours, mais beaucoup plus dans le discours de droite de l’entre deux guerre.

Fonctions lecture

et topic modelling, vectorisation du sens.

Derrière logiciel, des logiques d’interface. TXM très orienté sur l’exploration du traitement automatique de la langue et ce que signifie en langage naturel. L’équipe de Nice qui met l’accent sur le traitement statistiques.

Essayer de comprendre ce qu’il y a derrière chaque logiciel. Des outils qui correspondent à des logiques différentes.

Je crains l’homme d’un seul livre, il faut vraiment craindre l’homme d’un seul logiciel. Car des parti-pris, etc. Dès lors que l’on utilise plusieurs logiciels, neutralise certaines fonctionnalités. Par exemple pour ma part, j’utilise rarement l’analyse factorielle sans analyse arborée. Important car en multipliant les points de vue, peut confirmer les choses.

Statistique, pas seulement du calcul, une grande part qui est orientée sur la sémiologie graphique à travers la visualisation.

## Deuxième journée

Retour sur les lectures

Salem démonstration mathématique. Tant que dans le texte de Mayaffre volonté d’offrir un panorama des outils logiciels utilisés pour développer ces analyses de réseau.

Plusieurs corpus et orientations présentées dans Mayaffre. Salem un seul corpus.

Une idée à la Guilhaumou, aller chercher de l’idéologie à partir du discours chez Salem. Chez Mayaffre, volonté de faire émerger les thématiques.

On emploie plus aujourd’hui « analyse de données textuelles » pour reconnaître que le texte est considéré comme une donnée. La donnée est quelque chose d’élémentaire, en soi elle n’a pas de sens. Mais on peut passer de la donnée à l’information. Quand on travaille, on a des informations que l’on transforme en données, ou en obtenus (cf. Bruno Latour). Il y a deux niveaux de connaissance à partir de cette information : Jacques et Jeune, connaissance parce que j’ai interprété l’information. L’âge moyen est une donnée mais c’est une connaissance car elle nous permet d’interpréter quelque chose.

Les deux auteurs ne parlent pas du tout de la même manière des opérations de délinéation. Quid de l’appelation forme. Mayaffre, Pincemin sont plus dans une dimension linguistique. Ils parlent donc de mots. Salem n’est pas du tout dans cette perspective linguistique. Il emploie une définition très informatisante. Pour lui une forme une chaîne de caractère délémitée par des caractères délimitants. Mais les deux réfléchissent sur l’échelle pour déterminer quelle échelle est pertinente pour travailler sur les textes.

Salem ne reste pas au niveau des simples formes mais à des suites de formes qui sont les segments répétés. Le mot seul n’a pas de sens, c’est le contexte qui va faire émerger le sens. Coocurences : mots formes ou lemmes qui gravitent fréquemment autour d’un mot. Le sens d’un mot, la somme de ses enfants (Harris). Le segment répété, une séquence ordonnée de formes, en réalité une locution. Segment répété est le segment le plus fort.

Séquences figuées : pouvoir d’achat

Séquences aglomérées : duquel, desquels, etc. pour lesquelles peut avoir plusieurs formes

Lemmatisation ou étiquetage morpho-syntaxique, ce n’est rien d’autre qu’une transformation de données.

Passage à la méthodologie de l’AFC

L’AFC a été fondée pour la linguistique. Une méthodologie commune, quelle différence entre les deux. Attractions et répulsions. Comment c’est construit ? Rien d’autre qu’un nuage de point dans le quel observe la distance entre les points. Les deux auteurs ont la même pratique de l’AFC. Mais d’un point de vue scientifique comment l’AFC arrive-t-elle dans le discours et à quoi sert-elle ?

Plan factoriel très proche de celui présenté par Salem chez Mayaffre. Est-ce que mesure des contrastes entre discours de président. En réalité mesure de la diachronie du temps entre les présidents. Retrouve parabole. Ne jamais perdre de vue que des variables cachées en arrière de certains textes (du temps, de l’espace). Ces analyses exploratoire doivent permettre de déterminer s’il n’y a pas d’autres choses derrières plus structurantes dans les contenus.

Ligne de gradation.

Peut voir des formes qui vont se rassembler. Mais parfois formes employées dont les contextes changent et forment un isolat. Question du pourquoi.

Quelles sont les limites de ce que l’on peut calculer ? À minima des coocurrences binaires, cad deux formes qui sont en attraction ou en répulsion. Mais il peut aussi s’agir de polyoccurences, pas des segment répétés, mais des suites de mots qui se fréquentent.

Cf. étude de Lafont sur les résolutions de congrès des centrales syndicales française.

Pour Mayaffre, l’analyse factorielle est émergeantiste. Il l’utilise pour dire qu’est-ce que j’ai dans les données.

Benzécri lorsqu’il fonde la méthode, rejet des méthodes hypothético-déductive. Quand fait une régression on exporte les hypothèses : est-ce que l’âge explique le poids ? dans l’analyse factorielle, va construire des catégories que nous n’avons pas déterminées.

Une stratégie exploratoire.

Produire une matrice des formes généralisées. Calcule lemme par lemme, forme par forme le nombre de phénomène coocurentiels. Voir les phénomènes se distinguer sans a priori. L’analyse factorielle étant souvent un travail sur l’écart, emploi récurrent de certains termes, emploi assez homogène des termes.

## Lexico

Lexico est un logiciel développé par André Salem. C’est un logiciel qui intègre cette notion de segment répété. Nous allons voir comment fonctionne.

Logiciel qui est l’héritage de Saint-Cloud. Un logiciel qui intégrait des fonctionnalités très modernes comme drag-en-drop. Très remarquable car peut accéder directement à des données. Concentré de la philosophie lexicométrique. Logiciels qui ont connu les évolutions de la ds

À Montréal : Dominique Labbé, Monière

Pas de traitement automatique de la langue intégré. Mais possible de récupérer corpus lemmatisé dans TXM, etc.

Lexico3 possibilité de segmentation importante.

Possibilité de choisir les délimiteurs. Dire au logiciel ce qu’est une forme : une concaténation de lettres comprise entre duex caractères délimiteurs.

Salem, obsédé par la simplicité et la non infantilisation des utilisateurs.

Méthodes contrastives qui nécessite de regrouper les textes par locuteurs, etc.

Une fonctionnalité pour partitionner le corpus. Retrouve la structure du corpus précédente qui correspond aux balises placées au début des textes.

Propose à cet endroit, dans une perspective comparatiste, les PCLC Principales caractéristiques lexicographiques du corpus. Une des forces du logiciel. Permet de voir la quantité des sources. Permet alors d’avoir un rapport réflexif sur la composition du corpus. Permet de juger de la représentativité du volume.

Question qui revient souvent de la taille nécessaire pour le corpus. Différence entre le plus petit texte ou le plus grand. Comme compare, ne peut pas facilement comparer des textes textes de taille trop différente. **Avec un rapport entre 1 à 10, devient alors fragile statistiquement.**

Aller sur la carte des partitions. Choisir qui.

Drag and drop de la forme sur la carte des sections, permet d’afficher les valeurs. Fonctionnalité presque exclusif à Lexico. Joli mais aussi un parti-pris matérialiste du texte. Le texte se matérialise par des § qui sont matérialisés avec des carrés. À l’intérieur de cela peut projeter un mot et préssentir qu’il y des fonctionnements linguistiques qui font qu’un mot apparaît régulièrement.

Répartition irrégulière. Mot thématique. Idée que va pouvoir visualiser des comportements très différents : mots en raffale, etc. Par exemple produit un article sur les segments répétés longs dans les deux mandats de Jacques Chirac. Lieu où plus de liberté pour le locuteur. Sorte de prêt à parler. Si on projette simplement sur cette carte de section ces segments répétés longs, s’apperçoit que plus y vieilli et que ne prend plus de risque, plus de segments répétés longs. Sorte de refuge.

Segment répété, paramétrage et seuil minimal des occurences. Si trop bas plein de séquences textuelles. Dépend de la taille du corpus. Encore possible de revenir sur les délimiteurs. Peut paraître superfétatoire, mais ici retrouve des séquences de mots. "Je suis parti, et je suis revenu". Comme on redécoupe les unités textuelles qui feraient sens, on repose la question du délimiteur.

Ici, beaucoup de speetch to texte. Beuacoup de ponctuation ajoutée. De même pour les phrases. Rastier explique que la phrase est un artefact du grammairien.

Idée que le mot fait sens avec ses coocurences. Mais rapidement rendu compte que le mot sans contexte pas de sens. Comment faire pour dépasser dimension mot : transformation linguistique du texte avec étiquetage. Mais Salem pas favorable à cette transformation. Idée que va pouvoir avoir des observables qui feront plus de sens. Croit à la statistiques, comme va avoir des segments répétés, des morceaux du texte qui probablement font du sens ensemble.

La spécificité ne peut se considérer qu’à partir d’une partition.

Pas de logiciels de textométrie qui ne présente pas cette fonctionnalité. Si on en comprend l’usage et la philosophie, compris ce qu’est la philosophie de la textométrie, de ses orignes à aujourd’hui. Donnée textuelles que va synthétiser dans un tableau dont les lignes sont les parties et les colonnes sont les formes. Il s’agit d’un tableau de contingence en statistique, on parlera de table lexicale en textométrie.

Ce que l’on a va explorer c’est ce tableau, il y a deux méthodes : une fondée sur la spécificité, l’autre fondée sur l’Analyse factorielle.

Question savoir si prendre toutes les premières formes, quid des mots-outils. À chaque fois, choix méthodologiques et choix de recherche que va imposer à ces sources. Processus de fabrication qui est important dans la construtcion des données.

La spécificité est la recherche des mots qui sont caractéristiques d’une sous-partie du corpus par rapport à l’ensemble du corpus. L‘intérêt de cette spécificité, c’est qu’a priori, il n’y a pas de pré-supposé pour le logiciel, va regarder toutes les spécificités pour toutes les formes.

Au départ on a un tableau de contingence (à double entrée) qui croise les formes et les temps. En noir apparait le nombre d’occurences. À droite apparaît un indice spécificité

Khi2 si dans un tableau de contingence, prend les marges. Apparaît x fois (somme des occurrences). Métrique quantité d’écart entre ce que l’on observe et cette situation d’indépendance. Dans la littérature anglo-saxone, se traite avec le Khi2. Mais Lafon a proposé autre chose. 

Débat entre Guéraud et Lafon. Critique Guéraud de la spécificité car la distribution du vocabulaire pas une loi normale. Une échelle de valeur et regarde la propabilité d’avoir telle ou telle valeur. Loi gaussienne. Cela voudrait dire que la fréquence d’apparition des mois devrait suivre cette répartition. Or, on a vu que la distribution des fréquences de mots, une distribution de Zipf Pareto qui est une distribution inégale.

Ce qu’a proposé Lafon reconnaît que pas cette distribution. Ce qui l’intéresse pas le vocabulaire total, mais le vocabulaire local. Loi de Pascal, loi hyper-géométrique. Nous avons un sac avec 1000 billes, 800 rouges et 200 noires. Lorsque tire les billes 80% tirer une rouge et 20% des noires. Un tirage avec remise. Si un tirage sans remise, chaque fois que tire 10 billes. De la probabilité, donc résultats qui peuvent être moins probables. Quand tire dix bille, calcul combinatoire qui permet de calculer la probabilité de tirer trois ou sept. Il est donc possible de calculer la probabilité sur un tirage de 10 d’obtenir 3. C’est ce que permet de faire la loi hyper-géométrique. Il s’agit d’une probabilité exacte.

Si tire 9 billes rouges tout d’un coup. Beaucoup de chances, car tirer 9 billes rouges sur un tirage à 10 est très très peu probable.

Que dit Lafon ? C’est qu’en réalité on a un sac de mots. Prend une forme, voit les marges. Quelle probabilité d’obtenir x rouge. Accèpte que le résultat attendu soit au-dessous ou pas de 5%.

Sous-entendu que dans cette partition un emploi de cette forme qui n’est pas lié au hasard. Il y a donc un emploi particulier de cette forme.

On prend la valeur absolue de l’exposant

0,5% = 5 * 10e-2

1,15 = 15 * 10e-1

Si +13, On récupère l’indice ou l’exposant qui va donner la profondeur. 10e-13 = une chance sur 10 000 milliards que l’emploi de la forme soit dans la norme.

Si 10 fois le mot progrès. La probabilité que 8 soit concentré dans une des textes, une chance sur 10 000 milliards. Plus l’indice est fort, plus est convaincu que un choix du locuteur qui utilise le mot progrès.

Dans un tableau de contingeance on ne peut pas comparer les lignes entre elles directement. Il faut passer par le listes de distributions absolues. La liste d’occurence en elle-même pas de sens.

Un tableau de contingeance se lit vraiment en deux dimensions.

Il y a quatre variable

- T ensemble du corpus (l’urne)
- t la taille de la partie
- F la fréquence du mot dans l’ensemble du T
- f le nombre de fois où le mot apparaît dans la partie.

Si joue sur proportionnalité peut comprendre qu’il y a une moyenne ou norme endogène. La spécificité calcule cet écart entre l’attendu théorique et ce que l’on observe.

Indice positif sur-représentation

Indice négatif sous-représentation

Entre les deux la normes. MAis pas pour autant que pas de signification. Peut charier un sens contrastif.

Difficile pour isoler identifier des régularité. Pour cela que l’analyse factorielle sera utile. Mais très pratique pour caractériser une pratique.

On a vu deux usages de l’analyse factorielle qui consistent à regarder quels sont les vocabulaires qui fonctionnent ensemble.

Dans lexico il n'y a pas le vocabulaire dans l’analyse factorielle. L’intérêt de l’analyse dans la philosophie de Lexico c’est de classer l’ensemble des textes. L’ensemble des mots sont là mais l’analyse pas là pour classer les mots mais les textes. Ils ont en revanche participé à la construction du graphique. C’est eux qui y participent.

## Analyse factorielle des correspondance

L’analyse à la française. Une méthode née dans le contexte français en particulier dans le domaine de l’analyse linguistique.

[Jean-Paul Benzécri](https://fr.wikipedia.org/wiki/Jean-Paul_Benz%C3%A9cri) croise Noam Chomsky dans un séminaire sur la traduction automatique. Chomsky dit qu’il ne peut pas y avoir de procédure automatique pour déterminer la structure de la langue.

Benzécri persuadé de trouver la structure divine dans l’organisation sociale.

Benzércri réfute une approche déductive de Chomsky, et développe une approche inductive qui réfute toute approche déductive... Faire émerger des variables.

> Dans les études statistiques conçues pour répondre à une hypothèse explicite, les épreuves de validité ont
> un rôle essentiel (cf §§ 2.2.3 & 2.2.6). En analyse des correspondances, il n’y a d'autre hypothèse a .priori que l’existence entre les deux ensembles I et J d'une liaison dont on cherche la structure.

Pour ça va utiliser table de contingence. Pearson à la fin du 19e siècle propose une méthode pour mesurer... le Khi2.

Benzércri propose dans la lignée de Harris de traiter le texte en ligne et colonne. Une élève [Brigitte Escoffier-Cordier](https://fr.wikipedia.org/wiki/Brigitte_Escofier-Cordier) qui va programmer solution sur un IBM 1620 en 1965.

Antoine Prost travaille avec des doctorants de Benzécrit.

Vulgarisation des travaux de Benzécir par Philippe Cibois. Rencontre les travaux de Benzécri et sa thèse est une reformulation de ses travaux. Débouche sur un Que sais-je qui est disponible aujourd’hui sur son site web.

- https://books.openedition.org/enseditions/1443
- https://cibois.pagesperso-orange.fr/PrincipeAnalyseFactorielle.pdf

Voir son analyse des textes d’Elisabeth Teissier qui est intéressante car une réflexion sur la preuve en Sciences humaines. Un objet réflexif sur nos pratiques.

L’AFC a été rapidement fondatrice de la lexicométrie. Comme les logiciels ont rapidement intégré les spécificités, elles ont rapidement intégré l’AFC.

L’idée c’est de représenter les données linguistiques fondamentales d’un corpus contrastif de la manière la plus synthétique possible. Soit un corpus partitionné en quatre partie par locuteur. Je veux représenter la fréquence du mot France dans ce corpus partitionné. Imaginons que De Gaulle utilise 8 fois. Mitterand 2, Sarkozy 7, Macron 3fois. Ici commence à construire un tableau de contingences.

Comment représenter ces données ? la manière la plus simple consiste à faire un graphique en batonnet où aura en ordonnée la fréquence du mot "France" et en absice les locuteurs. Alors interprétation possible, De Gaule et Sarkozy utilisent beaucoup le mot France.

Un peu plus complexe, on veut représenter notre discours non pas via un mot mais via deux mots. Par exemple Ouvrier et France. On compte dans les 4 sous-corpus. Comment maintenant représenter ce tableau de contingences élémentaires à deux dimensions. Bien sûr le graphique en bâtonnets est encore valable, il suffit d’ajouter dans une autre colonne les effectifs d’ouvrier par locuteur. Et ainsi de suite en multipliant les mots.

Toutefois, il existe une autre façon de représenter ces batonnets qui est une représentation dite vectorielle. On va tracer un vecteur, une flêche que l’on oriente comme ça. Vecteur France que l’on va graduer. Et on va mettre sur ce vecteur France la position de mes locuteurs. De Gaule, Mitterand, etc. Sur un vecteur France, on aura des individus tendus vers ce patriotisme et d’autres moins. Il s’agit d’une représentation vectorielle.

Passons aux deux dimensions qui nous intéressent. On construit un vecteur perpendiculaire ou orthogonal. Avec ouvrier et France dans les deux dimensions. Puis on situe nos deux coordonnées sur ces vecteurs.

Alors devient possible dans notre espace à deux dimension identifier des locuteurs qui aiment bien les ouvriers et qui n’aiment pas trop la France. Ou ailleurs des locuteurs qui aiment beaucoup la France et pas trop des ouvriers.

Si maintenant on ne veut pas deux mots mais trois mots. Technologie, par ex. On veut maintenant représenter ces trois dimensions. Désormais nous avons une troisième dimension que l’on veut exprimer. Cela reste encore compréhensible pour nous. On pourrait simuler une troisième dimension qui serait la profondeur avec un autre axe et placer nos points dans cet espace à 3D comme on a procédé précédemment.

C’est là que l’analyse factorielle des correspondances intervient dès lors que l’on ne veut plus représenter 2, 3 ou 4 dimensions, mais peut être l’ensemble du tableau de contigeance qui contient les formes de notre texte. Il s’agit d’ajouter une quatrième dimension, 5e, etc. 

Ici que va avoir recours à une comparaison boiteuse du hérisson et de la giraffe. Tous ces mots qui vont se succéder on pourrait imaginer qu’il y aurait autant de vecteurs qu’il y a de mot. Ce que l’on dessigne serait une sorte de hérisson avec autant de pic que l’on a de mots. Mais comment extraire le maximum d’information pour comprendre l’essence de ce hérisson sachant que plus le loisir de lire chacun de ces vecteurs ou de ces dimensions. L’analyse factorielle est un procédé mathématique destiné à extraire le plus possible d’information de ce hérisson. Cela consiste au fond à écraser ce hérisson de manière à extraire le plus possible d’information.

Par exemple, il faut que la distinction ouvrier / France se retrouve en même temps que toutes les autres informations contradictoires ou complémentaires. Ici que l‘on passe du hérisson à la giraffe. Le hérisson il est plutôt uniforme et que l’on l’écrase dans un sens ou un autre toujours la même chose. Mais imaginez au lieu du hérisson la giraffe ou un long vecteur qui serait le cou, un autre la longueur, et enfin un autre moins intéressant qui serait la profondeur. L’analyse factorielle, sait comment écraser une giraffe de la manière la plus intelligence pour restituer le maximum d’information de la giraffe à savoir qu’elle a un grand coup et qu’elle est longue. Dans notre vecteur, des informations qui sont essentielles et d’autres moins importantes à expliquer. C’est cela que l’analyse factorielle restitue. Dans l’écrat entre Macron et de Gaule, le vecteur plus le mot France ou ouvrier, mais un concentré d’information qui vont avoir un même profil que le mot France. Ces différentes dimensions vont être en quelque sorte concentrées dans un seul vecteur qui sera par exemple horizontal, et qu’il y a un autre vecteur sommet (réunissant plusieurs mots) et qui pourriat permettre distinguer les autres choses dans l’axe.

Comme nous avons une matrice, on peut dire que l’on a un vecteur de Gaule définit pas X coordonnées, un autre pour Mitterand. Ce que Benzécri explique c’est que ces vecteurs on peut tous les mettre ensemble. Même démonstration que l’on aurait donc pu faire dans le sens inverse et l’intérêt de l’analyse factorielle c‘est de pouvoir le faire dans les deux sens. Aurait pu traiter les colonnes avant les lignes. L’idée est que à la fin des fin, on aura la même représentation mais avec les formes associées qui expliquent la position.

Ici crie au génie et surtout la facilité de lecture, malgré les risques d’interprétation. Mitterand et Macron se ressemblent parcequ’ils aiment bien le mot ouvrier. Etc. Très rapidmeent on s’aperçoit que l’axe factoriel, souvent représenté en pointillet se fait en quadrants où l’oposition ne peut pas être plus maximale que dans la diagonale.

Transformation une géométrisation d’espace et tout de suite la méthode a été conçue pour pouvoir présenter une représentation visuelle.

Chaque facteur un résumé, on simplifie le tableau d’origine et se retrouve à avoir la droite et la gauche.

Brigitte Escofier-Cordier, « L'Analyse Factorielle des Correspondances », *Cahiers du BURO (Bureau Universitaire de Recherche Opérationnelle)*, vol. 13,‎ 1969, p. 25-59 ([lire en ligne](http://math.agrocampus-ouest.fr/infoglueDeliverLive/digitalAssets/25975_Th__se_Brigitte_Escofier.pdf) [[archive](https://archive.wikiwix.com/cache/?url=http%3A%2F%2Fmath.agrocampus-ouest.fr%2FinfoglueDeliverLive%2FdigitalAssets%2F25975_Th__se_Brigitte_Escofier.pdf)] [PDF])

## L’Analyse factorielle des correspondance : principes

Que se passe-t-il avec un tableau de coocurrence pour construire un vecteur. Cf. Ouvrage de Philippe Cibois dans sa thèse. L’Analyse factorielle, Paris, PUF, 2000 [1983], pp. 22-30.

Le but de l’Analyse factorielle est de partir d’un tableau de contingeance comme ceci et de pouvoir les représenter. Pour pouvoir comparer on a besoin de produire un tableau des écarts à l’indépendance qui permet de matérialiser les écarts théoriques à l’indépendance. De combien s’écarte-t-on par rapport à une situation d’équi-répartition. On multiplie les sommes marginales et les divise par le total.

L’idée de l’AFC est d’essayer de résumer l’information contenue dans le tableau des écarts à l’indépendance. Ici ce tableau est petit, mais projetez-vous sur votre table lexicale, vous aurez un très grand tableau. Il sera assez difficile de pouvoir trouver des lignes qui se ressemblent et malaisé de percevoir des phénomènes de sous-représentation ou de sureprésentation.

Ici dans ce tableau peut identifier des lignes qui se resemblent assez. Sur et sous-représentation.

L’analyse factorielle des correpsodnance réalise une première approximation du tableau des écarts à l’independance à partir de coefficients qui résument ce tableau de douze cases en 7. 

Ici des écarts très conséquents vont être priorisé dans le jeu de calcul des coefficiants avec la contraintes de trouver les bons coefficients. Ces coefficients sont soit positifs ou négatifs. Autres plutôt négatifs. Mais aurait pu le prendre à rebours. 

On a créé un contraste par un jeu de positif et de négatif.

Ma matrice deux vecteurs, et les deux mis bout à bout fait un facteur.

Ici une approximation. Plutôt sureprésenté. Voit des endroits où des approximation contrainte s‘est plantée par le jeu des contraintes. L’approximation est bonne pour expliquer les écarts à l’indépendance. Mais peut parfois créer des aberrations. En hiéarchisant les écarts, certaines explications qui seront mauvaises ou incomplète. Il reste quelque chose à expliquer.

On obtient quelque chose de spacialisé. Il reste de l’information à expliquer.

Le tableau des écrats à l’indépendance moins ce qui a été expliqué. Maintenant on va se pencher sur des choses peut-être un peu plus ténues. Il nous reste des écarts à expliquer, l’algorithme va chercher des coefficiants pour expliquer le reste. Progressivement par approximation successive arrive à réduire les écarts.

Souvent regarde les premiers facteurs mais il peut y avoir plusieurs manières de voir les choses.

Pour fabriquer les coéfficients va utiliser la métrique du Khi2

Premier facteur Tableau qui représente 80% des phénomènes d’écart à la norme. Attention écart à la norme. Si de petites effectifs 

Si l’inertie est constante, facteur après facteur. Alors pas de spécificité. On ne peut pas dire que deux auteurs ont des stocks de vocabulaires différents. En textométrie très rare. Veut simplement dire que la corrélation n’est pas prouvée statistiquement.

Il existe une métrique le Phi2 qui permet de montrer relation. Quantité d’écart au carré sur l’ensemble du tableau. Mesure l’intensité de la relation de 0 à 1. Déjà quan 0.18 estime que forte relation. Plus le tableau est grand plus le phi2 descend. Permet d’évaluer la robustesse des relations qui sont démontrées.

Antoine Prost, thèse de Denis Péchansky, « Et pourtant il tourne » Analyse factorielle seulement sur l’axe 1. Étude diachronique. Représente ça dans une collection de Saint-Cloud.

Si choisit les mots, ici pas une sélection a priori car il y a eu un traitement statistique. Allume les mots qui ont contribué à la fabrication de la représentation.

Première lecture se fait de gauche à droite, celle qui a le plus d’information. Une chronologie qui joue. Chirac faisant la liaison historique entre les septénnats. Pourquoi ? Regarder les mots utilisés, de gauche à droite. Fonction présidentielle, visiblement autour de noms très régaliens, ou de politique internationale. Peuple, communauté, coopération. Enjeux de la guerre froide.

Dans les années 90, le président de la République se ministérialisation. S‘occupent plus des affaires domestique et surtout la crise économique après le choc pétrolier. Parlent du travail, de la retraite des salariés, l’euro. Attention au danger d’une surinterprétation possible. Les analyses factorielles fonctionnent tellement bien. Mais par la construction, on va très vite conclure à une césure temporelle autour du milieu de l’axe 1, là où devrait plutôt regarder un continuum. L’analyse factorielle grossit le trait en mettant en avant les différences puisque c’est ce qu’elle recherche.

En cherchant les mots et les adjectifs, voit facilement apparaître transformation de la fonction présidentielle entre chef des armées et de guerre, et chef des affaires domestiques.

Deuxième axe = lecture de haut en bas. Sans jamais remettre en question la césure gauche droite. Peut dire, regardez, Giscard et De Gaule très opposés sur un vocabulaire très gaulliste. De même opposition entre Sarkozy et Macron. Macron essayant de renouver avec un certain soufle Gaulien aevc des mots plus compliqués.

Information dans l’information.

Ici ensemble du nuage de mot pas trop parabolique. Cependant la littérature montre que série textuelle chronologique avec unité de locuteurs ou de conditions d‘énonciation. Parabole Guttmann ? Analyse factorielle réagit différemment par positionnement en parabole. Axe 2 parabolise l’axe 1. Car l’axe 2 reproduit de manière quadratique l’information présente dans l’axe 1.

Linguistiquement signifie que l’on est condamné à parler demain comme parlait la veille. Difficile de faire des hiatus linguistiques. Un hiatus dans l’histoire. Ce que l’on appelle le temps lexical. Cf. André Salem. 

Un numéro d’histoire et mesure consacré à cette notion de temps lexical et où remet sur le chantier cette notion.

Une manière de faire pour soutenir l’analyse, essayer de nommer l’axe. Très efficace pour la mise en récit de l’analyse factorielle. Mais attention, seulement les mots que vous choisissez vous. L’analyse factorielle dit seulement qu’il y a des clivages.

À un moment donné, doit franchir le rubicon de l’interprétation. Simplement une prudence à avoir. Avec l’analyse factorielle Stéphane Lamassé explique que l’on repousse le moment de l’interprétation. Le corpus une soupe de données, ne sait pas ce qu’il y a là-dedans. Ce que l’analyse factorielle fait c’est classer.

Danger de surinterprétation importants. Donc toujours important de croiser cela avec d’autres outils. Notamment l’univers des arbres plantés, etc. Suan Yong recruté à Nice, spécialiste des arbres. Dans hyperbase, analyse arborée utilisée au quotidien pour croiser nos résultats. Pour voir si on a la confirmation.

Un jeu visuel de distance. Distance et structure en branches. Retrouve même opposition entre les anciens et les modernes. Mais avec une information classificatoire supplémentaire. Notamment Pompidou et De Gaulle reliés par un nœud = proximité de leur discours en fonction du tableau de contingeance.

Craindre l’homme d’un seul livre, d’un seul logiciel, mais aussi d’une seule méthode.

Construction d’un arbre d’autres références. Quand retombe sur les pattes alors peut considérer que bonne voie d’interprétation.

L’analyse factorielle pas trop sensible à la taille des corpus car totaux internes. Mais si trop de zéro ou d’impacts pb (matrice creuse). Grand spécialistes qui disent, attention avec l’analyse factorielle, toujours un résultat. Ici qu’il y a un risque de surinterprétation.

Pour déterminer le nombre de formes à prendre en compte, à vous de déterminer à quel moment, l’analyse comment à être affectée par des cellules moins fréquentes. En général pas de cellule inférieure à 5.

Au début grande partie explicative avec les deux premiers facteurs. Si plus beaucoup de différences, s’arrêter au deux premiers car plus beaucoup de différence. Phénomène d’inertie, premier indicateur de la fiabilité de l’analyse. Peut-être s’arrêter au deux premiers, mais là explication sur pt %.

Critère de Kaiser qui consiste à prendre les facteurs qui conrtibuent à plus que la contribution moyenne de tous les facteurs.

Si avait élargi aux adverbes, pronoms etc. n’aurait peut être pas eu les mêmes résulats. Des processus machines qui inspire des interprétations.

Il faut admettre et intégrer que la lexicométrie n’a pas la présention de prouver le sens. Car le sens ne s’objective pas. Pas la prétention d’objectiver un discours mais le protocole interprétatif. On commence avec une sortie machine comme cela.

Corrélation chronologique qui montre que pas un hasard. Donc question pourquoi. La statistique me dit de regarder quelque chose en me signalant que pas le hasard. Si pas normal, une signification historique ou discursive et moyen d’écrire une thèse.

Parfois choses passent par l’amélioration statistique. Denis Pechansky, thèse bidon du fait qu’ignorait l’effet Guttman. Essaye analyse pour voir évolutions mais depuis sait qu’un artefact.

Analyse hiérarchique, Michel Jambu ? Comme dans un système orthonormé, possible calculer la distance euclidienne entre deux points. Recherche de la paire qui a la distance minimale. = premier groupe. Puis baricente et réitère avec tous les autres points, etc. Puis regarde la distance entre les paires. Ici les paires de mots. La hauteur est normé (hauteur d’howard).

Méthodes postfactorielles pour visualiser ces classes dans l’espace factoriel.

Cos2 qui donne la fiabilité de la représentation selon les facteurs. Une indication pour éviter les biais de lecture. Si proche de 1, les deux facteurs expliquent beaucoup sur la représentation des deux candidats. Dans les autres cas, les deux facteurs expliquent peu le positionnement. Pas les facteurs les plus contrastifs. Est-ce la modalité Jadot qui construit le facteur, ou contribut très peu au facteur.

Quand plan factoriel, ajouter le tableau des coordonnées et justificatifs. 

## 3e jour

Hier nous avons vu des méthodes constrastives.

Si pas de coude dans l’interprétation, quand s’arrête-t-on de poursuivre les différents facteurs pour l’analyse ?

Décroissance progressive des facteurs, arrive souvent quand les textes sont très semblables. Comme prologues identiques dans le corpus, les facteurs avaient du mal à distinguer les textes.

Dans Lexico3 un outil qui s’appelle l’accroissement du vocabulaire qui est très pratique pour évaluer l’homogénéité du texte.

Analyse factorielle qui essaye d’expliquer croissance de la population mondiale. Dans un énorme tableau de contingeance croise tous les critères.

Écarts à l’indépendance. Somme des écarts zéro. Car un stock. Khi2 rapport au caré rapporté à la somme des invidus. Plus important plus s’écarte de l’indpendance. Relation entre les données et les colonnes. Une analyse très utile produite par Pearson.

Un Khi2 on s’en sert pour voir si le chiffrage que l’on a obtenu sur une métrique est significatif ou pas. C’est-à dire si on aurait pu l’obtenir par hasard.

Question du sens se heurte à savoir quelle unité linguistique s’attacher pour saisir le sens. Textométrie originale regarder forme graphique originale. Mais immédiatement aperçu que le mot pas toujours porteur de sens, notamment pour des questions d’homographie. C’est la raison pour laquelle la réponse la plus correcte philosophiquement a été la co-occurence. Car s’attache au texte tel qu’il est, et va lui appliquer une approche statistique. Deux mots qui occurrent deux mots qui apparaissent statistiquement ensemble. Voir non plus le mots mais la molécule ou la paire co-occurentielle. Avec l’idée que dès lors qu’un mot coocure, donne du sens.

Multitude de recherche avec des déclinaisons. C’est une chose que de vouloir étudier les paires les mieux représentées dans le corpus. Mais on peut également à partir de cette simple approche co-occurentielle, on peut faire des choses plus compliquées. À savoir définir chaque mot à partir de son profil co-occurentiel. Liste les mots en colonne et lui rapporte tous les mots qui lui sont systématiquemnt associés. Mots qui ont la même sémantique, le même champ lexical, ont la même isotopie. Tempête et mourrir pas le même champ lexical mais si décrit catastrophe sont reliés thématiquement dans certains corpus, sont isotopique. Les mots qui ont le même profil coocurrentiel sont des mots qui structurent dans le même sens le texte et le corpus.

Par ailleurs traitements à la Martinez, de premier ordre, second ordre, etc.

On peut considéréer la co-occurence de deux manières

- on peut dire que deux mots se ressemble dès lors que co-occurents dans un §, polycoocurence, de second ordre, trame. On mobilise la coocurence pour dire que deux mots ont des relations sous condition. Dire que dans la même séquence syntagmatique se fréquentent.

- Dire que deux mots sont co-occurents parce qu’ils ont le même profil co-occurentiel. Or, très fréquent. Chaise peu fréquemment employé avec fauteuil car peut employer l’un pour l’autre. Pour autant ces deux mots ont des relations sémantiques très forte et ils ont le même profil de co-occurences. Ce que l’on appelle des corelats. Les formes sont en corélation sémantique. C’est un tableau de contingeance qui va être traité mais il sera triangulaire ou carré et non triangulaire. Dans les colonnes le mots A, et dans les lignes remet ces mots et dans la cellule le nombre de fois où se fréquentes : la coocurences. Ici pas un tableau de contingeance mais une matrice triangulaire ou carrée (car dédoulé sur la diagonale). La diagonale reste vide.

  Souvent première chose que regarde car très parlants. Identifie ainsi pour chacun polarité : politique internationale, économique, domestique et.. En quatre cadrants car l’analyse factorielle a tendrance à diviser les choses par quadrants.

  Cette approche à l’origine de Word2vec et du Topic Modeling qui sont beaucoup utilisés en IA.

  Des linguistes qui jugent qu’une co-occurence pour être traitée doit respecter le côté gauche ou droit. L’ordre syntagmatique de la langue française est très important.

## Conférence Co-occurences

Pour l’essentiel nous nous sommes tenus jusqu’à présent à une approche très fréquentante. Une approche qui ne s’intéressait pas tant au sens du signe compris dans un contexte textuel qu’à compter ou comparer ces formes entre elles. Ces opératiosn reposaient toutes sur une étape, un traitement essentiel et nécessaire, qui était la délinéation. Or, on l’a vu un lemme ou une forme peut être profondément polysémique. Cette délinéation en cassant la continuité du discours a pour effet de rompre ce lien.

Exemple du travail de Damon sur les discours de Nicolas Sarkozy. Se rend compte que la forme travail est particulièrement représentée chez deux candidats (Laguiller vs Sarkozy). Mais sans doute pas avec des définitions identiques ou de représentation. 

Exemple de la forme *canard* qui a priori est explicite mais peut présenter une grande quantité de sens. Cf. le Trésor de la langue française.

On ne peut pas penser le sens sans retour au contexte. Logiciels qui permettent de traiter ces formes par l’intermédiaire du retour au contexte.

> « Les occurrences – seules et désincarnées – sont donc les entrées utiles et nécessaires pour le traite- ment lexicométrique mais le retour au (co)(n)texte est posé comme la condition de l’interprétation. »
>
> Damon Mayaffre, « L’entrelacement lexical des textes, co- occurrences et lexicométrie », Texte et corpus, no3, août 2008, pp. 91-102

Cooccurrence, permet de hiérarchiser des phénomènes. Les concordances ne permettent pas de le faire. Une déclinaison de vive la stat !

Tout un appareillage conceptualisant. Réflexions que l’on peut dater au lendemain de la 2nd guerre mondial. John Roopert First ?, le premier à considérer que le sens vient du contexte de l’énonciation.

Textométrie un ensemble de méthodes agencées entre elles qui forment un protocole de recherche. Première phase liée à la constitution du corpus. Fournit le corpus à la machine, délinéarisation du texte puis un ensemble de procédures statistiques.

Passage par la recontextaulisation des formes. Voyez notre entêtement, car pour retourner au texte, ici aussi en faisant de la statistique et non pas de manière « sauvage ». Ici devient possible d’articuler cooccurences, et occurences. Administrer la preuve avec des indicateurs statistiques mais donner de la chaire pour illustrer les phénomènes à l’aide d’exemples sélectionnés dans le concordance.

cf. Marie-Anne Polo de Beaulieu, « Panorama de la lexicométrie », Histoire & Mesure, II/no3-4, 1987, pp. 173- 197.

Antoine Prost, Les mots, etc. Démonte définitivement les logiques mal abouties de l’historien face au texte et comment il y a un argument d’autorité dans lequel la place malhonète de la citation s’impose. Moi qui ait beaucoup travaillé sur... la preuve et trouve une bonne citation. Prost explique qu’il s’agit d’une logique d’avocat, pas de savant car la citation ne peut pas être probatoire et n’est qu’une citation. Avec une lecture plus systématique et exhaustive, vous allez pouvoir extraire cette citation d’un concordancier ou d’un § où calculé cette concordance. Il me semble que la production de cette citation là est un peu plus probatoire. Du coup, c’est le statut même de la citation qui s’en trouve un peu changée et la solidité même de votre démonstration qui se trouve renforcer.

Une manière d’éviter la critique qu’une citation serait anecdotique. Ici la citation illustre la réalité.

On va donc regarder un système de relation entre formes au sein d’un contexte pour essayer de définir une relation.

> « En synthétisant (. . .) l’ensemble des relations de cooccurrence entre les mots d’un texte, on accède à une représentation de la textualité qui permet de cerner le rôle de chaque mot dans la construction de l’ensemble du lexique textuel. Inversement, on peut en tirer des faits concernant la place que le texte donne à chacun de ses mots, et sur l’influence que peut avoir le contexte – pris dans sa globalité – sur le sens des mots. 2 »

\2. Matthias Tauveron, « De la cooccurrence généralisée à la variation du sens lexical », Corpus, 11 | 2012.

Il y a plusieurs façons de définir ces relations et les rendres explicites. L’expression de coocurence ou de co-présence ou de présence simultanée de deux unités linguitiques au sein du même contexte.

Idée de colocation, l’idée qu’il y a une relation privilégiée de quelqeus mots reliés par une structure syntaxique et dont les affinités

D’après Damon Mayaffre (2008) :

- **Cooccurrence** : « co-présence ou présence simultanée de deux unités linguistiques (deux mots par exemple ou deux codes grammaticaux) au sein d’un même contexte linguistique » (p. 92)

- **Collocation** : « Les collocations sont des associations privilégiées de quelques mots (ou termes) reliés par une structure syntaxique et dont les affinités syntagmatiques se concrétisent par une certaine récurrence en discours. » (p. 93).

- **Corrélation**:«lacorrélationstigmatisedesco-occurrents d’un certain type, ceux qui entretiennent une relation sé- mantique. » (p. 93) 
- **Isotopie** : « Concept créé par A.-J. Greimas (. . .) l’isotopie désigne globalement les procédés concourant à la cohérence d’une séquence discursive ou d’un message (...) fondée sur la redondance d’un même trait dans le dé- ploiement des énoncés (...) 3 »
- **Segment répété** : « suites de formes graphiques non séparées par une ponctuation forte 4 »

\3. Patrick Charaudeau et Dominique Maingueneau (dir.), Diction- naire d’analyse du discours, Paris, Seuil, 2002, p. 332.

\4. André Salem, « Segments répétés et analyse statistique des données textuelles », Histoire & Mesure, 1986 vol. 1 – no2, p. 8.

Exemple5 :

> « Ils [les travailleurs] y [un nouveau rapport de force] trouvent des conditions plus favorables pour leurs luttes revendicatives, pour défendre avec succès leurs intérêts les plus urgents, obtenir certains résultats, mettre en échec certaines mesures anti-sociales. Le pouvoir et le patronat doivent en tenir compte. »
>
> Pierre Lafon, « Analyse lexicométrique et recherche des cooccur- rences », Mots, no3, 1981, pp. 95-148.

Lafon distingue trois choses

- corrélations fonctionnelles
  ici prend l’exemple des adjectifs, attirés par les substantifs. Relation fonctionnelle. Intérêt > urgent, etc. On peut faire le pont avec les différentes propriétés et les échelles lexicales.
- Association des signifiés
  Exemple dans la 2e phrase mise en relation entre pouvoir et patronat. Association qui ici n’est pas fonctionnelle. Les deux signifiés sont unis par des intérêts communs.
- appel sémantique
  pas tant fonctionnels mais plus fonction du langage. Favorable va annoncer succès. Succès annonce échec, etc.

Pour résumer, il y a trois approches de la coocurence.

Depuis First? et Harris.

recherche polarisée --> on part du mot pôle vers ses co-occurents

deuxième approche recherche systématisées --> fait émerger touets les paires de mots qui s’attirent significativement

troisième approche, recherche généralisée --> où on étudie le comportement de X mots (idéalement ensemblu corpus) pour observer l’organisation coocurrentielle générale du texte

Une approche émergencielle. Explore déjà les contenus et ensuite va dans le détail.

Coocurence peut être comprise comme manière de comprendre le sens d’un texte. Pour Pierre Guiraud, le sens d’un mot "se défit finalement par la somme de ses emplois".

Pierre Guiraud, *Problèmes et méthodes de la statistiques linguistique*, Paris, Presses Universitaires de France, 1960, 145 p.

Guiraud un linguiste français qui s’opposait à d’autres approches. Phrase qui préside au Trésor de la langue française. Critique des dictionnaires. Le sens d’un mot le trouve en contexte d’utilisation. Notre apport une utilisation statistique de cette phrase.

Ce qui plaide sur Guiraud sur la relativité du sens qui ne peut se comprendre qu’en contexte, s’illustre avec les strounfs. Cette nuit j'ai bien strounfé ! se comprend qu’en contexte.

François Rastier : "Le sens n’est pas fait de références mais de différences."

Constater que des mots sont coocurents, ce n’est rien d’autre que de les contextualiser l’un par l’autre.

On pourrait s’en tenir aux segmentés répétés. Traitement concurrentiel peut s’intéresser à des choses plus récurrentes. Certain nombre de calculs qui peuvent être mis en œuvres.

## Calculs

Coocurence binaire, calcul simple. Prend une forme pôle ou une coocurence pôle. Cela donne un contexte, ici peut déterminer une fenêtre. Les coocurences peuvent être étudiées au niveau du § de la phrase. Va les dénombrer = coocurence.

Ensuite distance entre la forme pôle et ces formes. Distance zéro, segment répété, mais peut être une forme plus lâche avec des mots qui s’intercale.

forme pôle

contexte ou fenêtre

décompte des occurences

distance qui sépare forme pôle des coocurences



Du point de vue calculatoire trois méthodes retenues.

Première méthode test-score t-score et information mutuelle. Plutôt en linguistique de corpus Church et Hunt ?

Fréquence observée moins fréquence théorique divisé par racine carée de fréquence observée.

Identifier les mots qui s’attirent, information mutuelle. 

Probabilité une fréquence d’apparition. On a une information mutuelle qui permet de mesurer l’apparition au sein de couples. Mais gère mal les unités de faible fréquence. Classement des unités qui peut donc être biaisé. De même les phénomènes très massifs peuvent être surreprésentés. Ce que regarde c’est si s’écarte beaucoup de la moyenne.

Selon loi binomiale

Deuxième méthode Beauchemin et Cucumel, 1995. Orientée sur la quantité d’emploides formes. Forme pôle, coocurrent qu’observe et va déterminer la probabilité que les deux soient coocurents. Test de probabilité en dessous du seuil alpha de 5% alors oui.

Manière de classer les coocurents.

Distribution hypergéométrique du côté de l’ADT et de la textométrie. La norme aujourd’hui. Salem et Lebart.

Calcul de toutes les permutations possibles. Et calcule les permutations observées et les permutations théoriques et regard si significatif.

Si cette approche tend à être reconnue, lié à la distribution du lexique dans les textes qui ne suit pas une loi normale mais loi de paretto. Explique que du point de vue probabiliste, une loi hypergéométrique qui sera plus adaptée pour analyser les phénomènes.

Mathématiques française ont réfuté les approches fréquentistes des approches binomiales.

Retrouve le même système d’indice de spécificité et qui doit être lu de la même façon. Comme le même calcul génial. Se lit de la même manière. Attraction ou répulsion.

Calcul de probabilité, possible ajouter troisième, quatrième forme.

Dans la pratique de la coocurence on voit d’emblée que le choix du contexte est déterminant dans l’analyse de vos résultats. Il faut donc le définir.

Il peut s’agir d’un nombre déterminé de mots

la phrase

le paragraphe

Une fois que vous avez retenu une échelle d’analyse vous y tenir sinon vos résultats vont variés puisque plus la fenêtre est large plus grand nombre de coocurence.

Plus élargit le contexte, plus aura des phénomènes attraction sémantique. Plus réduit le contexte plus aura des phénomènes d’attraction syntaxiques.

Taille arbitraire qui a plus ou moins de sens selon les auteurs si on choisit nombre de mots. Pour la phrase, ponctuation forte. Mais s’enferme dans une réalité plus syntaxique (définie comme telle), là où recherchez plutôt du sémantique. § qui renvoit au moment philologique de la constitution du corpus.

Des relations assymétriques.

Si prend le pôle je, le coocurent est vouloir. Indice très élevé. Distance très courte. 

vouloir je, très différent car d’autres verbes

Montre bien qu’il y a une assymétrie de la relation.

À partir des travaux de Vitré? notion d’énergie coocurentielle. Idée de disponibilité concurrentielle. Question de savoir ce que le pôle apporte au coocurrent. Disponibilité ce que la forme pôle va recevoir de chacun des concurrents.

Retenir que des coocurences binaires qui permettent...

Deux sous-ensemble pouvant être déduits. 

Martinez, tentative approche géo-lexicale. Espace guide dans deux guides touristiques. 19e et 20e.

PCLC du corpus. dont décrit la longueur, etc. Se penche sur les coocurents binaires de la forme pôle ville. Car a réalisé une analyse factorielle des correspondance préalable qui lui a permis de voir qu’un certain nombre de formes banales, peu importe le temps. Essayer de percevoir comment peut varier dans son sens, dans l’espace (synchronie), et contraste par rapport à la diachronie.

Des formes banales peu contrastives, plutôt près du centre et ne contribuent pas. Limites de la coocurence binaire qui apparaît vite.

Cherché déplacer. Forme place. évoluent fortement. Coocurences puis revient aux concordances pour dire qu’au 20e siècle d’avantage la promenade, cheminement urbain. Guides du 19e perpective statique de la visite de la ville. Plus apréhension espace urbain mobile.

Polyoccurents pourraient être des panachages du... pas toujorus distance, peut être des groupements d’élocution.

### Abres de mots

Une autre manière de visualiser ces phénomèes

Martinez, étude avec Philippe Gambette, sur l’affaire du Mediator.

S’intéressent au traitement médiatique de l’affaire.

AFC qui fait ressortir différence entre les journaux. Permet de dresser une typologie de la presse et des titres = deux sous-corpus.

S’intéressent aux coocurence de medecin, forme surreprésenté, s’intéressent au sens en réalisant les polyoccurence. En dressent le graphe et montre que 4 grands sens, formes du discours associé.

Nuage arboré, ou tree cloud. Ajoute de la diachronie dans l’arbre.

Balance lexicale, travaux de Jean-Guy Savard. Objectif dressé un tableau d’association. Une visée d’exhaustivité. Matrice des coocurence qui ne sera pas une matrice carrée.

Une méthode qui fait appel à la notion de fenêtre et de coocurences. L’analyse de similitudes ADS, méthode Reinert dévelopée avec plusieurs logiciels. Dont Alceste, aujourd’hui Iramuteq.

Idée de reconstituer des mondes lexicaux. Regroupe les mots deux à deux. Produit des dendogrammes qui créent des classes de mots régulièrement employés ensemble qu’on appelle des mondes lexicaux.

Permet de cerner la structuration discursive. Comment dans le discours les mots sont structurés. Nous projette dans une autre manière de faire de l’analyse du discours

Filet statistique qui permet de faire remonter du corpus des phénomènes d’attraction. Est-ce que cela relève de la grammaire fondamentale, etc. Mais aspect fondamental, on postyle qu’il n’y a pas de gratuité. De fait quand on pratique. Jamais resté sec sur un constat statistique ou lexicométrique. Jamais de gratuité linguistique.

---

## Vendredi

Quand on parle, on ne le fait pas de manière séparée du texte des autres. L’intertexte. Non à partir de rien. La force de l’IA peut-être pas tant de décrire le texte mais peut-être de réussir à percevoir dans un texte donné les emprunts et les empruntes d’un autre texte.

Genette, un texte est un palimpseste. Idée qu’avec un révélateur peut voir dessous d’autres textes qui l’ont précédé. Possibilité de révéler le deep text.

Cardon, Dominique, Jean-Philippe Cointet, et Antoine Mazières. 2018. « La revanche des neurones : L’invention des machines inductives et la controverse de l’intelligence artificielle ». *Réseaux* 211 (5) : 173. https://doi.org/10.3917/res.211.0173.

IA parfois les résultats pas stables. Comment fait pour interpréter des résultats qui ne sont pas le même. Algo decohonem, algo de classification. Des formes très stables. Variance forte que sait classer. Avec Stéphane Lamassé a utilisé le fait que des formes qui ne se classaient pas pour étudier la norme de langage. On a retourné le problème.

Faiblesse de l’IA, idée de la prédictibilité. Interprétation des algorithme, font-ils sens, ou nous donnent-ils le moyen de comprendre ce qu’ils font. Des conférences entières sur ces sujets. Je pense que nous les SHS comme comprend bien les objets peut comprendre certaines caractéristiques de ces objets. Les informaticiens sont très preneurs de ces retours. Car n’arrivent plus à comprendre comment ça marche.

Sur la question des corpus, clairement nous positionne un peu différemment de la notion de corpus depuis 30 ans. Jusqu’à présent corpus contrastif qui constitue la norme et va pouvoir se caractériser par des saillances, etc. Le corpus en IA un gros défaut car dans l’approche supervisée, la base d’apprentissage par la machine pour ensuite produire la connaissance. Ne parle alors plus de corpus mais de base. Pas ce qui est étudié. Vertigineux ou inquiéttant car comprendre que l’ensemble de la connaissance produite par l’IA est directement dépendante de ce qu’on lui a donné pour apprendre. Il suffit donc qu’il y ait un biais pour tout avoir à jeter. Tant et si bien que nos corpus cessent d’être exploratoires et devient à l’initiative de la recherche comme non pas comme un objet à interroger mais à partir duquel apprendre.

Un très beau protocole, corpus apprentissage dont 90% sert à l’apprentissage. 10% qui sert de jeu de test. Seul les 10% restants que pourra explorer. Corpus qui doivent être beaucoup plus importants en DL. Pas une technologie nouvelle, deux révolutions : disponibilité des corpus et d’autre part la capacité de calcul (CPU).

## Damon

On peut considérer le texte comme un artefact et l’Intelligence artefactuelle pourrait être indiquée pour en témoigner.

La philologie qui a montré que c’est un fait de l’art, d’un auteur ou d’un éditeur. Ce qui est aujourd’hui bien démontré par une tradition millénaire.

Le texte n’est pas un objet naturel (Jean-Michel Adam, François Rastier...) et la question qui nous poursuit « qu’est-ce qui fait texte ? ».

Avec Ecco également bien entendu que le texte est un parcours de lecture, c’est le parcours du lecteur qui produit du sens. Autant de sens qu’il y a de lecteurs.

### Postulat

Le postulat c’est donc que cette IA pourrait embrasser cet objet qui n’est pas un objet naturel mais un objet artefactuel. Un texte est un parcours interprétatif. Ce qui ne nous choque pas comme linguiste.

L’ordinateur a une représentation du texte très artificielle par l’intermédiaire d’un code ou d’une représentation. Quand on donne du texte à manger à un ordinateur, il va le représenter de manière différente. Par exemple, on l’a vu, il délinéarise le texte. Pourtant l’obssession des linguistes la linéarité du texte. Même si les linguistes disent que le texte est un tissage. Rouleau qui contraint la linéarité du texte, mais le codex permet d’autres manipulation tout comme l’hypertextualité. En informatique surtout une vision réticulaire, en réseau. Par exemple co-occurence, des mots qui s’attirent les uns les autres.

L’IA donne une représentation du texte, très artificielle qui semble pouvoir mettre au jour des « artefactures textuelles ». Bruno Bachimont HDR. Idée que le texte est un construit, qu’il y a un architecte du texte qui ne l’a pas écrit linéairement.

Le numérique pourrait implémenter l’idée qu’un texte n’est pas seulement une suite, mais pourrait mettre à jour ces constructions textuelles qui font sens pour nous.

Ici, il s’agit d’éclairer et de croiser notre savoir lexicométrique avec la « boite noire » des réeaux de neurones artificiels.

### L’enjeu de l’intelligence artificielle appliquée au texte

Quels sont les éléments qui font texte, qu’est-ce qui à un moment donner fait du Macron ? En textométrie, choix d’une unité et statistique.

Il y a un moment donné où on admet que l’urne qui constitue le texte, est un texte. Un phénomène que l’on appelle la textualité. Moment où les mots rassemblés ensemble avec une grammaire mais aussi idéologie.

**Idée de découvrir « les unités constituantes/constitutives » du texte ? Pour nous la spécificité. Mais rapidement s’aperçoit que peut être d’autres unités.**

**L’idée que l’IA sans a priori aucun, va déterminer ce qui fait texte ou quels sont les unités constituantes ou constitutives du texte.**

Depuis 30 ans on a élargi le spectre de ces observables en textométrie. Là l’intelligence artificielle permettrait d’élargir encore. Par exemple travailler seul pas pertinent car Sarkozy et Laguiller utilisaient tous les deux ce terme. Aussi pourrait nous induire en erreur. D’où le travail de deuxième main de la co-occurence qui permet de résoudre le problème. Coocurrence très marxiste chez Laguiller alors que chez Sarkozy aucun coocurents identiques, très hégélien, réussite par le travail, etc.

L’enjeu de l’IA serait donc de pouvoir découvrir désormais, un peu automatiquement, parcequ’elle apprend des données, ce qui fait texte.

Une question théorisée depuis longtemps depuis les linguistes. Le syntacticien sait son objet, le phonologue a le phonème, le morphologue sait son objet. Mais quelle est l’unité, la forme minimale du linguiste de corpus ? La forme graphique souvent efficace, mais aussi des locutions, etc. Que serait un « textème » ? 

Saussure qui théorise cette question, toute son œuvre se heurte au syntagme. Déclaration des droits de l’homme de 1789. Déclaration ? Déclaration des droits ? Déclaraction des droits de l’homme ? oui mais de 1789.

« énigme insoluble » Legallois 2006

Qu’est-ce qui fait sens ?

L’intelligence artificielle va être performante pour bien caractériser le discours de certains auteurs. La seule question, sur quoi l’IA s’est-elle appuyée pour comprendre ce que dit Macron.

Travail de Lebart qui montre que l’IA reconnaît facilement Romain Garry dans Émile Agard, mieux que le Goncourt !

Bravo, reconnaît que très performant, mais comment a-t-on fait pour reconnaître ? La réponse que l’on va avoir, celle de motifs, des unités linguistiques, observables linguistiques. Des unités souvent dissipatives (Rastier) se transforme au cours de la lecture. Ces unités sont beaucoup moins stable que ce que la littérature linguistique définit habituellement : le mot, le lemme. Quand lit un texte pas seulement ça. 

L’unité pertinente du texte

mots, lemme, segments répétés, grandeurs textuelels, marqueurs, unités textuelles, patterns, n-grams, expression régulières, segments textuels, unités phraséologiques, lexies simples, séquences, paragraphes, coocurrences, passages, isotopie, routines, formules, collocations, textèmes.

La notion clef la plus avancée mais aussi la plus compliquée sans doute celle des "passages", dans la Revue Corpus, ou dans le Mesure et le grain. Des moments du texte qui n’ont pas forcément une définition grammaticale, mais des moments du texte qui à un moment font sens.

L’idée serait que l’IA choppe, attrape et objective ces objets qui font texte pour les extraire.

### Précautions sur la posture

Non pas néo-positiviste mais herméneutique

Croyance régulière des hommes sur les techniques, surtout en SHS, où croit qu’une machine va nous dire la vérité. Pas de vérité sociale qui soit, toujours dans une représentation du monde et des choses.

Pas de vérité absolue du texte, c’est à nous de l’interpréter.

**Conviction que l’IA sera une herméneutique ou ne sera pas. Elle-même une interprétation du monde, une interprétation, une représentation du monde.** Avec cette posture chevillée au corps si pas là pour produire de la vérité mais produire une interprétation, sans pour autant renoncer à l’objectivation de la preuve.

François Rastier parle de sémantique interprétative.

- Le texte/le sens n’est pas un donné mais un interprété (Rastier)
  Pas un sens donné dans un texte, mais une interprétation donnée à un objet matériel. Ecco qui dit que tout est interprétation, mais que pour qu’îl y ait interprétation, il faut qu’il y ait quelque chose à interpréter. Une grosse concession pour un tel herméneute (aux limites de l’interprétation).
  Le sens n’est pas déjà là. Il s’agit d’extraire des chevilles interpétatives à partir desquelles va pouvoir articuler notre savoir d’historien.

- le numérique est lui-même est une représentation/interprétation du monde et des données (Douehi)
  DH, épistémologique.

- Une sortie-machine n’est pas une preuve mais un interprétable ; n’est pas la fin de l’analyse mais son commencement
  Commun à la lexicométrie et à l’IA. Quand on a une sortie machine, d’abord un point d’interrogation. Pas la fin de l’analyse mais son commencement.

Les collègues qui font de l’IA se targuent d’avoir un système bio-inspiré. Des neurones qui collectent l’information un peu à la manière des synapses. De mots et des relations entre ces mots qui produiraient du sens, de l’intelligence. Fait donc penser à la coocurence. 

Laurent Vanni dit finalement rien fait d’autre que ce que l’on fait et qui marche très très bien sur les images.

http://bcl.unice.fr/membres/Laurent.Vanni/

Va retrouver des enchaînement, des combinaisons de pixels qui va lui permettre de reconnaître le chat versus le chien. Regarder toutes les combinaisons, les séries de pixels pour identifier que bien Macron. En zoom, en flouté, regarder que le nez. Faire défiler des fenêtres convolutionnelles. Série de filtres qui sont passer pour que sans a priori, identifie des éléments de décriptage.

Ce que fait, la même chose, non pas sur le pixel mais sur le mot. Idée que le texte de Macron qui nous intéresse. On va donc essayer de décripter le texte en appliquant des filtres pour repérer des enchaînement de lettres. Pondération des pronoms, etc. pronoms relatifs, combinaison pro- avec proton relatif. Est-ce que cela marche.

Le début de l’IA assez proche de la lexicométrie. Si vous voulez reconnaître avec un filtre grosser toute la littérature, ramenez tout au nom propre. Il peut s’agir d’un filtre très pertinent pour décrire un roman. 

On voudrait que l’IA nous apporte une plus-value d’interprétation mais précisément va devoir cacher les informations les plus grossières pour pouvoir aller chercher d’autres moyens pour le déterminer. Différence entre un chat et un chien, mais sans parler des moustaches...

Comme plus sur une base d’apprentissage, moins dans une logique d’émergence. Va apprendre mais à partir de quoi. Filtres tout azimuts pour faire resortir ce qui est pertinent.

### Comment on s’y prend ?

Proposition d’un modèle simple, propositionnel, pour l’intertexte.

L’idée c’est qu’on a les mots, "une transformation en profondeur de notre pays" notre texte linéaire un texte de Macron. Sur ce texte, prendre le premier mot et fait une opération très connue en text-mining que l’on appelle l’embeding. Il représente les mots et chaque mot est doté d’une représentation numérique.

Sur ce texte, on va opérer une convolution. On va regarder sur ce texte, en faisant circuler des fenêtres pour essayer de voir si certaines suites ne sont pas chargées d’informations pour caractériser Macron.

Beaucoup d’opérations qui s’opèrent pour trouver dans la structure du tableau quelque chose qui peut permettre de reconnaître que du Macron.

Idée que puisse classifier les textes.

Va lui demander de faire tous les essais possibles. Ici que supervisé. Ce que en anglais appelle une prédition. Nous dirions plutôt classification.

Ici l’apprentissage parce que l’on va lui demander de refaire tous les parcours possibles pour arriver à la conclusion.

Également back-propagation. Où change quelquechose. Là que la plusvalue.

Le vrai vérou scientifique est de passer de cette prédiction qui est forte et qui n’est plus à démontrer, à une description. Passer à une IA descriptive et pas seulement prédictive. Dire comment cela a fonctionné. L’enjeu c’est de faire le chemin inverse. Maintenant que sait dit moi comment.

Passer de la convolution à la décovolution. Retour aux textes et aux unités. Cf. article 2018 à Sydney.

Richesse qui est de réfléchir à la complémentarité du séquentiel et du fréquentiel. La textométrie est principalement fréquentielle. Ici un petit changement, la répétition d’une structure va changer mais arrive à un peu plus de qualitatif, et va un peu plus regarder l’axe syntagmatique (la fenêtre coulissante) que l’axe paradigmatique. Donc une complémentarité qui peut être intéressante.

L’acte premier de tous les logiciels est la tokenisation. La segmentation. On découpe le texte en unités minimales que l’on va se mettre à compter et qui aboutit à l’index des formes, le dictionnaire de fréquence. Tournez comme vous voulez, le premier acte est de tokeniser.

En Analyse de données textuelles ADT, une des conséquences de cette tokenisation dans uns phrase comme « je pense donc je suis » les deux je vont avoir la même représentation. Va les mettre dans la base de fréquence. Voit les limites, par exemple avec les homographes. Mais seulement une caricature du problème car on peut penser que l’homographe peut potentiellement avoir un sens différent au fil d’un texte ou d’un corpus.

Saussure a dit dans une conférences où quelqu’un s’écrit, Messieurs, Messieurs, je vous dit que... que les deux messieurs ne sont pas identiques. Très clair dans les débats contradictoires ou politiques où avance sur une orientation doxale.

En lexicométrie, prix par ça. Représentation unique. Postule que je = je. En AI avec une fenêtre, la considération de ce je là sera distinct. Alors ne sera pas doté du même embeding. Un nouveau mot. On peut dans les deux cas supposer que le je sera suivi d’un verbe. Pas étrangers l’un à l’autre. 

Au fond ce phénomènes nécessaire de contextualisation que l’on fait un peu de manière secondaire en textométrie, se retrouve à la matrice. Les deux je qui le départ sont considéré de manière différente, porte en lui dès le départ son contexte. Choisir 3, 6 ? 6 permet de ne pas être trop rapproché de la syntaxe et permet d’aller vers un contexte sémantique.

Séquentiel complémentaire de fréquentiel.

Comme en lexicométrie, on va pouvoir compter mais la séquentialité a permis de contextualiser le mot. On est à la fois dans le paradigmatique et le syntagmatique. Or Saussure a fondé la linguistique sur cette distinction. Touche les deux principes fondamentaux de la linguistique choix paradigmatique et axe syntagmatique.

Nos modèles sont un peu plus complexe qu’ils n’y paraissent. Sont multi-niveaux pour analyser le maximum d’informations linguistiques pour les analyses. Comme le texte est lemmatisé et étiqueté morpho-syntaxique. Donc chaque mot plusieurs représentations (lemme et catégorie-morphosyntaxique). Il y a donc trois représentations du mots à trois niveaux linguistiques.

La machine va s’emparer de cette séquence soir dans l’un des axes ou l’autre.

Machine qui surligne les observables linguistiques qui lui ont permis de reconnaître que ce texte était très macronien. Multichannel, donc plusieurs niveaux. Allume la forme graphique, si nécessair ele Lemme et le code grammatical.

Concentré macronien, parler des acteurs européens sous condition de nous projeter au futur, etc. Acteurs pour éviter lutte des classes, rivalités politiques. Suffixes en -tion. Pour mettre en marche un mot, ajoute le suffixe -tion processus de mettre en marche un mur, construction. Transformation son mot préféré.

Exhibition de nouveaux observables linguistiques (cf. Rastier) qui font du texte à un moment donné. Dans une posture de contextualisation initiale. Peut-être le même mot, mais dans un contexte différent ne sera pas qualifié de macronnien.

J’aime la France, je déteste la France. Dans les deux cas occurence du mot France. Si le dit souvent peut croire que dit même chose. Ici dans l’idée de IA comme contexte immédiat nuance que conserve.

Ces motifs prétendent rendre compte de la langue de Macron. Démonstratif, cette transformation qui n’est jamais définie dans son discours.

Ici comme en lexicométrie, comme si avait allumé la spécificité.

### Répondre à l’intertexte

On peut essayer de voir dans un texte donné, chez Macron de voir s’il n’y a pas l’influence d’autres personnes. Au moment où l’IA apprend du Macron, IA affiche des phrases qu’elle considère très macronienne.

Mais il y a aussi des échecs. Phrase que trouve être du Gisacards. Soit considère qu’un écchec, soit considère que taux d’accuraci suffisamment important pour dire que reconnaît bien les chats et les chiens. Alors si texte Pompidou, alors là peut dire qu’il y a une emprunte pompidolienne. Un peu comme du plagiat. Si la machine reconnaît très bien. Phrase très nominale. Agrégé lettres classique, normalienne, virgule adjectif, etc. Vision avenir, talent, ambition, grande croyance des trente gloriuse pétrie de culture classique.

C’est sur une soi-disant erreur de la machine que s’est mis à croire à l’intérêt de l’IA. Macron qui dit aux vœux de 2018, on ne peut pas travailler moins pour gagner plus. IA me dit du Sarkozy, non du Macron. Mais pas bête comme erreur. En effet, le taux d’emprunt de phrases de Macron à ses prédecesseurs Sarkozy. 7% phrases "empruntés" (avec une emprunte).

Fait beaucoup de tests, et repère bien les emprunts revendiqués. Nous a confirmé dans le fait que marchait. Mais attention pb de l’intertextualité et de l’apprentissage. Intertexte qu’il repère est conditionné au fait qu’au départ lui ait appris ce qu’était du Pompidou.

Mon idée, c’est que le corpus en lexicométrie comme en IA est une objectivation de l’intertexte. Si met ensemble les corpus dans un corpus de travail (corpus réflexif) c’est que l’on considère que l’ensemble fait sens. Cf. Rastier qui parle du principe d’architextualité où considère que tout texte plongé dans un corpus en reçoit des déterminations.

Barthes et tous les autres incapables de dire ce qu’est l’interjette mis à part l’érudition. Un peu des notions floues que nous implémente. Évidemment beau jeu de dire que limité.

Blanc. Discours rapporté, citation implicite, théorie sur le dialogisme, polyphonie d’un discours. Si arriev à l’objectiver serait bien.

Spenberg sur le monde des idées

Correction : Back propagation pour classer texte en arrière. Une fois que l’on a classé le texte deux. Perpective : vers une implémentation du cerle herméneutique ? Rêvons un peu. Permet de faire le retour vers les données.

Correspond à l’apprentissage.

N’a pas du tout parlé de logiciels de clusterisation ou de réseau comme Gephi. Ici tous les mots qui ont un lien au contexte de coocurence. Logiciels très forts car montrent qu’il y a un lien coocurenciel évident dans pouvoir-d’achat mais en classant ensuite les mots dans leur champ lexical.

Matrice coocurentielle. Le sens d’un mot qui coocure avec autre. Mais ensemble de cette matrice, c’est finalement votre texte. Alors renoue assez intimement avec l’étymologie du texte tissus, ou tissage.

Exemple

corpus apprentissage sur les différents présidents de la République, peut soumettre un nouveau texte.





Filtres et lemmatisation



