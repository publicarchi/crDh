---
date: 2016-11-16
tags: cr, conference, crihn, Bruno Bachimont
---

# Conférence de Bruno Bachimont, 16 novembre 2016

UdeM, CRIHN et Chaire de recherche sur les écritures numériques

Bruno Bachimont est docteur en informatique et en épistémologie. Il associe une approche philosophique à une connaissance profonde de la technique en tant que telle. Il est actuellement directeur de la recherche à l’université de Compiègne. Parmi ses ouvrages, notamment le Sens de la technique, le numérique et les calculs, chez Encre Marine en 2010.

## De l’épistémologie de la mesure à celle de la donnée : un nouveau nominalisme pour les sciences de la culture 

évoquer avec vous des réflexions sur le phénomènes du big data et l’ère de la données. à son sens des questions épistémologiques nouvelles qui se posent. Au-delà des promesses que l’on nous fait des questions épistémologiques et philosophiques à aborder. 

Les Big data, ça Trump énormément… malgré toutes les promesses liées à une avalanche de donnée,s pourtant pas été en mesure de prédire le résultat de l’élection.

- une mutation annoncée et ses questions
- une triple phénoméno-technie (collecte, …) et autant de problèmes
- Un nouveau paradigme : le nominalisme du social
  aborder la question sous un angle nominalisme. Nominalisme qui au 14e s. a permis de replacer la question du rapport entre la science et la nature de manière différente. Un nouveau déplacement de paradigme en train de se tramer. Un nominalisme du social.


- Des enjeux restant à relever
  - le fait humain est-il une donnée ?
  - les résultats sont-ils intelligibles ? nouvelle dimension du paradoxe de Memnon réactivé, une anthropologie à construire en conséquence.
- Construire une épistémologie adossée à une anthropologie

## Une révolution annoncée, les “ big data”

Les caractéristiques souvent associées au Big data peuvent être ramenées à trois grandes propriétés sans être exclusive.

D’abord leur masse (elles sont big) mais pas seulement issues de la massification de l’information. Elle est issue de deux autres facteurs complémentaires.

- la dynamicité, on s’intéresse à des données qui sont produites de manière différente, cycliquement, régulièrement de manière "spontanée" avec des systèmes de captation branchés sur le réel
- hétérogénéité. Les big data nous ont montré que le numérique permettait de réunir dans une même homogénéité du calcul des données qui peuvent être de nature très différente. Le numérique nous permet ainsi de faire une synthèse de l’hétérogène : poser ensemble des choses qui n’ont rien à voir. Facile de réunir par exemple des sons et des textes. Simplement à rapprocher des codes de calcul. 

Le big data enfants des cette dynamique. En résulte une approche nouvelle.

Tout n’est pas big data, ce n’est pas parce que l’on a beaucoup de choses, qu’on a des big data, mais lorsque l’on réunit ces trois facteurs que l’on a des big data. Commence lorsqu’il y a une rupture du fait de la masse. Le big peut apparaître très tôt, apparaissent lorsque la masse implique un décrochage et de confier à la machine les opérations. Dégager des propriétés seulement visibles par la masse.

Les 4V, slogans souvent associés au big data. Le V comme Volume, V comme vélocité, le V comme variété (hétérogénéité), et enfin un V comme véracité. Consiste à dire que les données n’ont pas forcément besoin d’être véridique, mais leur tout qui fait sens.

www.datasciencecentral.com/profiles/blogs/data-veracity

Pour cela que peut à voir avec le linked data, où des faits bien identifiés par des liens. Les big data des infra enregistrement, où les données pas de sens en tant que tel. Le sens vient de la globalité pas de la localité. Des données qui possèdent une sémantique depuis la masse et pas d’elles-mêmes.

Techniciens des big data alors confiants à l’égard de la données, car le sens qui vient de la masse.

Google Flu, observation des données correspondant à la propagation des données récoltées sur les réseaux sociaux, et données épidémiologiques standards. Article qui présentait une corrélation quasiment absolue entre ce que pouvait prédire en regardant directement les objets ou la circulation de l’information. Observation de la circulation de l’information presque aussi fiable. Mais en réalité pas un observatoire de la maladie mais de la rumeur. Raison pour laquelle décalage entre ce que peut observer directement et information. Néanmoins fortement marqué les esprits.

Indépendamment de l’intérêt de l’étude, important car permet d’envisager un déplacement de la pratique scientifique.

- Nouveau paradigme scientifique pour les SHS pour Manovich. = cultural analytics, approches par les données qui permettent d’aborder différemment l’approche du social. Possibilité de revenir à l’analytique. Cependant l’analytique du calcul n’est pas l’analytique de la preuve. Pas parce que l’on calcule que l’on est capable d’administrer la preuve. Mais une ouverture sur la manière d’aborder la critique du social.
- Effacement de la pratique savante critique au profit d’un empirisme directement au contact de la données. Chris Anderson. Applique une heuristique connue dans le monde scientifique : slogan modèle X données = constantes. Plus dispose de données plus puissance. Dans les années 70, travail sur des modèles linguistiques forts car travaillait sur peu de données. Avec arrivée de Google s’aperçoit que beaucoup de modèles ne dispose pas de connaissance linguistique et seulement modèles de Markov, etc. lié à massification. Le monde que l’on a dans les données, donc pas besoin de théorie = un empirisme radical qui permet de se passer de théorie du fait de la massification.
- Ancrage objectif de la décision. Rationnel parce que calculatoire de la prise de décision. La délibération ne fait plus partie des nécessités du social. Lorsqu’il y a des démonstration, plus besoin de délibération. Artisote éthique à Nicomaque.

Cultural analytics

terme proposé par Lev Manovitch, plusieurs étapes : collectes des données, analyses statistiques, visualisations interactives. Données —> Analyse —> réduction

masse qui fait que ne peut voir qu’à travers des réductions pour l’analyse.

Ex. LinkFluence.net, sur les politicospère juin 2009. L’idée étant de pouvoir observer globalement en ayant recours à la masse grace à ces outils. Ici voit sites républicains davantage reliés à des sites de production d’armement. Intensité réciproque des relation.

Construction de représentation synoptique destinée à comprendre la nature des données contenues.

Un problème à part entière de savoir comment va représenter la complexité. Trouver des métaphores ou des formes graphiques destinées à rendre compte des résultats des ces analyses complexes.

Visualcomplexity.com

Dans la suite de Anderson, des courants qui revendiquent cette radicalisation. Pas tant l’effacement de la science mais l’émergence d’une nouvelle science. Cf. article suite conférence de Jim Gray, The Fourth Paradigm, Data Intensive Scientfiic Discovery ! en outre disparu après la conférence.

https://blogs.msdn.microsoft.com/escience/2009/10/16/the-fourth-paradigm-data-intensive-scientific-discovery-book-released/

- association empirique à des phénomènes
- science moderne avec loi pour comprendre le monde
- calcul et simulation, permettant expérimenter phénomènes naturels pour pouvoir les expérimenter
- les données

Une distinction entre la simulation du calcul scientique et les données. On n’est pas dans la simulation de lois déjà connue au préalable, mais dans l’extraction de connaissance et de lois qui ne sont pas connues auparavant. Il n’y a pas d’hypothèses sur les connaissances qui permettent d’exploiter ces données. On considère que les corrélations qui vont permettre de faire ces relations.

Distingue une science du passé, computationnelle. Et science du futur qui va travailler avec des données et donner lieu à l’émergence du savoir. Idée qu’en partant des donnés brutes va pouvoir reconstruire toute l’information qui nous permettra de reconstruire le monde et l’étudier. 

Un rêve gigantomachique car la données non seulement un outils qui nous permet d’étudier le monde, mais la données  présente une exhaustivité, on dispose des données complètes, et la carte va remplacer le territoire. Idée que les données sont isomorphiques au monde.

Or tradition scientifique habituelle a toujours fait une différence entre les modèles et la réalité, c’est à dire la carte et le territoire. Mais la justement dans les modèles pas de données mais de la mesure.



Trois problèmes, deux enjeux.

D’un côté les données et la collecte, puis le traitement et enfin leur visualisation. cf. Manovich.

Lien abritraire, conventionnel, produit par l’acte de collecte de la donnée. | donnée - mesure

Un arbitraire de l’interprétation. Dès lors que des big data, la sémiotique la plus sauvage qui permet d’interpréter les résultats. | perception - langage

Paradoxalement, très grande sophistication de la collecte des données, et un arbitraire grossier de la représentation. Les données ne sont pas des mesures. La perception que l’on a des visualisations proposées repose sur une sémiotique du visuel qui ne présente pas l’expertise critique du textuel constitué dans le temps.

Ces trois étapes décrites par Manovich correspondent peu ou prou à trois déplacements. Ici que reprend l’idée de Bachelard d’une phénoménaux-technique. Idée que les phénomènes sont construits. 

Construction des données. Les données sont construites, il y a des choix arbitraire de la collecte. Il y a un déplacement considérable de l’arbitraire du format, de la collecte.

Captation et traitement des données. Les outils de collecte des données sélectionnent, transforment, formatent les données lors de leur captation.

Présentation des analyses effectuées. Des algorithmes de représentation dans l’espace des données. Plus résultat de choix arbitraires d’un programmeur plutôt que des propriétés intrinsèques des données. Des algorithmes de placement épais complet, donc des choix arbitraires possibles. Peut donc avoir des biais, des artefacts qui altèrent l’interprétation sémantique de l’interprétation. Ce n’est pas parce que deux points sont proches que sont proches sémantiquement, cela peut être pour les faire simplement tenir sur l’écran.



La question de la collecte

Il y a un mythe de la donnée brute. Les données semblent être "données" car elle sont trouvées. Celui qui collecte n’est pas celui qui a construit des données. 

Les données sont construites comme artefacta c’est une construction technique par rapport à un fait social. 

Data comme capot, elles sont transformées par leur captation (les textes deviennent des sacs de mots).

Idée que les documents sont des ressources. Captation qui va consister à casser le lien entre le document et la donnée, rendre exploitable le code lui-même. Choix pour optimiser la captation du codage. Perte de l’hétérogénéité ou pertinence et caractéristique sémantique intrinsèque.



La question du traitement 

Des réseaux neuronaux, systèmes multicouche. mais personne ne sait réellement ce qu’il se passe dans les couches basses du système. Fonctionne bien, mais s’aperçoit que les couches indifférentiable, inintelligible pour l’homme. On a bel et bien une approche analytique, mais inintelligible.

Incapable de dire en quoi le calcul qui a produit le résultat une preuve et pas un résultat de production.

Pas nouveau, cf. calcul infinétisimal. Mais comme pas d’élément de référence. Les fictions restent des fictions, les imaginaires restent des imaginaires.



La question de la rstitutution

Nous autres simples mortels sommes des animaux sémiotiques. Surdoués pour donner du sens à tout ce que l’on voit. Cf. François Rastier. Mais ne sait jamais si résultat.

Souvent ce qui motive la représentation des données, vient de sémiotique empruntée par ailleurs.

Ex. piliers de la création. énergie qui donne lieu à la création d’étoiles. Intéressant dans ce cliché, qu’évidemment pas une photo. Un télescope spatial ne prend pas de photos. Des algorithmes de traitements qui vont produire l’image. Or intéressant de voir que des stéréotypes culturels des mondes nouveaux, des mondes en création. Cf. Nouveau monde, falaises de la rivière du Colorado Thomas Moran.



Des constats sévères

Des données construites, des calculs inintelligibles, des résultats fantasmes.

Des constats sévères mais liés au fait que prend au sérieux les gens du big data qui disent que font de la science comme avant mais avec de nouveaux moyens. Les anciens réponses ne conviennent pas à ces nouvelles approches. Si révolution il y a, va devoir envisager nouvelles pratiques différemment.



épistémologie des données, un nouveau paradigme ?

Comprendre le mythe, mais aussi savoir comment les aborder.



Origines hétérogènes, traitement homogène. 

La force du paradigme de la données : permet de considérer des données de nature et d’origine hétérogène pour les unifier dans un format unique permettant un traitement homogène. On élargit le champ du possible, ou du pensable de même que l’écriture avait permis cet élargissement cf. Goody.

Ici un nouvel outil synthétique. Ne veut pas dire que d’emblée produit du sens. l‘écriture n’a pas immédiatement produit la grammaire mais condition de l’émergence de la grammaire.

Nécessité de sortir du paradigme de la mesure. Théories qui permettent d’avoir une théorie de la mesure, les mêmes que celle qui permet d’analyser les résultats. Il y a donc une intelligibilité continue du phénomène depuis son début jusqu’à son analyse finale. Pris beaucoup de temps à construire, mais sait que quand fait une analyse expérimentale, on dispose d’un appareil théorique homogène. Une homogénéité entre la théorie et la pratique.

Avec le big data. Plus de continuité épistémique. triple rupture, entre les outils qui servent à collecter et analyser, et ceux qui servent à analyser et représenter. raison pour lequel trouble. Ne sait jamais de quoi en parle ni ce que l’on en dit.

On prétend savoir de quoi on parle car voudrait bien que les données soit les données de quelque chose. Ne peut pas s’en sortir avec rmq de Russel sur les mathématiques, car ici voudrait que parle du réel.

Un vrai chantier, pour donner un statut de connaissance à ce que l’on fait avec nos informations.



Proposition, déplacement de la donnée qui permet d’aborder un nouveau domaine du réel qui jusque là résistait au calcul, le social.

Pour cela qu’un nouveau nominalisme. Cf. Roseline de Compiègne, les mots sont des flatus lens vocales.

Guillaume d’Ockham, un philosophe qui a révolutionné le monde philo et scientifique du 14e car a posé un nouveau cadre.

cf. travaux Hans Gonas ? et Ruprich paquet ? sur le nominalisme médiéval.

Sortait du réalisme médiéval où la structure syntaxique du langage reflète l’ontologie du réel. Les relations entre les mots renvoient à des relation entre les essences. Réels, res (essences). D’où le trivium. Comprendre le langage, c’est comprendre le réel.

Pour les nominalistes, le langage pas le reflet du monde. Mais un signe du monde. Suppositions simples, etc. ruine des essences.

Révolution qui consiste à détruire le lien qui existe entre le langage et le monde. Va permettre de construire un nouveau lien entre le langage et le monde, celui du calcul scientifique et le monde. Pour que Gallilé soit possible, il fallait avoir Ockham avant.

La relation du mot à la chose n’est plus constitutif de la connaissance de la chose, qui doit donc se constituer autrement.

Le langage devient une donnée, un fait, comprendre le monde, consiste donc à analyser computationnellement le langage comme information. La donnée, ce qui permet d’abolir le lien entre le langage et le fait social pour permettre d’aborder d’une nouvelle manière le fait social.

Cette donnée on l’a.



Seconde révolution nominaliste. Constitution de grandes bases de donnéeS. Massification qui rend les données inaccessible à la conscience humaine, va devoir construire des outils de lectures distantes qui vont pouvoir les lire pour nous. 

Cette révolution nominaliste est donc d’une certaine manière en partie en acte. 2 millions d’heures numérisées à l’INA, YouTube, etc. Bases de données, Open Data.

Une révolution nominaliste supplémentaire qui renouvellerait le geste d’OCk vers les sciences de la nature, vers les sciences de la culture.



On retoruve nos deux enjeux.

Le rapport au fait humain, critique du calcul

Le rapport à la complexité, le paradoxe du Ménon



Au fondement du social et du culturel

L’idée des sciences sociales en général, d’analyser le fait social mais plus précisément le fait humain (Marc Bloch). Un fait, et humain. Un fait humain qui est arrivé des humains ce qui suppose une empathie interprétative Ginsburg ou Dilthey qui fait que peut comprendre ce fait car aurait pu nous arriver. Ce qui fait que peut en comprendre aussi la différence. Pour expliquer la différence, il faut une identité partagée. Il n’y a pas de différence absolue autrement dit.

Différenence qui ne peut être différence que si un fait commun qui permet de l’appréhender. Pour être intelligible, fait le postultat qu’il est interprétable. D’où complexité sur l’analyse historique de la Shoa.

En même temps, veut que cela soit des faits, un objectif de véricité. Verne, un roman vrai. Un régime de contrôle interprétatif, contrôle empirique. Fiction du fait humain rapporté à un contrôle qui permet de passer du fait à sa reconstruction.

Dans l’optique du calcul, la difficulté que va avoir, c'est que perte du fait humain comme fait car va l’effacer.



La complexité des résultats

Retours aux beaux calculs. Retours aux belles figures

Revisite à nouveau frais, de manière sophistiquée le paradoxe du Ménon (Platon). Demande toujours au gens des big data de savoir ce que nous a permis d’apprendre que ne connaissait pas déjà.

Question de la connaissance. Paradoxe par lequel commence le dialogue que connaissance impossible. car si aborde qqch de nouveau, ne le reconnait pas. Et si le connaît déjà, alors rien à apprendre.

Pour le moment l’exploitation que l’on fait, confirmer ce que l’on a deviné par d’autres moyens ou de reconnaître ce que l’on sait déjà.

Un problème de fond, car le problème de la complexité. Les données étant tellement complexe que ne peut les aborder directement. Il faut réduire la difficulté. Ce que propose le big data, c'est la réduction. Expertise des acteurs du big data, travail d’accompagnement ou font accoucher les client pour reconnaître dans les données les connaissances qu’ils possédaient déjà. C'est donc une sorte de maïeutique.



## Conclusion

à la fois très sérieux, et une mystification.

Les big data reconfigurent notre rapport au monde, le fait social comme donnée (nominalisme) [en tous les cas un chantier épidémique majeur pour comprendre comment déléguer à ces outils]

Reconfigurent notre rapport au savoir, la donnée en lieu et place de la mesure.

Mais surmonter, la mythologie de l’interprétation graphique, l’inintelligibilité de son fonctionnement. Donc doit pouvoir construire une critique raisonnée pour donner du sens au calcul et à ces visualisations.

Mais aussi un déplacemement plus profond de nos perspectives entre le social et l’humain.

Kant révolution de la mesure. 

Que puis-je savoir ? science newtonnienne

que dois-je faire ? morale

Que m’est-il permis d’epsérer ? la doctrine de la religion

Qu’est-ce que l’homme ? L’humain est)il mesurable



Pour la révolution de la donnée. épistémologie de la donnée, critique pour interpréter les calculs et leurs résultats.

Une éthique de la donnée, déontologie des big data.

Quel sens de l’humain reste-t-il (cf. transhumanisme)

L’humain est-il calculable ?



Hilbert sur l’infini, il ne faut pas que l’on nous chasse du paradis que Cantor a créé pour nous.

XXIe ne pas nous chasser du paradis que les big data pourraient créer pour nous, suppose pouvoir disposer d’une anthropologie assumée.

## Discussion

Idée rupture entre la donnée et le réel.

La même chez les nominalistes, car sont dans un paradigme de la représentation. Si fait un coup à la Hegel en disant que pas de rapport entre les deux et que le réel, c’est ça. Alors la question plus celle du réel.

Un enjeu fondamental pas de comprendre le monde mais de contrôler le monde. Prophétie autoréalisatrice. Plus un isomorphisme mais une appartenance à la même sphère.

R Il y a effectivement un déplacement, la donnée n’est plus une représentation du monde. Donc ta proposition entre bien dans mon discours car ce qu’il faut envisager. Mais le problème se pose quand a une exigence de véridicité scientifique.

Prédire ce n’est pas expliqué. Dès lors même plus du pilotage, car ne peut que constater a posteriori, et pas diriger. Le pilotage dans la métaphore aristotélicienne sait où veut aller. En revient à la non décision, non pilotage, car délègue à l’incompréhensible. Ne veut pas dire que ne doit pas utiliser le calcul, mais considérer que pas outil de la décision. Par définition le calcul ne décide de rien.



Données génétiques, clair que pas des représentation mais des entités qui ont un mode de représentation différentes de l’analogique. Léotard de l’inhumain dit est-ce que la question essentiel l’ici et maintenant. Ordinateur. Sur le paradoxe de Ménon, je ne sais quoi et presque rien.

R je ne sais quoi, différence entre la preuve et le calcul. L’ordinateur le monde de la conséquence automatique. Donc lui arrive bien quelque chose mais jamais ni l’auteur, ni le spectateur, ni l’acteur de ce qui lui arrive. Paradoxe, en déléguant la décision et en remettant à l’ordinateur. 

La philosophie de la connaissance pouvoir penser i



Michoulan. Critique et déconstruction. Ménon et théorie de la connaissance question illumination du passé.

Actuellement dit plutôt que n’y est pas encore plutôt que de dire, circuler il n’y a rien à voir.

Théorie de la réminiscence de Platon, c'est qu’il y a un travail. Cf. Phèdre, le Big data peut-être un outil pour la réminescence, mais pas pour autant un outil de production de la connaissance en tant que tel. 



