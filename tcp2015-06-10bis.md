# tcp 2015-06-10bis
Atelier Big data

https://mensuel.framapad.org/p/c0RWK9EwdC

Norme CTS et Cite pour la citation de texte

Présentation rapide de son intérêt pour le sujet en terme d’illustration.
SyMoGIH tentative de mise en place d’une base de données, publication des données dans une application commune. On va donc dire que l’on se place dans une logique de big data. L’idée est que les données sont des octets plus ou moins structurés à plusieurs niveaux. Comment les employer et aller les chercher dans la base, par exemple pour les étudier avec air. En réalité, quand on fait ce travail on ne connaît pas forcément les données publiées par d’autre. Il convient d’explorer au préalable les données.
Les données entrées dans la base, pointent pour les personnes vers des références vers IDref. Projet en phase alpha de créer un point d’accès SPARQL pour donner accès aux données, ouvrir ces entrepôts fermés pour les mettre en circulation.

On nous dit que modifie la manière de voir le monde.
Des perspectives de grandeur (stockage, ampleur, etc.)
Phénomènes des 3V volume, variété de sources, vitesse (changement en temps réel)
Articulation avec l’informatique décisionnelle qui utilise la statistique descriptive pour analyser des phénomènes basé sur un modèle du monde
Dans le phénomène big data, utilisation de la statistique inférentielle données à densité en information dont le grand volume permet d’inférer des lois... --> les mathématiques sont chargées de trouver un modèle dans les données
Système expert (intelligence artificielle) ; aide à la décision à partir de la fouille et de l’analyse des données.

Pour pouvoir faire quelque chose de ces données, il y a une demande de requalification des données autour d’un cahier des charges assez stricte.
- ouverture des données, réutilisabilité
- décrire des aspects propres à une discipline (modèles de métadonnées)
- représentation de la provenance des données
- représenter des informations contextuelles
- incertitude
- qualité des données
Vaste programme très intéressant et très difficile.

Texte dans lequel trouve des mentions d’acteurs ou d’entités que pourrait vouloir extraire. On pourrait utiliser la TEI pour faire cela.
Une fois que des entités-nommées sont identifiées, on souhaiterait les mettre en relation avec des référentiels. Autrement dit relier nos données avec celles d’autres producteurs.
Comment extraire des connaissances : production d’un tableur dans lequel place des données structurées. Peut également exprimer la même chose à l’aide de graph, en pointant vers un système de référence grâce à des identifiants.
Enfin, on peut encore produire une ontologie pour donner de la sémantique à ces structures.
On peut ensuite extraire ces données et les visualiser pour les analyser en les présentant en modèle de graph.

Se pose alors la question de savoir comment fait-on pour donner aux données extraite une structure identifiable.
- KCL Factoïd onto historique King’s college
- CIDOC-CRM issu du monde bibliothécaire
- Simple Event Model SEM modèles génériques issus de la volonté de rendre interopérable les données
- Symogih

dataBnf, Ministère de la culture,...
Isidore, Biblio it.
Ressources que peut analyser

Illustrer certaines étapes dans ce processus de prise de conscience de l’existence de données différentes et de mise en relation.
- csc que dispose de textes, d’images pour lesquels va disposer d’outils pour faire de la fouille (textométrie, classement par sujet)
- problématique d’indexation de textes d’images : identification d’entitées nommées, concepts, etc.
- Une fois qu’a retrouvé tous ces lieux : Alignement d’entités nommées et concepts avec notices d’autorité des bib, vocabulaires, taxonomies, et thesaurus qui structure conceptualisation d’un domaine, etc.
- Extraction de connaissances (relations entre entités nommées, concepts, dates, etc.) --> données semi-structurées
- Stockage dans bases de données relationnelles ou enrtepôts RDF. Une fois qu’a extrait ces connaissances où les stocker pour les rendre accessibles à d’autres chercheurs en les munissant de tous les éléments nécessaires (provenance, métadonnées, etc.). Condition de l’utilisation pour la recherche.
- Problématique de la création de centres de données, avec problématique de curation de contenu.
- fouille de données, systèmes experts, validation de la connaissance par les chercheurs-experts de domaine. Ce que l’on peut faire avec ces données en terme de production semi-automatique de al connaissance.


## Projet Rekall
comment conserver les œuvres numériques
Re-Collection, Art, New Media, and Social Media
http://re-collection.net
Richard Rinehart and Jon Ippolito

Presque tous les spectacles sont confrontés aux technologies numériques à moins d’avoir un plateau nu.
Logiciel pour musique, système de partition inventé.
Et conduite de régie.

Problématique de conservation. Comment passer de la création à la tournée. Comment fait-on pour que les œuvres continuent à fonctionner avec de nouvelles technologies. Cf. Nam Jun Paik avec moniteurs qui ne sont plus fabriqués : met-on a jour l’œuvre ou conserve œuvre et devient la trace de l’œuvre et ne voit plus la vidéo...

Plusieurs solutions imaginées comme le stockage, l’émulation, la migration et la réinterprétation. Pb authenticité.
Cycle techno 4 ans. Ensemble des œuvres de l’art de la scène confrontées à l’obsolescence technologique.
Nombreux documents nativement numériques. Archives de la création. Ici confrontés problèmes de BigData.

Comment mettre à jour les technologiques d’une œuvre au fur et à mesure de son évolution dans le temps.

Chargement des documents dans un fichier
Extraction automatique des documents et possibilités de modification des métadonnées. Possibilité de jouer sur les axes X ou Y (Y processus de création, X temporalité du spectacle) pour commencer à analyser la création.
Affichage possible des extensions de fichiers, voit que passe des images vers le texte : en réorganisant les données dans l’espace commence par avoir des éléments sur la création dans le processus artistique.
Zoom possible dans les sous-partie.

Espace d’annotation vidéo, dans la fenêtre supérieur. Vue synchronisée de plusieurs flux vidéo. Possibilité dans l’espace inférieur de relier des documents avec la timeline et les organiser sur une ligne de temps.
Manque encore des marqueurs textuels et vocaux pour ajouter des annotations sur ces vidéos.
Travail avec artistes pour documenter processus de création au fur et à mesure d ela création. + version simplifiée sous forme de webapp pour pouvoir créer des webdoc notamment pour les théâtres dans des actions de médiation.

réalisation http://www.buzzinglight.com/work/

## Elastic search
Kibana extension d’elastic search qui permet d’exploiter les données en termes d’interprétation.
Mais formats courants JSON, possible employer 
