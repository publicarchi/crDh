# tcp 2015-06-10bis
Atelier Big data

https://mensuel.framapad.org/p/c0RWK9EwdC

Norme CTS et Cite pour la citation de texte

Présentation rapide de son intérêt pour le sujet en terme d’illustration.
SyMoGIH tentative de mise en place d’une base de données, publication des données dans une application commune. On va donc dire que l’on se place dans une logique de big data. L’idée est que les données sont des octets plus ou moins structurés à plusieurs niveaux. Comment les employer et aller les chercher dans la base, par exemple pour les étudier avec air. En réalité, quand on fait ce travail on ne connaît pas forcément les données publiées par d’autre. Il convient d’explorer au préalable les données.
Les données entrées dans la base, pointent pour les personnes vers des références vers IDref. Projet en phase alpha de créer un point d’accès SPARQL pour donner accès aux données, ouvrir ces entrepôts fermés pour les mettre en circulation.

On nous dit que modifie la manière de voir le monde.
Des perspectives de grandeur (stockage, ampleur, etc.)
Phénomènes des 3V volume, variété de sources, vitesse (changement en temps réel)
Articulation avec l’informatique décisionnelle qui utilise la statistique descriptive pour analyser des phénomènes basé sur un modèle du monde
Dans le phénomène big data, utilisation de la statistique inférentielle données à densité en information dont le grand volume permet d’inférer des lois... --> les mathématiques sont chargées de trouver un modèle dans les données
Système expert (intelligence artificielle) ; aide à la décision à partir de la fouille et de l’analyse des données.

Pour pouvoir faire quelque chose de ces données, il y a une demande de requalification des données autour d’un cahier des charges assez stricte.
- ouverture des données, réutilisabilité
- décrire des aspects propres à une discipline (modèles de métadonnées)
- représentation de la provenance des données
- représenter des informations contextuelles
- incertitude
- qualité des données
Vaste programme très intéressant et très difficile.

Texte dans lequel trouve des mentions d’acteurs ou d’entités que pourrait vouloir extraire. On pourrait utiliser la TEI pour faire cela.
Une fois que des entités-nommées sont identifiées, on souhaiterait les mettre en relation avec des référentiels. Autrement dit relier nos données avec celles d’autres producteurs.
Comment extraire des connaissances : production d’un tableur dans lequel place des données structurées. Peut également exprimer la même chose à l’aide de graph, en pointant vers un système de référence grâce à des identifiants.
Enfin, on peut encore produire une ontologie pour donner de la sémantique à ces structures.
On peut ensuite extraire ces données et les visualiser pour les analyser en les présentant en modèle de graph.

Se pose alors la question de savoir comment fait-on pour donner aux données extraite une structure identifiable.
- KCL Factoïd onto historique King’s college
- CIDOC-CRM issu du monde bibliothécaire
- Simple Event Model SEM modèles génériques issus de la volonté de rendre interopérable les données
- Symogih

dataBnf, Ministère de la culture,...
Isidore, Biblio it.
Ressources que peut analyser

Illustrer certaines étapes dans ce processus de prise de conscience de l’existence de données différentes et de mise en relation.
- csc que dispose de textes, d’images pour lesquels va disposer d’outils pour faire de la fouille (textométrie, classement par sujet)
- problématique d’indexation de textes d’images : identification d’entitées nommées, concepts, etc.
- Une fois qu’a retrouvé tous ces lieux : Alignement d’entités nommées et concepts avec notices d’autorité des bib, vocabulaires, taxonomies, et thesaurus qui structure conceptualisation d’un domaine, etc.
- Extraction de connaissances (relations entre entités nommées, concepts, dates, etc.) --> données semi-structurées
- Stockage dans bases de données relationnelles ou enrtepôts RDF. Une fois qu’a extrait ces connaissances où les stocker pour les rendre accessibles à d’autres chercheurs en les munissant de tous les éléments nécessaires (provenance, métadonnées, etc.). Condition de l’utilisation pour la recherche.
- Problématique de la création de centres de données, avec problématique de curation de contenu.
- fouille de données, systèmes experts, validation de la connaissance par les chercheurs-experts de domaine. Ce que l’on peut faire avec ces données en terme de production semi-automatique de al connaissance.


## Projet Rekall
comment conserver les œuvres numériques
Re-Collection, Art, New Media, and Social Media
http://re-collection.net
Richard Rinehart and Jon Ippolito

Presque tous les spectacles sont confrontés aux technologies numériques à moins d’avoir un plateau nu.
Logiciel pour musique, système de partition inventé.
Et conduite de régie.

Problématique de conservation. Comment passer de la création à la tournée. Comment fait-on pour que les œuvres continuent à fonctionner avec de nouvelles technologies. Cf. Nam Jun Paik avec moniteurs qui ne sont plus fabriqués : met-on a jour l’œuvre ou conserve œuvre et devient la trace de l’œuvre et ne voit plus la vidéo...

Plusieurs solutions imaginées comme le stockage, l’émulation, la migration et la réinterprétation. Pb authenticité.
Cycle techno 4 ans. Ensemble des œuvres de l’art de la scène confrontées à l’obsolescence technologique.
Nombreux documents nativement numériques. Archives de la création. Ici confrontés problèmes de BigData.

Comment mettre à jour les technologiques d’une œuvre au fur et à mesure de son évolution dans le temps.

Chargement des documents dans un fichier
Extraction automatique des documents et possibilités de modification des métadonnées. Possibilité de jouer sur les axes X ou Y (Y processus de création, X temporalité du spectacle) pour commencer à analyser la création.
Affichage possible des extensions de fichiers, voit que passe des images vers le texte : en réorganisant les données dans l’espace commence par avoir des éléments sur la création dans le processus artistique.
Zoom possible dans les sous-partie.

Espace d’annotation vidéo, dans la fenêtre supérieur. Vue synchronisée de plusieurs flux vidéo. Possibilité dans l’espace inférieur de relier des documents avec la timeline et les organiser sur une ligne de temps.
Manque encore des marqueurs textuels et vocaux pour ajouter des annotations sur ces vidéos.
Travail avec artistes pour documenter processus de création au fur et à mesure d ela création. + version simplifiée sous forme de webapp pour pouvoir créer des webdoc notamment pour les théâtres dans des actions de médiation.

réalisation http://www.buzzinglight.com/work/

## Elastic search
Kibana extension d’elastic search qui permet d’exploiter les données en termes d’interprétation.
Mais formats courants JSON, possible employer D3JS
Open source.

Norme CTS
http://cite-architecture.github.io
textGroup, Work, edition (réalisation de cette œuvre dans une version particulière)

Spécification qui permet de citer des textes sans difficultés. Possibilité de s’accorder sur les identifiants. Permet interopérabilité large, et l’utilisation d’outils déjà développés.

http://capitains.github.io/Nemo/app/perseids.html
interface produire en angular JS pour visualiser les données par des requêtes.

## Vega
vocabulaire égyptien ancien
http://vega-vocabulaire-egyptien-ancien.fr

Possible d’afficher en regard les notices
Mode d’édition dans l’interface en front-office.
Possibilité de signer l’article lorsque fournit des indications de nuances sur l’interprétation.
Jauge pour indiquer le niveau de complétude de la notice.
Affichage des différentes formes.

Nuances pouvant être catégorisées comme fausse, incertaine, etc.
Un élément important du système, c’est la possibilité de pouvoir dépasser la graphie ou la translittération en donnant accès, directement depuis la notice, aux clichés épigraphiques. Première fois, où l’on pourra facilement comparer l’ensemble des connaissances disponibles et les objets eux-mêmes. Avec ce moyen, mot commun mais en le cablant à l’objet a pu repérer graphie fausse = erreur d’interprétation de toute la communauté.

8 000 mots, 3 ans pour traiter l’ensemble du corpus.
Sommes nous vraiment dans les SHS en train d’abandonner la qualité ? Est-on vraiment en train d’aller vers l’analyse quantitative ?
Comment va s’articuler avec les sciences humaines.

BigData qui existe mais qui n’est pas réellement accessible. Nombreuses normes et formats. Le problème pour beaucoup de projet réside dans l’accès à la ressource proprement dite. Le big data ne peut donc exister que si le chercheur ait accès à ces données. Pas acceptable de devoir scraper pour travailler. N’existera que quand tous les projets mettront à disposition leur données sous forme de dump ou d’API.

Ok mais une logique programme. Or recherche conduites par des chercheurs particuliers, recherches individuelles. Manque donc un système de sas en fin de processus. Peut très bien imaginé qu’au moment où on publie les données on pourrait libérer les données mais cela implique que cela soit également considéré comme une forme de publication.

D’abord besoin d’une prise de conscience dans notre communauté de ce que constitue la donnée, la question du financement viendrait ensuite. Si condition posée par les financeurs...
Des choix en permanence qui sont réalisés lors de l’élaboration des données, à chacune des étapes on procède à des choix méthodologiques, or il faudrait que ces choix qui ont déterminé la constitution des données soient correctement documentés. Choix importants car conditionne le traitement que pourra opérer par la suite.

DMP dans les agenda 2020 sont standardisés. Procédure lourde très formalisée et standardisée. Certains pays plus avancés.
Création autorités dans ID-ref.
Besoin de disposer des vocabulaires standardisés, catalogue de catégorisation des métiers et des professions qui est collaboratif.

Pas de vocabulaires unifiés des émotions.
Plusieurs vocabulaires existants. Travail d’alignement.

Grande partie du travail conceptuel déjà réalisé. Besoin de les mettre dans des cases.
Problématique du LinkedOpenData de l’alignement. Mais pour tirer parti du big data, besoin de structuration. Des outils mais pas magiques, peuvent nous permettre d’obtenir des angles de vue sur nos données, mais ne permettront probablement pas les analyse ou de produire des résultats automatiquement.
Ici très bien de ne pas avoir mis en opposition la structuration des données et le big data.

Institut universitaire européen, modélisation du travail de tous les acteurs à travers d’ontologies interne et gestion des métadonnées avec Alfresco. Tout le monde doit travailler sur les données, des réunions pour la mise en données de notre travail quotidien, remplace aujourd’hui la mise à disposition par l’intermédiaire d’un document en ligne. Aujourd’hui doit assurer le versement, permettre la recherche, rationnalisation du fonctionnement de l’administration.

Possibilité d’injecter des données de différents types : structurées ou non structurées. De fait si part avec des données structurées des possibilités de restitution même si présente un coût en termes de constitution des données.

Sans doute un leurre des Sciences obsédées par la précision, du chiffre alors que pour nous ce qui est important c’est la méthode. On pourrait se demande s’il n’y a pas un leurre dans la tentation de réduire la recherche à la donnée, ou à ce que la donnée veut dire. Comment va-t-on faire pour ne pas se laisser dévorer par la tentation de réduire l’analyse des phénomènes à la données. Comment travailler avec ces données là pour conserver la méthode SHS ?

C’est déjà le cas ! le processus de la recherche est déjà largement conduit par la donnée, qu’elles soient ou non déjà produites sous format informatique. Important de prévoir une API pour distribuer les données qu’il s’agisse d’un SPARQL endpoint ou autre.

Difficulté liée au fait que recherche que conduit de manière soit privée soit individuelle soit collective. Déjà réaliser qu’il existe un potentiel. Si l’on arrivait à convaincre la communauté de la nécessité d’enregistrer la trace de l’activité des chercheurs.
Tout à fait d’accord avec la logique qui consiste à définir des API, ou des entrepôts de données. Fait apparaître une dimension nouvelle dans le partage de la recherche. D’ailleurs en sciences dures, la forme de diffusion des résultats de la recherche change avec publication ouverte des données, etc. risque de captation des données de recherche, etc. obligation de donner un droit de regard sur un objet toujours construit.

Au niveau H2020 formation dans cette logique pour établir cette logique, car tout à fait nécessaire de disposer de ces moyens. Mais des nouvelles figures professionnelles entre les chercheurs et les producteurs de données, pour leur montrer en quoi c’est essentiel, et comment répondre à ces obligations pour obtenir des financements.

Veiller à ne pas aborder la question des données du stricte point de vue de l’évaluation et du financement, sinon va se faire détester ! Regréter échec constitution outils enregistrement de la trace qui suppose :
- environnement de travail user friendly au plus près du travail quotidien du chercheur
- modèle conceptuel suffisamment extensible
- exploitation et instrumentation.

Trois univers différents
Ce que constitue
Ce que rapporte
