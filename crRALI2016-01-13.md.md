
## Séminaire RALI-DIRO, 13 janvier 2016 : Guy Lapalme, Traitement automatique de la langue et Web sémantique,

[http://www.iro.umontreal.ca/lapalme][1]

Travaille depuis plusieurs années dans le domaine du traitement de la langue, et plus récemment dans le domaine du web sémantique. Voudrait avancer quelques idées sur la manière dont le TAL est utile pour le web, et d’autre part le web est utile pour le TAL.

TAL Pour le Web syntaxique, moteurs de recherche.
D’une certaine manière, le TAL a sauvé le Web. Car pour se rendre sur une page, il faut pouvoir connaître son adresse. Or si l’on peut disposer aujourd’hui de moteurs de recherche, c’est grâce à tout le travail produit dans le domaine du TAL.

cf. Jan-Yun Ne

QUery
user interface
caching
ranking

= Query-time computing

mais couches inférieures, du TAL.
Moteurs qui sont capables d’analyser les pages, construire des index. 
Index builder
Web Page Paser
Crawler

Web


Le TAL est également essentiel pour le web car la majorité de l’information qu’on y trouve est essentiellement en langue naturelle. Pourtant la majorité des utilisateurs du web ne sont pas des humaines.

Humans account for less than 40% of Global Web Traffic, Mashable
Moins de 40% des visites par les humains. 30% par les moteurs.
L’essentiel des visites consistent en de l’analyse de textes en langue naturelle : du TAL.

Si l’on s’intéresse au TAL pour le Web sémantique.
Alors que le web est pour l’essentiel un web de document, pour la présentation. Mais comme les machines consultent de plus en plus le web, il est important de l’organiser autrement. Le web sémantique est conçu de sorte qu’on puisse produire un web qui puisse être consommé par les machines.

Tout un travail a donc été produit ces dernières années pour essayer d’organiser l’information. Plusieurs formats et langages définis qui sont des façons de présenter l’information, côté machine. 
Le problème c’est que peu d’humains souhaitent écrire du RDF. On souhaite donc continuer à produire des pages. Comment va-t-on faire pour que les gens puissent continuer à écrire comme ils l’ont toujours fait et que cela puisse malgré tout s’intégrer sur le web sémantique.

RDF défini l’information comme un réseau. Chaque entité est identifiée par un URI, et on organise les informations sous forme de réseau avec des graphes orientés.
RDF peut s’exprimer de différentes façons, mais ce qu’il faut bien identifier c’est que cela n’a rien à voir avec le texte.

Le web syntaxique tel qu’on le connaît propose des liens entre des pages dont le contenu n’est pas très organisé.
Le web sémantique quant à lui propose des liens très fins entre l’information.
La question c’est donc de savoir comment, en partant de simples pages web, on pourrait arriver à produire de l’information pour le web sémantique.


## TAL pour le web sémantique, le défi de l’annotation

Pages HTML offrent une structuration peu utile pour pouvoir manipuler l’information.
Vue par une machine ou un humain, peu éloigné. Simplement pas accès au contenu du texte, et dispose du balisage HTML qui permet de déterminer la sémantique de certains éléments.
entête, sous-entête, de l’italique, du texte qui ne dit rien, et des liens.

Pour pouvoir passer du web syntaxique au web sémantique, besoin de pouvoir annoter ce contenu à l’aide de différents outils
- détecteur d’entités nommées
- détecteur de dates
Il s’agit de pouvoir identifier des entités et de les nommer.

RDF, lie un sujet S et un objet O à travers une propriété P
chaque sujet et objet disposent d’une adresse unique
Pb de mappage sur les URI

Nombreux travaux TAL à ce niveau
- désambiguisation du sens des mots
- détection d’événénements
- analyse de sentiments
- liaison d’entité
Ensemble des problèmes de sémantique que retrouve donc ici.

Ensuite voudra pouvoir passer à la production d’ontologies (OWL) à partir desquelles on va pouvoir commencer à produire des déductions
- individus
- classes
- propriétés
L’idée essentielle du web sémantique est de pouvoir découvrir des connaissances qui ne sont pas explicitement renseignées.

TAL pour remplir une ontologie existante OWLExporter
TAL pour créer une ontologie
- concept de base pour débuter à récolter des entités
- récolter des dépendances (identifiées par un analyseur entre entités)
- extraire propriété du chemin entre concepts

Le TAL est donc bel est bien essentiel pour le Web
Si tout le monde peut en être convaincu ici, le problème c’est que personne ne le sait, cela n’est pas reconnu comme tel.
Le Web a donné une grande impulsion au TAL récemment.
« Le TAL d’une certaine façon cela ne marchera jamais, car sitôt que ça marche, on appelle ça de l’ingénierie ! »


## LE Web pour le TAL

Mais si le TAL est essentiel pour le web, d’une certaine manière aujourd’hui le Web est aussi devenu essentiel pour le TAL.
Pendant longtemps, une des difficultés du TAL consistait à pouvoir disposer de corpus traitable par la machine, aujourd’hui plus le problème avec le Web. 
Même si n’écarte pas tous les problèmes d’organisation des corpus, au moins un accès essentiel. 

En outre, on dispose aujourd’hui de contenus intéressants à propos du langage élaborés par des web participatifs.
Ressources intéressantes en TAL.
- wikipédia
- DBpédia
- etc.

Une façon intéressante de pouvoir utiliser de grandes quantités de textes consistent à produire des modèles de langues.

Distribution de probabilités sur les chaînes d’une langue
Compte les fréquences de mots/caractères sur les textes (plus il y en a, mieux c’est).
- google web trillion word corpus
- hansard canadien (bilingue)
- etc.

Que peut-on faire avec cela ?
En se limitant au modèle de langue (ML) unilingue, plusieurs choses possibles
- classification de document : comparer un texte avec le ML d’un type de texte
- recherche d’information : comment le ML de la requête concorde avec celui d’un document
- identification de langue : comment une séquence de caractères se compare-t-elle avec les séquences habituelles d’une langue
- accentuation de caractères
exemple RALI projet Zodiac : système essentiellement basé sur un Modèle de langue, mais aucune linguistique ici. Une erreur tous les 200 mots, peu \> aux humains.

Langues sur le web
55% des contenus sont publiés en anglais
24% en français
source : w3techs.com
Une distribution plutôt unimodale.
D’où l’importance de pouvoir profiter des textes bilingues et en particulier des concordances. L’idée d’un concordancier consiste à prendre des paquets de textes bilingues, d’aligner les phrases et ensuite de considérer chaque paires de phrase comme un document.
Dans première étape pour TransSearch. Ensuite amélioré pour essayer de comprendre à quoi correspond les parties à l’intérieur de la phrase.
La grande utilisation des textes bilingues sur web était destinée à la traduction automatique.
Lorsque la théorie des approches statistiques à la traduction a été développée, au départ seulement le Hansard existant.
argmax P(e l f) / e = argmax P(f | e) P(e) / e
P(f | e) modèle de traduction entre la source et la cible calculée à partir…
Calcul de probabilité sur des grandes quantités de textes. cf. Philippe Langlais.
Une grande table avec des chaînes et des probabilités.
Cela ne dépend pas du web, mais la source qui a permis de disposer de grandes quantités de textes pour pouvoir calculer  ces probabilités c’est le web.

ex. WATT warning-advertissement translation-traduction
modèle qui donne d’excellents résultats pour le domaine.

Maintien que si l’on était capables de pouvoir développer de tels modèles de traduction, c’est grâce au web.

## Enfin le Web pour le TAL pour le Web

Traduction automatique de page avec Google Translate
Ontologies pour aider les usagers à trouver de meilleures informations sur le web (Yellow Pages)
Reconnaissance de la parole (Siri)

= le Web ne serait donc pas le même sans le TAL
= le TAL ne serait pas le même sans le Web

## discussion

Qu’est-ce qui s’est davantage développé
- méthodes statistiques
- plus de gens qui travaillent en TAL car des outils disponibles
Des besoins en traduction automatique, mais pas seulement car travaux sur le déplacement de robots, etc.
Le point ici, c’est que peut disposer de textes en grandes quantité. 
Longtemps, une vision baisée des informations, obligé d’aller chercher dans des gros corpus. On a donc maintenant accès à des textes tels qu’ils se présentent en effet.
Sans compter maintenant que de nouvelles sources comme les textes issus des réseaux-sociaux.

Dans le traitement automatique des langues, il y a aussi langues, quelle place voies-tu ici pour le linguiste ?
Le rôle du linguiste a toujours été d’étudier des phénomènes et d’en retirer des données.
Aujourd’hui les corpus disponibles sont plus vastes. Si pendant longtemps une vision formelle sur la justesse de la langue, depuis longtemps apprécie la diversité de la langue. De fait, plus de travail qu’auparavant car plus de textes.

Dans le web sémantique, des raisonneurs qui permettent de faire des inférences. Sont-ils utilisés couramment ?
Pour le moment ne connaît pas d’application où utilise ce genre d’outils. En fait, les utilisera plutôt pour valider des ontologies que l’on veut utiliser dans un autre cas.
Production ontologies, classes organisées par des propriétés.
Raisonneurs qui permettent de vérifier que l’ontologie est consistante (par exemple pour voir si a défini des propriétés sur les classes dont ne pourrait jamais avoir d’instance)
= débuguer des ontologies
= aussi possibilité de ranger dans la bonne classe. Mais pour le moment ne connaît pas encore d’application industrielle où cela est essentiel.

Intéressant car parle de faire du web sémantique, et non pas le web sémantique. Penses-tu qu’il y aura une convergence, où se fera par derrière.
Pense que fera du web sémantique sans même que s’en aperçoive. D’ailleurs en partie déjà le cas avec RDFa. Codage embarqué dans la page web et destiné aux moteurs (Rich snipets de Google, etc.)
Déjà des ontologies bien faites pour coder à la main les heures d’ouvertures d’un magasin, etc. et interfaces pour publier sur son site.
RDF pas fait pour faire de l’information, mais plutôt de la méta-information.

Apprécie beaucoup vision très large du TAL car souvent vision restreinte du TAL. Nombreuses méthodes modifiées de TAL traditionnelles qui sont modifiées pour être appliquées à la recherche d’information. Il faut donc changer la vision du TAL pour considérer que cela englobe beaucoup plus de choses.
Une tendance pour extraire de l’information sur les textes. C’est peu le cas de Google mais plus intégré par Bing (Cortana dans Windows 10), et moteur de recherche chinois Taïdu encore plus avancé pour extraire des relations ou des inférences à partir de textes afin de répondre à des questions relativement simples.

Quand donne des exemples de traduction automatique qui fonctionne cite souvent la météo. Domaine limité, mais Google translate étonnamment bon, surtout si traduit dans une langue que l’on connaît. Fait alors de la traduction pour comprendre ce dont cela parle. Veut comprendre l’essentiel, pas encore assez fiable pour la citation et la publication.






[1]:	http://rali.iro.umontreal.ca/lapalme/