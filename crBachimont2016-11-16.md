# Conférence de Bruno Bachimont, 16 novembre 2016

UdeM, CRIHN et Chaire de recherche sur les écritures numériques

Bruno Bachimont est docteur en informatique et en épistémologie. Il associe une approche philosophique à une connaissance profonde de la technique en tant que telle. Il est actuellement directeur de la recherche à l’université de Compiègne. Parmi ses ouvrages, notamment le Sens de la technique, le numérique et les calculs, chez Encre Marine en 2010.

## De l’épistémologie de la mesure à celle de la donnée : un nouveau nominalisme pour les sciences de la culture 

évoquer avec vous des réflexions sur le phénomènes du big data et l’ère de la données. à son sens des questions épistémologiques nouvelles qui se posent. Au-delà des promesses que l’on nous fait des questions épistémologiques et philosophiques à aborder. 

Les Big data, ça Trump énormément… malgré toutes les promesses liées à une avalanche de donnée,s pourtant pas été en mesure de prédire le résultat de l’élection.

- une mutation annoncée et ses questions
- une triple phénoméno-technie (collecte, …) et autant de problèmes
- Un nouveau paradigme : le nominalisme du social
  aborder la question sous un angle nominalisme. Nominalisme qui au 14e s. a permis de replacer la question du rapport entre la science et la nature de manière différente. Un nouveau déplacement de paradigme en train de se tramer. Un nominalisme du social.


- Des enjeux restant à relever
  - le fait humain est-il une donnée ?
  - les résultats sont-ils intelligibles ? nouvelle dimension du paradoxe de Memnon réactivé, une anthropologie à construire en conséquence.
- Construire une épistémologie adossée à une anthropologie

## Une révolution annoncée, les “ big data”

Les caractéristiques souvent associées au Big data peuvent être ramenées à trois grandes propriétés sans être exclusive.

D’abord leur masse (elles sont big) mais pas seulement issues de la massification de l’information. Elle est issue de deux autres facteurs complémentaires.

- la dynamicité, on s’intéresse à des données qui sont produites de manière différente, cycliquement, régulièrement de manière "spontanée" avec des systèmes de captation branchés sur le réel
- hétérogénéité. Les big data nous ont montré que le numérique permettait de réunir dans une même homogénéité du calcul des données qui peuvent être de nature très différente. Le numérique nous permet ainsi de faire une synthèse de l’hétérogène : poser ensemble des choses qui n’ont rien à voir. Facile de réunir par exemple des sons et des textes. Simplement à rapprocher des codes de calcul. 

Le big data enfants des cette dynamique. En résulte une approche nouvelle.

Tout n’est pas big data, ce n’est pas parce que l’on a beaucoup de choses, qu’on a des big data, mais lorsque l’on réunit ces trois facteurs que l’on a des big data. Commence lorsqu’il y a une rupture du fait de la masse. Le big peut apparaître très tôt, apparaissent lorsque la masse implique un décrochage et de confier à la machine les opérations. Dégager des propriétés seulement visibles par la masse.

Les 4V, slogans souvent associés au big data. Le V comme Volume, V comme vélocité, le V comme variété (hétérogénéité), et enfin un V comme véracité. Consiste à dire que les données n’ont pas forcément besoin d’être véridique, mais leur tout qui fait sens.

www.datasciencecentral.com/profiles/blogs/data-veracity

Pour cela que peut à voir avec le linked data, où des faits bien identifiés par des liens. Les big data des infra enregistrement, où les données pas de sens en tant que tel. Le sens vient de la globalité pas de la localité. Des données qui possèdent une sémantique depuis la masse et pas d’elles-mêmes.

Techniciens des big data alors confiants à l’égard de la données, car le sens qui vient de la masse.

Google Flu, observation des données correspondant à la propagation des données récoltées sur les réseaux sociaux, et données épidémiologiques standards. Article qui présentait une corrélation quasiment absolue entre ce que pouvait prédire en regardant directement les objets ou la circulation de l’information. Observation de la circulation de l’information presque aussi fiable. Mais en réalité pas un observatoire de la maladie mais de la rumeur. Raison pour laquelle décalage entre ce que peut observer directement et information. Néanmoins fortement marqué les esprits.

Indépendamment de l’intérêt de l’étude, important car permet d’envisager un déplacement de la pratique scientifique.

- Nouveau paradigme scientifique pour les SHS pour Manovich. = cultural analytics, approches par les données qui permettent d’aborder différemment l’approche du social. Possibilité de revenir à l’analytique. Cependant l’analytique du calcul n’est pas l’analytique de la preuve. Pas parce que l’on calcule que l’on est capable d’administrer la preuve. Mais une ouverture sur la manière d’aborder la critique du social.
- Effacement de la pratique savante critique au profit d’un empirisme directement au contact de la données. Chris Anderson. Applique une heuristique connue dans le monde scientifique : slogan modèle X données = constantes. Plus dispose de données plus puissance. Dans les années 70, travail sur des modèles linguistiques forts car travaillait sur peu de données. Avec arrivée de Google s’aperçoit que beaucoup de modèles ne dispose pas de connaissance linguistique et seulement modèles de Markov, etc. lié à massification. Le monde que l’on a dans les données, donc pas besoin de théorie = un empirisme radical qui permet de se passer de théorie du fait de la massification.
- Ancrage objectif de la décision. Rationnel parce que calculatoire de la prise de décision. La délibération ne fait plus partie des nécessités du social. Lorsqu’il y a des démonstration, plus besoin de délibération. Artisote éthique à Nicomaque.

Cultural analytics

terme proposé par Lev Manovitch, plusieurs étapes : collectes des données, analyses statistiques, visualisations interactives. Données —> Analyse —> réduction

masse qui fait que ne peut voir qu’à travers des réductions pour l’analyse.

Ex. LinkFluence.net, sur les politicospère juin 2009. L’idée étant de pouvoir observer globalement en ayant recours à la masse grace à ces outils. Ici voit sites républicains davantage reliés à des sites de production d’armement. Intensité réciproque des relation.

Construction de représentation synoptique destinée à comprendre la nature des données contenues.

Un problème à part entière de savoir comment va représenter la complexité. Trouver des métaphores ou des formes graphiques destinées à rendre compte des résultats des ces analyses complexes.

Visualcomplexity.com

Dans la suite de Anderson, des courants qui revendiquent cette radicalisation. Pas tant l’effacement de la science mais l’émergence d’une nouvelle science. Cf. article suite conférence de Jim Gray, The Fourth Paradigm, Data Intensive Scientfiic Discovery ! en outre disparu après la conférence.

https://blogs.msdn.microsoft.com/escience/2009/10/16/the-fourth-paradigm-data-intensive-scientific-discovery-book-released/

- association empirique à des phénomènes
- science moderne avec loi pour comprendre le monde
- calcul et simulation, permettant expérimenter phénomènes naturels pour pouvoir les expérimenter
- les données

Une distinction entre la simulation du calcul scientique et les données. On n’est pas dans la simulation de lois déjà connue au préalable, mais dans l’extraction de connaissance et de lois qui ne sont pas connues auparavant. Il n’y a pas d’hypothèses sur les connaissances qui permettent d’exploiter ces données. On considère que les corrélations qui vont permettre de faire ces relations.

Distingue une science du passé, computationnelle. Et science du futur qui va travailler avec des données et donner lieu à l’émergence du savoir. Idée qu’en partant des donnés brutes va pouvoir reconstruire toute l’information qui nous permettra de reconstruire le monde et l’étudier. 

Un rêve gigantomachique car la données non seulement un outils qui nous permet d’étudier le monde, mais la données  présente une exhaustivité, on dispose des données complètes, et la carte va remplacer le territoire. Idée que les données sont isomorphiques au monde.

Or tradition scientifique habituelle a toujours fait une différence entre les modèles et la réalité, c’est à dire la carte et le territoire. Mais la justement dans les modèles pas de données mais de la mesure.



Trois problèmes, deux enjeux.

D’un côté les données et la collecte, puis le traitement et enfin leur visualisation. cf. Manovich.

Lien abritraire, conventionnel, produit par l’acte de collecte de la donnée. | donnée - mesure

Un arbitraire de l’interprétation. Dès lors que des big data, la sémiotique la plus sauvage qui permet d’interpréter les résultats. | perception - langage

Paradoxalement, très grande sophistication de la collecte des données, et un arbitraire grossier de la représentation. Les données ne sont pas des mesures. La perception que l’on a des visualisations proposées repose sur une sémiotique du visuel qui ne présente pas l’expertise critique du textuel constitué dans le temps.

Ces trois étapes décrites par Manovich correspondent peu ou prou à trois déplacements. Ici que reprend l’idée de Bachelard d’une phénoménaux-technique. Idée que les phénomènes sont construits. 

Construction des données. Les données sont construites, il y a des choix arbitraire de la collecte. Il y a un déplacement considérable de l’arbitraire du format, de la collecte.

Captation et traitement des données. Les outils de collecte des données sélectionnent, transforment, formatent les données lors de leur captation.

Présentation des analyses effectuées. Des algorithmes de représentation dans l’espace des données. Plus résultat de choix arbitraires d’un programmeur plutôt que des propriétés intrinsèques des données. Des algorithmes de placement épais complet, donc des choix arbitraires possibles. Peut donc avoir des biais, des artefacts qui altèrent l’interprétation sémantique de l’interprétation. Ce n’est pas parce que deux points sont proches que sont proches sémantiquement, cela peut être pour les faire simplement tenir sur l’écran.



La question de la collecte

Il y a un mythe de la donnée brute. Les données semblent être "données" car elle sont trouvées. Celui qui collecte n’est pas celui qui a construit des données. 

Les données sont construites comme artefacta c’est une construction technique par rapport à un fait social. 

Data comme capot, elles sont transformées par leur captation (les textes deviennent des sacs de mots).

Idée que les documents sont des ressources. Captation qui va consister à casser le lien entre le document et la donnée, rendre exploitable le code lui-même. Choix pour optimiser la captation du codage. Perte de l’hétérogénéité ou pertinence et caractéristique sémantique intrinsèque.



La question du traitement 

Des réseaux neuronaux, systèmes multicouche. mais personne ne sait réellement ce qu’il se passe dans les couches basses du système. Fonctionne bien, mais s’aperçoit que les couches indifférentiable, inintelligible pour l’homme. On a bel et bien une approche analytique, mais inintelligible.

Incapable de dire en quoi le calcul qui a produit le résultat une preuve et pas un résultat de production.



