# crSCDH2017-05-30.md

Travailler de manière critique sur les DH.

Orientalisme, conscience de ce qui est. Processus histoire. Gramsci, acte d’élaboration critique.

Topic modeling, marquage des lieux visiter et analyse du discours. Graphing Austin Clarke.

Textes qui résistent objectivité du traitement computationnel.

## Nénuphar et l’araignée

réflexion sur l’essai de Claire Legendre. Numérique qui propose un réagencement des contenus dans toutes les disciplines. Historiquement la production documentaire a mis l’accent sur la stabilité. Avec le changement des supports, la circulation des contenus, l’ensemble des rapports au savoir remis en question.

Nécessaire de jeter les bases de théoriques et repenser les outils en lien avec possibilités d’édition de fouilles de textes. Premier exemple d’essai d’application à la littérature française. Grand accueil critique.

Ouvrage seulement disponible en version papier. Annotation de l’ouvrage. Claire elle-même s’interroge sur l’attitude de lire avec stylo. Ne pas lire vraiment, demeurer actif. Hors du texte, juché au dessus. Accès au pdf, possibilité d’exploration par moyens électroniques.

Word cloud. Tirer profit du texte en format numérique, assister à l’analyse, comparer.

Démontrer l’utilité d’une approche basée sur l’analyse des données textuelles et sur la fouille. Perspective de l’utilisateur final.

En fait, une réponse à Claire sur ce que peut faire avec un ouvrage déjà en format numérique. Assister l’analyse et le comparer avec d’autres ouvrages. Démontrer l’utilité d’une approche basée sur l’analyse de données textuelles. Mais ici dans la perspective particulière de l’utilisateur final.

Une démarche en cinq étapes comme c’est souvent le cas dans les projets de fouille de texte. Constitution du corpus. Ajout de textes, nettoyage de l’ouvrage pour retenir l’essentiel du contenu. Transformation en format numériques admissibles pour les algorithmes de fouilles et d’analyse. Puis interprétation des résultats.

Première étape qui consistait à constituer le corpus. Un difficile retour à la réalité car habitude de travailler avec données fournies par des organisation et donc étape triviale. Dans le premier cas pas de pb. Mais comme voulait comparer avec autres textes de Claire, a chercher à en trouver d’autres.

Conversion epub vers txt. Pb des DRM. UTF8 vers ANSI. Bruit des caractères. Étonné qu’en 2017 dans un contexte où le numérique est partout soit encore confrontés à ce genre de difficulté.

Corpus complet, caractérisé par 23 394 formes 273 000 occurrences. Grossièrement un petit corpus.

Filtrage

- anti-disctionnaire
- lemmatisation (pour augmenter la redondance)
- seuils statistiques (fréquence min-max, combiné avec mesures connues en science info comme mesure TFxIDF)
- Nombre total à retenir en fonction de la corrélation (mesure de chi2)

Lexique, mot principal peur, vie, mort, amour, mer, etc. liste de mots triés par fréquence. Sur le corpus entier première observation et probablement la seule que l’on puisse faire, retrouve ici des noms de personnage. Ce que ne retrouve pas dans le nénuphar et araignée.

A dû ajouter la suppression des entités nommées pour ne pas brouiller l’analyse.

Application d’algorithme de topic modelling. Voit apparaître les principaux thèmes présents dans le corpus. Important de regarder la colonne de fréquence. Symptomes, douleur, angoisse, termes qui sont assez caractéristique de cet ouvrage. Ce que parvient à identifier avec un algorithme d’extraction thématique.

Application d’algorithme de segments répétés, ici trier par fréquence. Supposait trouver "fumer une cigarette", "cancer du poumon", mais à ma grande surprise, essentiel des segments répétés qui portent sur le temps, l’angoisse du temps qui passe et de la mort qui va arriver. Or cela possible avec stratégie de segments répétés.

Modélisation thématique sur l’ensemble du corpus, retrouve personnage, en les supprimant beaucoup plus parlant. Trouve le nénuphar et l’araignée avec les mots thymus et cancer.

Voulu explorer la relation entre ces différents thèmes et leur distribution à l’intérieur du corpus. Certains termes que retrouve dans d’autres ouvrages. Passe en revu les thèmes.

Algorithme de classification hiérarchique ascendante, 300 mots selon TFxIDF, mesure de similarité : mesure de cosinus entre les documents. Permet de montrer proximité marquée entre certains ouvrages. Et corrélation.

Même résultats positionnés à l’intérieur d’une analyse factorielle des correspondances (AFC). Ici volontairement ajouté beaucoup de mots. Voit extraits les termes les plus significatifs. Araignée, papillon, réseau lexical lié au nénuphar et s’oppose autres ouvrages.

épuré avec mots caractéristiques, voit dispersion entre les thèmes abordés.

Une première étape pour l’analyse de corpus. L’approche s’avère efficace. Fait au départ sans trop de soucis. Prochaine étape travailler sur des corpus plus larges. Comparer avec autres auteurs et varier les approches.

## Abi Lemak, Considering Visual Culture in Arthur Conan Doyle’s The Adventures of Sherlock Holmes : A Study of Textual Editing

Kirshenbaum, media archeology, pour conditions textuelles. John Maths?? suggère engagement critique avec l’interface ebook. Pour lui plate forme future déjà là. Questionner les modes de lectures que la technologie définie. Question de la préservation des enregistrements culturels papiers. Comment traiter conservation de ces contenus matériels. Comment concilier ces objectifs contradictoires.

Culture visuelle 19e. Contrôle visuel qui devient de plus en plus important depuis fin 18e. En tous les cas phénomène de brouille entre médias. Exemple de ConanDoyle, ensemble édition. Examiner trois éditions populaires, suppose que culture visuelle se concrétise par méthode de déduction qui est celle de Doyle.

édition de 78 qui recopie facisimile édition originale. Se présente comme reproduction à l‘identique de l’original. Suggère valeur et unicité.

Plusieurs éditions, 77

Protéger la première rencontre du lecteur avec le texte. Skeumorphisme. Conserver proximité avec les images.

édition numérique de Sherlock Holmes for the iPad. Un exemple de tous els possibilités, idées de jeu.

## Discussion

Pour Clark topic qui ne présentaient rien de pertinent. Démontrait que l’auteur ne rentrait pas dans les catégories. Intéressant car nous renseigne sur l’algorithme. Revenir sur cela pour comprendre ce qui n’a pas marcher, ou trouver une autre approche.

## Text analysis

Kadushin et al. Chum 1968, 194

The dangers of inserting our hypotheses into the data and then pulling them bad out like rabbits from a hat are really at issue.

Question de l’usage instrumental et usage interprétatif des outils informatiques.

Via projet par Sally Sedelow et Spiral qui réplique ces outils.

Content Analysis in GI

General Inquired développé par chercheurs à époque où texte peu utilisé.

Sans doute une des applications les plus problématiques pour un humaniste. 

Premier d’une longue lignée d’outils qui permettent l’analyse de contenu. Aujourd’hui LIWC2015 qui sort une table avec lien vers les fichiers et de comptes de catégories. Peut alors présenter les résultats sous forme de graphiques.

Que faisait General Inquired 

Operate on set of document

Tagging of sentences

- lemmatize words
- look up in Dictionaries of Categories
- Optional "sentence summary" processes

Output Results

- listing of sentences and tags
- Tag taille of categories per documents
- graphs
- WWCs

Tourne sur des mainfrain. De ce genre d’outils que logiciels qu’utilise aujourd’hui son issus.

Une des choses que nous avons fait, sommes allés voir la documentation et avons fait une réimplémentation dans un nouvel environnement que nous appelons spiral. En fait, un environnement de notebook avec Voyant.

Utilisation du dictionnaire de catégorie original. Basic spreadshit, données de Harward et combined categories. 190 catégories différentes au total. Plusieurs dupliquées (en fonction des dictionnaires).

Quelles étaient donc les présupposés interprétatifs des collègues à cette époque.

Programme dont l’objectif destiné à analyser al propagande de guerre allemande. Puis devenu populaire dans le champ des études sociales. 

Berelson 1952 Content analysis and communication research. 

Un livre très influent sur l’analyse de contenu pour les auteurs.

-  dans lequel manifest content 
- une méthode considérée comme objective au sens que precessus et catégories définies comme des réultats reproductible
- quantitatif, au sens ou des fréquences numériques sont assignées à l’analyse de catégorie.

Peut reposer sur des actions.

Savoir si peut aller d’un texte explicite à inférer sur ce que dit la personne. 

Analyse de contenu, suppose que l’analyse du manifeste à du sens de même que le contenu.

Pourquoi pas utilisé dans les sciences humaines ? Sans doute car l’analyse de contenu n’est pas herméneutique.

Stone "Thematic Text Analysis" 1977

Dr Sally Yates Sedelow

Phd 1969, System development corporation, une spinoff de Rand fondée en 1969 lorsque développement informatique massif aux EU.

Développa le Via dans un contexte de recherche. Première Prof femme en informatique. Sans doute l’une des première bourse du NEH dans le domaine des DH.

Supervised par John El Smith

Via is Foundation for RATS … /ARRAS

Utilisation de la source plutôt qu’un dictionnaire de catégorie préexistant. Ainsi création d’une sorte d’environnement herméneutique où la persistance sert à l’analyse.

Possibilité d’éditer les fréquences, possibilité de relancer le dispositif sur le texte.

Comparer les environnements

Dans General Inquirer

Diagnostic tandis que via herméneutiqeu

Contenu versus mots

Reproductible, vs circulaire

sciences hum vs humanités, etc.

Savoir si la forme des outils que nous construisons influence l’interprétation. Alors besoin de nous tourner vers les outils eux-mêmes. Choses qu’avons essayé de faire avec Spiral en essayant de reconstituer l’outil.

Spiral est un environnement de programmation littérale suivant le modèle de Donald Knuth. Différence avec Ipython, à la différence d’avoir du texte et du code que peut exécuter ce que peut exécuter c’est des panneaux de voyant. Des catégories offrant fréquence des catégories par texte.

Notebook encourage ouverture, et amène chercheur plus loin car catégorie et environnement font partie de l’interprétation et peut les changer

## Dominique, Des quoi est-il question dans le discours en art contemporain ?

La fouille de textes appliquées à l’art contemporain au Québec. Travail qui émerge d’une collaboration avec un collègue archiviste en histoire de l’art. Un étudiant qui a travaillé plusieurs mois avec nous.

Idée de constituer un corpus de fouille dans le domaine de l’art contemporain et d’y appliquer des traitement.

Introduction générale, contexte du travail, objectifs, ect.

S’intéresse ici à la question de la diffusion du discours sur l’art contemporain au Québec. Plusieurs réseaux de diffusion, les musées, les galeries commerciales et les centres d‘artistes autogérées et institutions. Ce qui nous intéressait la distinction entre les différents endroits.

Ici dans le contexte du travail de K. Bender de Distant Viewing qui peuvent être appliquées au domaine de l’histoire de l’art. Par définition les arts visuels sont par nature visuels, il est difficile de traduire par écrit tout ce qu’une œuvre peut exprimer. À partir de ce moment le contexte sur les œuvres peut devenir intéressant. S’est intéressé sur les textes concernant les œuvres.

Une expression par procuration du contenu visuelle. Cette expression que voulait analyser et extraire à l’aide d’outils de fouille. 

Un courant tellement important que certains ont avancé le contexte de Artspeak qui Interoation Art English 2015 Levine et Rule. Suggère qu’un langage qui peut caractériser les œuvres.

Lorsque parle d’algorithme de fouille de texte, beaucoup d’utilisation développés dans l’intelligence artificiels. Des travaux qui ont essayé la même chose dans des contextes différentes. 

Par exemple travaux se sont intéressés aux images. Saleh et Elgammal 2016 sur catégorisation des œuvres.

D’autres intéressés à des possibilité d’utiliser des réseaux neuronaux pour reconstituer le style d’un artiste. ici image carctéristique.

Ici pas tant l’image qui nous intéresse mais le texte dans une perspective de distant reading. Essayer de voir de quoi question au Qc et en quoi les centres d’artistes autogérés se différentient de ceux des galeries commerciales.

Pour atteindre cet objectif, avons produit une cartographie des thèmes en art contemporain. Donc une analyse en diacrhonie à partir delaquelle a essayé de comprendre de quoi il était question dans le corpus.

élaboration du corpus avec catalogues, extraction du texte. Supprimé les paratextes. S’est limité à des textes disponibles sur le web.

Artiste, année, titre exposition, le texte et notes biographiques. On a travaillé sur l’analyse de ces données textuelles en lien avec d’autres variables.

49 centres artistes

près 4000 document

998155 mot, ration élevé car vocabulaire spécialisé.

Idem pour les galeries commerciales 22 galerie, etc.

Exemple de documents.

Travail qui repose en grande partie sur l’application du topic modelling ou d’algorithmes de classification thématiques. Des approches qui consistent essentiellement à découper le texte en différents segments et d’essayer de caractériser les thèmes correspondant à chaque segment.

Travail dans l’environnement Mallet, travail sur les corpus différemment et cherché à adapter incrémentionnellement de 5 à 100.

Supposait extraction du lexique, normalisation de casse, suppression des mots fonctionnels.

35 thèmes optimaux pour les galeries et 25 pour les centres.

Pour les galeries subv, obtient thème art public, ville, projet, espace. Thème de l’art sonore, musique, son, installation, écoute, audio.

Pour les galeries subv 2000 à 2016 répartition des différents thèmes. Attire l’attention sur le fait que des thèmes qui sont caractéristiques du vocabulaires de la discipline : artiste, projet, travail, installation, production. Les termes plus spécifiques ceux que retrouve dans la partie inférieure.

Pour les galeries commerciales, même principe. Mais un des thèmes les plus fréquent ici la peinture et les termes associés, couleur, etc. Distribution à travers le temps des thèmes caractéristiques. Se retrouve ici avec un terme concernant une technique parmi les thèmes les plus présents.

Si devait regrouper tous ces themes, peut dire qu’une partie qui correspondent au vocabulaire de base de la discipline. Thèmes plus spécifiques, et junk qu’il faudrait analyser en détails.

Résultats en quatre étapes pour chacune des institutions en termes de topics.

Dans les galerie sub, augmentation croissante de la sculpture, des installations sonores et de la technologie ce qui n’étonne pas compte tenu des missions de ce genre de galeries. Alors qu’en contrepartie des thèmes à la baisse comme la peinture et la photographie qui à l’inverse sont en progression dans les galeries. Donc ici une évolution inverse entre les deux établissements.

Temporalité et rêve.

Au niveau des galeries commerciales. La peinture stable. Utilisation vidéo en croissance après avoir été nulle. le dessin un peu à la baisse. Même chose pour la photo.

Mémoire en croissance, de même que le thème de la culture autochtone lexicalisées par les termes suivants...

Si confronte paysage/nature plus présents dans les galeries commerciales.

Le thème de la photo en dent de scie dans commercial et plus élevé dans les galeries subventionnées.

Et utilisation d’un médium comme la peinture extrêmement caractéristique de ce que l’on trouve dans les galeries commerciales. 

à l’inverse l’utilisation de la vidéo surtout dans les centres d’artistes subventionnés et moins dans les galeries commerciales. Sans doute car plus difficile à vendre qu’à exposer dans un centre d’artiste autogéré.

Exemple de visualisation pour application simple. Ici couple les informations avec les différentes galeries dont on a utilisé les textes. Peut voir où trouver des œuvres utilisant des nouvelles technologies à l’intérieur de ces centres d’artistes autogérés. De même peinture dans ensemble des galeries commerciales.

Sans doute un corpus extrêmement riche, fait à la main très propre. Utilisation sur le même corpus d’approche prédictive en utilisant des textes supervisés.



