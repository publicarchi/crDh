# Cr Websem 9 juin 2019

Accueil Téluq, LISEF, 2e année

Comité Christian Aubry, Gilbert Paquet, Michel Héon, Josée Plamondon

http://www.licef.teluq.ca/

## Michel Héon, Josée

Une initiative démarrée il y a 4 ou 5 ans avec l’idée de faire connaître le web sémantique au Québec. Réseauté, échanger, présenter les choses que l’ont fait. Question des usages du websémantique. 

Remercie commanditaire. Stardog 

## Stardog

Stardog’s Knowledge Graph platform enables fast and flexible data unification so you can query, analyze, and uncover hidden insights.

3 ans beaucoup focalisé sur comment, aujourd’hui plutôt le quoi.

Données non-structurées une manière d’aborder le marché d’une autre manière et construire des produits centrés sur les données.

Travail avec organisations publiques pour publier données.

## Michel Héon, 10min talk. Les graphes de connaissance en 10 min

Cotechnoe

Ontologie, petit être et le grand Être

Être l’essence, être l’objet.

Sémiotique de Charles Sanders Peirce, 1938. Théorie du sens. 

Ontologie spécification formelle d’une conceptualisation partagée.

Les domaines du discours, conceptualisation de cela qui est un domaine partagé, où développe des vocabulaires pour parler de cela. Ses symboles qui seront capables de représenter la réalité (l’ontologie en informatique que l’on appelle la spécification formelle).

Plusieurs formes : Manchester syntaxe, functional syntaxe, owl, rdf. Seulement des notations.

Partie conceptualisation partagée. Du point de vue des sciences cognitives, partage de connaissance entre les humains, mais aussi entre les humains et les machines (ex. Google). Sur le web parlera de l’interopérabilité des données.

Vocabulaire normalisé, un point de vue sur la donnée, sert à recycler une donnée pour différents usage

LOD 1239 jeux de données partagés en 2019-05-29 lod-cloud.net

et LOV vocation de partager des vocabulaires.

Un graphe une liste d’énoncés. Un énoncé est un triplet sujet, prédicat, objet. Et un objet ou un sujet peut lui-même être l’objet d’un autre énoncé, comme cela que legraphe se construit.

Plusieurs niveaux abstractions : graphe de données les faits, le niveau sémantique celui des models, Graphe de connaissance, interopérable.

## Discussion

Quelles possibilités alternatives. En fait, beaucoup de monde parallèles qui émergent. Chaque privé développe son propre knowledge graph. Idée du websem faire reposer sur un standard ouvert, partagé et gratuit à l’instar du HTML.

IEML, Pierre Levy, pour calculer les relations.

## Claude Coulombe

Doctorant, professeur chercheur, fondateur de Lingua technologies, DataFranca

Au départ travail dans le monde de l’IA. Mur des données massives. 

Secret assez bien gardé de l’apprentissage profond : nécessite d’énorme quantité de données. Mais aussi des infrastructures de calcul (attente sur Calcul Québec).

Beaucoup de cuisine.

Il n’est pas rare de se retrouver avec des quantités nettement insuffisantes de données pour entraîner un modèle profond. Ingénieurie qui a l’intérêt de ne pas nécessiter des attributs, les découvre lui-même à partir de données brutes. Mais cela prend énormément de données pour que le système soit en mesure de trouver des régularité et bâtir des attributs.

Un véritable enjeu pour les communautés linguistiques minoritaires qui ne disposent pas de suffisamment de données. Risque de disparition. Moins un problème en français mais peu de ressources publiques disponibles.

Mur de données qui représente également un enjeu pour les laboratoires de recherche et les géants du web.

Règles empiriques sur la quantité de données en apprentissage supervisé profond. On parle de 5000 exemples étiquetés par classe ! Simplement pour avoir une performance comparable à celle qu’obtiendrait avec méthode statistique. Pour dépasser les humains, alors parle de 10 M de cas par classes.

Si seulement dix exemples, laisse tomber ! James Dean.

L’idée est de créer de nouvelles données à partir des données existantes. Parle d’amplification car regarde la sémantique. Pour cela besoin de pouvoir créer des transformations sémantiqmenet invariantes.

Travaux précurseurs. En vision par ordinateur, une pratique courante que de créer des images par conversion géométriques qui préservent la similitude (Simard, Steinkrauss & Platt 2003) (Bunke 1997). Le cas pour ImageNet 2012. De même dans le cas de la voix.

Voulu le faire dans le cas des données textuelles car pas suffisamment de données pour apprentissage profond. Seule technique beaucoup utilisée substitution lexicale qui consiste à remplacer un nom par terme équivalent dans le thesaurus. Zhang & LeCun.

Pourquoi peu répandu ? car données textuelles difficiles, symboliques, compositionnelles, dispersées. Bruitées, ambiguës. Techniques basées sur la descente de gradient ne s’applique pas directement à des données discrètes åGoodfellow, 2016].

Difficile de générer des données textuelles réalistes.

Tout le monde en fait, personne en parle ?

Explication la plus réaliste sans doute sociologique. Plupart de travaux recherche des solutions d’apprentissage de bout en bout avec des niveaux de gradients. Beaucoup de publications mais peu de résultats pour le moment. Besoin de solutions pratiques à court terme.

Objectif étude faisabilité de techniques d’amplification textuelle par prétraitement des données qui soient pratiques, robustes et simples à mettre en œuvre.

Test de certaines méthodes, améliorations. 

- Règle du respect de la distribution statistique
- Règle d’or de la plausibilité
- Règle d’invariance sémantique

Injection de bruit textuel

Injection de fautes d’orthographes

Substitution lexicale avec dictionnaire

Vecteurs-mots

Génération de paraphrases, irréaliste et coûteuse. Plusieurs techniques : par arbres syntaxiques (Michel Gagnon et Lynne Da Silva). Autre technique par rétro traduction.

Exemple travail sur la polarité : critiques de films négatifs ou positif. avec ou sans amplification. Entraînement des modèles. expérimentation Perceptron multicouche, Réseaux récurrents LMCT, longue mémoire court terme et bidirectionnels biLMCT.

Amélioration de 4 à 21% par rapport à la base de référence. 

Avantages et inconvénients. Pratiquement facile à mettre en œuvre et utilisé car se base sur des services en ligne de fournisseurs traditionnels. Mais traitement massif.

Un nouveau mur qui se dresse à l’horizon, celui des modèles pré-entrainées. BERT de Google, Transformers de AI2, ELMo de OpenAI dont le controversé modèle génératif GPT-2 et ULMFiT de Fast.ai pourrait devenir le nouvel état de l’art.

Coût de production collossal. Modèles rendus disponibles sur la bonne volonté des GAFAM. Besoin de disposer des modèles entraînés ouverts.

Reste beaucoup de travail.

Wikipédia un outil essentiel pour travailler sur les langues. A du faire un lemmatiseur pour le français. Versé dans space et analytica. Depuis 2010 les chercheurs francophones ont démissionné.

http://www.linguatechnologies.com/expertise/

https://github.com/ClaudeCoulombe

## discussion

Quand arrête l’amplification ? Comment garantit que les données sont sémantiquement invariantes ?

## Production ontologie

Plusieurs outils pour modéliser

- Protégé, libre et gratuit sans lui n’aurait pas eu de développement possibles du websem
- Stardog Studio, pas encore un éditeur d’ontologie plutôt éditeur de requêtes
- TopQuadrand, Tropbraid composer (plusieurs versions)

Piles techno

Notions associées à la modélisation

- individus, les faits du domaine du discours mais nécessite de pouvoir construire une sémantique pour cela des classes et des propriétés
- classe où range les individus
- propriétés qui vont permettre de construire les relations entre les individus en définissant des propriétés dans le graphe

T-Box et A-Box

Souvent bdd en tableau. Possibilité exprimer en RDFs

Exemple film

- arbre de classes
- arbres de propriétés
- individus

Pour modéliser va représenter

Crée les individus, en fait crée la base de faits, les ressources qui pourront être publiées sur le web. Ensuite va créer la sémantique en créant les classes et les propriétés qui s’appliquent à ces classes.

Une fois qu’a renseigné le fait, peut faire rouler un moteur d’inférence pour déterminer que Connery est un acteur, et que tel autre est un film. Car déclaré que la propriété être acteur est une relation avec un film, etc. Pas possible dans un chiffrier excel ni en technologies relationnelles.

## Discussion

choix de modélisation : ce qui relève ontologie, ce qui relève du vocabulaire

Avant tout l’ontologie doit être utile au cas

Par ailleurs voir si peut servir à d’autres pour le partage

https://www.doremus.org/

## WebSem

LICÉ fondé en 1992, époque des systèmes experts. Laboratoire d’informatique cognitive et environnements d’éducation.

Informatique cognitive terminologie qu’utiisait au Québec pour l’intelligence artificielle. Dans ce cadre, produit plusieurs recherche : 

- Outils de modélisation graphique de connaissance Gémeau
- Des systèmes d’ingénieurie pédagogique, de recommandations. 
- Recherches qui nous ont conduit vers le web sémantique.

Depuis ce temps, l’IA beaucoup évolué, deux saveurs la modélisation des connaissance à la base du websem et d’autre part la nouvelle intelligence artificielle connexionniste basée sur l’extraction de données et apprentissage profond.

Comment marier ces résultats avec les développements du web sémantique. D’un côté beaucoup de régularité, de l’autre ne sait pas toujours ce que veut dire. Comment augmenter utilisation du web sémantique.

Michel Héon sort contrat grande banque canadienne qui a introduit technologies websem pour travail. Gros défi au départ extraction information.

Extract transform and load ETL, le modèle de traitement des données massives en cours. Des questions sur l’établissement de ces informations. Une fois que dans un data lake, question de comment va les interroger.

Rôle du web sémantique, d’abord car outil de standardisation des données. Permet de régler les problèmes d’interopérabilité des données.

Le problème, souvent les gens qui travaillent en IA ne savent pas travailler avec des données en graphes. Ils sont habitués à travailler sur des CSV. Vrai que l’entrée dans un réseau de vecteur pas un graphe mais un tableau matriciel.

Beaucoup de choses possibles, croise des connaissance. Donnée n’existe pas en tant que tel, mais comme machine learning, change de niveau. Besoin de pouvoir contrôler cette connaissance, posse des questions en termes de gouvernance des données.

Claude : question intéressante, devrait être avocat du diable et répondre ’donnez moi un point d’appui et on va soulever le monde’, donnez moi des données et va tout faire. Mais le problème c’est que souvent ces données n’existent pas. GaiBo Garbage input… foutaises en entrée, foutaises en sortie.

Beaucoup concerné par la question de la qualité des données. Peu de raisonnement en traduction, le plus souvent de la reconnaissance de formes. La partie faible des réseaux de neurones, de l’apprentissage automatique, très fort pour la perception mais ne fait que de la reconnaissance de formes. Ne signifie pas que ne puisse pas ajouter des modules. Deep Mind, une machine de turing et possibilité d’ajouter des systèmes complets. Encore loin d’un système qui pourrait faire autre chose que de la perception. Comme le cerveau le fait, pense que pourrai faire de l’apprentissage avec des réseaux de neurones. Automobiles encore des systèmes hybrides. Les réseaux conversationnels, encore 90% de règles et 5 à 10% apprentissage profond pour reconnaître l’émotion de la personne. Tout ce qui se fait en bas d’une seconde, peut le reproduire avec un réseau de neurone car du pattern matching, de la reconnaissance de forme. Pourra peut-être étendre les réseaux de neurones. Parle alors de la connexion entre les approches connexionnistes et les approches symboliques. Le point faibles les données disponibles. Or, quand n’en a pas suffisamment, va rajouter des données, ajouter des attributs pour donner une chance au système d’apprendre. Toujours un compromis entre des données, des connaissances a priori dans le système et autre. Gens qui produisent des applications ne se gênent pas pour intégrer des attributs de haut niveau.

Quant aux graphes vrai que réseaux de neurones le plus souvent des vecteurs. Mais peuvent représenter des graphes. Par ailleurs des réseaux de neurones récursifs dont certains spécialisés pour faire du traitement de graphe. Un nouveau domaine de recherche dans lequel beaucoup de potentiel.

Mais vraies solutions où chercheurs ne craignent pas de combiner les donner.

Belkacem Chikhaoui. IA existe depuis les années 60, travaille avec réseaux neurones. Première conférence 69. Ce qui a permis à l’IA de prendre ampleur deux choses : infrastructures de l’IA le websémantique et les réseaux de neurones.

Au départ le websem reposait plus sur le raisonnement. Mais aujourd’hui une révolution dans ce domaine. En prend les avantages et change manière de travailler avec. On a l’habitude de travailler avec des données structurées avec des matrice, des features et travaille sur les données. Mais pas toujours les données structurées. Alors savoir si les approches traditionnelles fonctionnent toujours. Pour les données multimodales, pas de solution qui permettent de traiter ces cas spécifiques. Pour y faire face travaille en hybridation.

Pour les graphes, s’y intéresse car véhiculent des relations très riches en termes de relation entre les attributs. Des méthodes qui traite ces système, percées récentes de l’IA. Savoir si peut généraliser ces méthodes, pas encore testé. Reste dans le langage naturel, voir dans d’autres contextes pour savoir si peut généraliser.

Des domaines qui ne peuvent pas être représentés sous forme matricielle. Des méthodes développées pour traiter ce genre d’informations. Développé notamment dans le domaine graphique car des informations très riches et que ne veut pas perdre.

Paquette. Teluq travaillé avec Hydroquébec sur une ontologie qui permet requêtes fédérées sur un ensemble de bdd. Ce qui manque complémentarité que peut apporter apprentissage à partir des données. Un travail très fastidieux, dont jamais certain qu’a capté toutes les connaissances qui correspondent aux données. Des approches complémentaires de plus en plus utilisées.

Héon un contexte industriel différent. Pas comme en laboratoire. Tendance à utiliser des chars d’assaults. Informaticiens préférant choisir des approches connues et stables qui ne vont pas planter.

Différence entre la recherche de pointe et l’industrie. En industrie pas encore accès aux réseaux de neurones pour traiter des graphes. Réseaux récursifs (et non pas récurrents) qui traitent des réseaux complexes.

Aujourd’hui quand achète objet, des composants de Machine Learning à l’intérieur des TripleStore. Directement dans les ontologies, reconnaissance de vocabulaires, de formes, etc.

Besoin amplification si peu de données. 

Données massives se caractérise par plusieurs caractéristiques 4 V dont la vérité des données qui détermine à quel point ces données sont réutilisables pour développer des approches robustes pour répondre aux besoins. Peut avoir des tonnes de données, mais savoir si peut disposer d’informations pertinentes pour les modèles d’affaires. Méthodes viennent après.

Quand peu de données plusieurs solutions : coder à bras qui fonctionne toujours. Autre possibilité utiliser les fameux modèles pré-entrainer. Par exemple marche très bien en vision avec ImageNet 1000 classes. Par dessus lesquels peut entraîner un modèle spécifique par exemple les chats qui baillent. Si modèle disponible pré-entrainé. Ajouter 400 ou 1000 photos de chats qui baillent. Fait de l’augmentation de données, tourne, joue sur les couleurs. Peut facilement multiplier par 100 ou par 1000. Alors peut entraîner apprentissage par transfert. Peut entraîner nouvelles couches avec ces modèles là. Et résultats extraordinaires. Les systèmes à base de règle existent encore.

Le travail du scientifique de données, ne travaille pas beaucoup sur les réseaux de neurones. 80% du travail de brasser les données et les compiler. 20% collecte et 60% organisation et nettoyage. Architecture et les méthodes loin derrière.

Pour revenir sur le transfert learning. Pas toujours applicable. Si part d’un domaine particulier ne peut pas forcément l‘appliquer à un autre domaine. Besoin de similarité entre les domaines pour entrainer des modèles pré-entrainés. Différentes couches pour l’apprentissage. Premières couches généralement des couches génériques : exemple des contours, etc. Puis va vers des spécificités. Souvent travaille dans des couches génériques puis essaye de faire de la rétropropagation pour transférer vers domaine spécifique.

Coût pour entraîner neurones. Calcul quantique très prometteur. Plus de secret non plus, dans le passé comme dans le futur.

Prolongement lexical.

Cas des musées où gros jeux de données.

Simple feature de l’inférence du websem peut apporter des bénéfices énormes sur l’entraînement de réseaux neuronaux. Alors sans doute peut généraliser beaucoup de choses. Permettraient sans doute de raffiner et générer de nouvelles données pour les réseaux de neurones.

Intéressant d’avoir l’information classée, car si prend l’exemple des chats où garde système de base et applique nouveaux classifiers. Peut le faire avec vase étrusque, bénéficie de ce qui est entraîné. Alors pourrait facilement à partir d’une photographie dire de quelle époque l’objet date. Peut favoriser le traitement de données très pointues. Puis appliquer techniques d’augmentation de données. Avec le fait que données très fines, peut faire classifiers très précis.

Sans doute seulement le début. Les technologies sémantiques ont probablement le potentiel d’être un réel apport pour l’IA, peuvent servir à classer les données, les pré-traiter et alimenter les systèmes IA. Inversement peut aider. Croisement des approches connexionnistes et formelle sans doute gagnant. Comme cela que fonctionne le cerveau.

Besoin de modèles publics. Ne sais pas où serait la recherche sans Wikipédia. Devrait avoir la même chose pour les modèles de langues, pour les images, etc. 60 000 pour calculer un modèle. Mais si besoin de paramètres..., etc.

Domaine médical ou partage de données pas possible pour diverses raisons. Comment arrive à développer modèle médical si pas les données. Toujours ces questions à réfléchir. Sur les questions éthique, création observatoire mondial.

## Modéliser des exigences : le cas du Réseau canadien d’information sur le patrimoine

Contribuer à la mise en réseau des données ouvertes et liées. Des projets et des modèles nombreux dans le domaine. Essayer de favoriser l’émergence de méthodologies efficaces

- identification des méthodologies, etc.

Rôle du RCIP, créer et maintenir un point d’accès en ligne...

Nomenclature, publication version 4. Publication en SKOS ajout de propriétés : 

- Définition de la source (n° 5)
- etc.

Tout est connecté au concept, pb avec cette propriété. Source qui devrait plutôt figurer sur la définition mais limités par l’outil.

Janvier 2020 pour une publication en LOD avec SPARQL endpoint, vues et négociations de contenu.

Modèle sémantique à partir du modèle existant, ainsi qu’enrichissements avec le Getty = sameAs

licence ouverte

Plusieurs enjeux : inadéquation entre les besoins et les outils. Au moment de la sélection du marché pas conscience de tous les enjeux technologiques. Des problématiques. Compréhension limitée des manipulations techniques (affaire avec consultant externe), malgré documentation des choses difficiles à éclaircir.

Par exemple 007 image. Modèle qui dit qu’une personne qui peut être représentée par une image avec la propriété foaf:img 

Mais pas de personne dans notre modèle. Donc supprimé, mais domaine réapparu… = pb affichage.

Makers in Canada. Édition collaborative et id internationaux.

Modèle par Stephen Hart

CIDOC modèle complexe car centré sur l’événement. Puissant pour la sémantique.

Pas force du modèle être centré sur les personnes mais comme Artefacts probablement basé sur CIDOC. Par ailleurs, BioCRM et évolutions possibles de CIDOC

Deux manières de coder avec BioCRM. Une très descriptive ou décrit Kennedi et son épouse. Autre version tournée par événement. Un mariage et des participants.

Quel périmètre pour Makers : provenance ? propriétaires ? question de la transmission information à inscrire dans des modèles de références. Réconciliation, migration.

Viser décentralisation.

Artefacts projet sur la glace. Contenu varié. Makers qui devrait nous aider à comprendre processus. Horizon de mise à jour dans 3 à 5 ans.

Revoir le modèle de données, etc. Faciliter la production, etc.

Enjeux : Langues officielles, collections autochtones, centralisation vs décentralisation.

10 principes de la Sunlight fondation. Pas encore politique ouverture par défaut.

Affirmation droit sur le droit auteur et vie privée.

Management, un responsable de la politique de données ouvertes et contact dans chaque département.

websemantique.slack.com

### discussion

Conseil des arts Canada

Patrimoine Canadien

Volonté avoir des politiques plus claires pour faire émerger

Change les pratiques de gestion de projets, infuse vers le haut.

Volonté à aller plus loin que les musées dans le domaine de la culture en général.

Discussion sur le CiDOC et son orientation événement (temporel). N’y a-t-il pas un danger d’échec dans cette transformation ? Du côté affaire, des informations pertinentes. Modèle qui devient plus complexe. Quand cherche simplifier le modèle perd l’information que veut gérer. Aime gérer le temps.

Possibilité de placer par graphes nommés, mais complexifie encore. De ce point de vue CIDOC reste pertinent. Plusieurs initiatives de simplification ex. Linked Art.

## Marina









## Présentation

Enseignement

- introduction générale aux métadonnées culturelle, part importante a
- atelier plus pratique web sémantique dans le cadre du DESS en édition numérique

Comment je me suis formé (auto formation HPI, Fun, etc.)

Besoin d’une formation spécialisée ?

Plan culturel numérique et Plan numérique du Québec

Domaine métier

Spécialistes des technologies, spécialisation de domaine

Cas des projets de recherche

- reprises de contenu
- réemploi
- production de données natives

Rôle à jouer des universités et des institutions patrimoniales

- importance des référentiels
- travail avec communautés

