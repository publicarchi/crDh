# AI4LAM Strasbourg

Rencontre régionale Grand Est Intelligence patrimoine et humanités numérique

## Alain Colas, Directeur bib nationale universitaire de Stbg.

Inscrire l’IA dans des processus métiers. Heureux d’accueillir, incarner transmission du savoir et ce qui a été développé depuis 2018 lors du démarrage du projet d’établissement. Mise en place d’un lab dédié aux SHS inauguré il y a un an. Innovation et service d’accompagnement aux projets de recherche en lien avec la bibliothèque numérique.

Une technologie qui commence à arriver à maturitée et commence à être exploitable en projets métiers. Déjà lors de mon stage une équipe travaillant reconnaissance de manuscrits. Début des années 90. Puis ensuite travailler avec Jean Gataigneau qui supervisait la partie programme des bibliothèques et leur avait donné carte blanche. Besoin d’une stratégie.

Remercie Emmanuelle Bermès notamment.

## Emmanuelle Bermès

Communauté assez jeune commencé à se réunir au niveau international en 2018 sous l’impulsion de la Bibliothèque de Stanford et Finlande. Constitution informelle de la communauté. Conférence Fantastic futurs organisée par la Bnf en 2021.

Richesse de cette communauté que de faire le pont entre plusieurs milieux patrimoniaux. Une communauté de praticiens de gens qui essayent d’utiliser ces technologies avec des niveaux d’avancement variables. 

Sondage sur la création d’un chapitre francophone. Deux régions Australie/Nouvelle Zélande, et la communauté francophone. Nous a en effet semblé qu’important de pouvoir échanger avec les collègues dans notre langue avec les autres membres du chapitre : Luc Béllier, Florence Clavaud, Philippe Colomb. D’autant qu’en effet les enjeux linguistiques de ces technologies sont prégnants. Depuis 2 ans, focalisation des médias ChatGPT et grands modèles de langue révolutionné le domaine en facilitant son utilisation. Enjeux linguistiques. 

Mais se souvenir aussi qu’une discipline pas si jeune. Officiellement née dans les années 50, et qui a connu plusieurs périodes. Lors création du chapitre, avant ChatGPT. Or technologie bien installée dans l’industrie mais peu déployée. Se demandait comment stimuler les expérimentations là où des usages potentiels.

## Projet Ritter augmenté

Structuration automatique et extraction d’entités-nommées pour les fichiers biblio. Pour la BNU, utilisation IA à des fins patrimoniales un enjeu stratégique. Des envies et des idées dans les équipes. Des fonds exceptionnels sur lesquels peut travailler. Des compétences, même si important de pouvoir s’associer de nouvelles compétences dans ce domaine si l’on veut favoriser l’émergence de ces technologies. Master en technologie des langues de l’U de Strasbourg qui nous permet d’avoir des expertises fines sur le sujet. 

À l’occasion de différents projets que l’occasion de monter des partenariats et de travailler avec des chercheurs. En fonction des besoins des chercheurs. Mais aussi favoriser les partenariats.

Appel Equipex Biblissima+. Idée de Daniel Bornman responsable fonds patrimoniaux. Apporter ensemble de séquençage technique sur un outil savant. Répertoire bibliographique des livres imprimés en Alsace édité entre 1937 et 1960. Le but arriver à une dématérialisations structurée et enrichie.

- édition numérique sur la plateforme [Estrades](https://estrades.huma-num.fr) (Guillaume Porte)
- versement dans le portail Biblissima+

6650, 7000 entrées très structurées. Auteur titre, adresse bibliographique. Renvois vers les différents répertoires, cote et provenance. Très normé mais évolution en fonction des volumes. Pensait que proche ISBD.

Acquérir le texte OCR, lancement structuration semi-automatisée, puis reconnaissance entités-nommées avec leur alignement sur des référenciels et des répertoires bibliographiques. Outils reconnaissance entités-nommées : Spacy, et service Entity Fishing de HumaNum. Pensait pouvoir utiliser les informations de mise en page pour reconnaissance. Existence d’une table permettant de traiter les choses comme base de connaissance pour améliorer la reconnaissance. Idem noms de lieux. Abréviations. Dates.

Entités nommées très importantes pour pouvoir procéder à une étude intellectuelle. Analyse de réseau. Stage pour état de l’art et bibliothèques NLP. Tester en situation réelle les outils et les hypothèses de travail.

4 stratégies pour l’extraction, la reconnaissance et la classification des entités nommées

**Stratégie par règle** avec extraction sur texte brut avec expressions régulières : exploiter la base principale pour aller chercher les entités dans le texte. Sait ce que rentre mais peut de variabilité, pb océrisation, etc. qui rendent difficle

Table analytique des références

Expression régulière (expression syntaxique de chaîne)

**Comparer avec des modèles spécialisés dans la Reconnaissance entité nommées** en allant regarder ce qui était disponible sur Hugging Face entraîné sur grandes modèles de données et certaines tâches.

Gestion des mots inconnus

Avantage, gestion variants linguistiques. Mais pas de spécialisation sur nos données.

Très facile à utiliser, quelques lignes de codes seulement (5 lignes).

**Spécialisation sur un modèle de données.** Entraînement d’un modèle de type BERT sur nos données

- Adaptation à nos données
- peut avoir déjà bénéficié d’un pré-entraînement pour une tâche ou une langue
- mais demandeur de données

Préparation des données pour l’entrainement. Demande des connaissances en TAL. Etape préalable encodage XML. Puis prétraitement/tokenisation. Segmentation caractères et mots. Grands modèles de langues utilisent leur propre tokenizeur pour pouvoir gérer des mots jamais rencontrés. Donc nouvelle tokenization où mots inconnus divisés en token qui correspond à une sous-chaîne. Doit donc ensuite procéder à un alignement. Pas une tâche complexe mais nécessite connaissances.

**Modèles génératifs via interace web**

 Prompts pas de connaissance.

Varibilité des réponses, demande connaissance pour réintégration.

Exemple interaction avec Chat de Mistral pour la tâche NER. Fonctionne assez bien.

#### Evaluation

Annotation manuelle 50 pages, 214 notices annotées manuellement.

Moyenne F1 mesure capacité extraction et classification (zéro plus mauvaise, 1 meilleure)

Le modèle par règle le plus performant alors que ces stratégies qui étaient censées être dépassées. Ensuite Chat Mistral qui est le seul comparable. Modèle pré-entraîné sur nos données, une amélioration mais très faible. Surtout quand prend moyenne non-pondérée.

La classification automatique par ces modèles pas encore tout à fait généralisable. En tous les cas pas avec si peu de données annotées. Pourrait encoder plus de données mais risque de produire un modèle très spécifique à nos données. Questions sur les modèles génératifs pour rendre plus reproductible ces informations. Utilisation de la structure des notices pas encore exploitée. Pose donc la question d’intégration de ces traitements automatisés dans la chaîne numérique. Pose la question des volumes de données disponibles, comment les pré-traite, etc. Sans doute pas tout à fait au point même si du potentiel.

## CiSaMe Circulation des Savoirs médiévaux au XIIe siècle

HTR écriture médiévale. Histoire du droit et droit canon.

Projet ANR, EPHE, Université Orléans. Unistra 2023-2026. Étudier la circulation des savoirs médiévaux que l’on qualifie de juridique, théologique etc. que postules séparés et qu’étudie en silos. Voire de quelle manière davantage construite de manière mêlées.

Regarder ciculation des sources dans une optique d’histoire des savoirs. Faire l’histoire de la distinction ou pas des disciplines ou une cartographie des disciplines à travers des sources scolaires.

Corpus partie édité, mais aussi parti manuscrit. Donc HTR. Construire une base de données, et pouvoir à partir d’outils textométrique travailler sur des notions et des idées mais aussi des références textuelles (autorités).

Projet ThéoDDISés financé par le Réseau national des MSH 2020-2022. Théologie et Droit au XIIe siècle. Amorce thématique. Géré par MSH Val-de-Loire à l’époque. Laurence Ragot et Georges Fi??

Abordait les choses avec grande naïveté.

Pourquoi HTR, rencontre gens du projet INDB travaillant sur des archives comptables, chantier ND.

eScriptorium. Entraînement de notre propre modèle. Choix d’abréviations développées.

Défaut de connaissance des acteurs locaux. Compris aussi que pouvait travailler avec des transcriptions sales.

Guidelines, suivant Arianne Pinche.

Développement modèles. 

Plateforme PHUN de la Misha.

### Discussion

En dehors de l’exploration textométrique, envisagez-vous l’utilisation de LLM pour repérer des formulations similaires (hormis citation)

Déjà compliqué avoir taux de reconnaissance entité-nommée suffisant pour exploration des allégations. Actuellement exploration LLM encore inenvisagé. Mais tel travail pour arriver déjà.

Passé à 95% reconnaissance grâce à du fine-tuning.

## Gallica Images

L’intelligence artificielle pour l’accès aux grands corpus d’image.

Alexandra Adamová

Un projet qu ne consiste pas en un projet de recherche mais plutôt un projet qui vise à entrer en production à l’échelle industrielle. Développements qui débutent seulement aujourd’hui après une longue phase de test.

Rappel historique sur la constitution des corpus de Gallica, constitués au fil des ans grâce à la numérisation patrimoniale. Première mise en ligne 1997. Enrichissement par des apports documentaires des bibliothèques partenaires. Cet élargissement du périmètre documentaire s’est accompagné du perfectionnement des techniques de numérisations

- 2000 OCR qui permet de débuté
- 2017 démocratisation modèle IA et fouille image

Projet issus de la démocratisation de l’accès IA et constat du fait que ces grands corpus numérisés ne sont pas facilement accessibles car les métadonnées descriptives ne permettent le plus souvent pas de faire une recherche à l’intérieur des documents.

10M documents, plus de 140 000 ms, plus 150 000 iconographiques...

Objectifs du projet

Engager un travail de recessement et d’indexation de toutes les illustrations présentes dans la collectionpour leur octroyer une meilleure visibilité

Mettre en place une solution de fouille d’iamges basées sur des technologies évolutives open source en tenant compte des projets et des usages.

...

La place des modèles algorithmiques

Sortir du mode de recherche et développement, entrer dans le mode d’indusrtialisation en tirant partie de cette expérience. En utilisant traitements de segmentation et de classification (recherche similarité visuelle, etc.). Traitements liés à la détection d’objet et reconnaissance de formes.

- Détecter les illustrations dans une vue numérisée avec coordonnées positionnelles. 
- Identifier la technique
- Permettre d’identifier les fonctions de l’image, scripteurs conceptuels
- par l’analyse de l’image et palettre colirométrique, collecter ces données pour offrir une recherche de ce type.

Chronologie projet IA

- R&D
- stratégie IA
- Industrialiser (chosiir un partenaire industriel, choisir des modèles algo)
- Développer (créer, entraîner, déployer)
- Mettre à disposition des publics (données / APIs, Interfaces)

Long et heureux de débuter développement qui devrait pouvoir donner accès aux contenus à partir de 2026.

Quelques cas d’usages, simples et logiques : base d’illustration constituée qui devrait permettre aux chercheurs, grand public, et pro de faire des recherches dans ce grand corpus. Aider des usages pro, catalogage et médiation. 

Sans entrer dans le détails des projets de recherche, nommer quelques uns qui ont eu de l’importance pour le projet. Celui avec Cergy sur corpus 3000 images pour tester approches basées sur analyse d’histogrammes de couleurs. Création de Gallica Pix 2017 a permis de faire une preuve de concept sur la recherche multimodale à partir d’illustrations (apprentissage machine, classification objets, entraînements locaux et services commerciaux). Gallica Snooc avec INRIA pour tester le concept de recherche par similarité visuelle. Projets de moindre envergure avec élèves ingénieurs d’EPITA sur le test de différents modèles de classification.

Première tentative de sortie R&D en 2020, avec rédaction du cahier des charges industriel et technique en vue de la recherche d’un partenaire industriel. Premier marché infructeux en raison financement insuffisant. Nouveau appel en 2022 grâce à financement Caisse des dépôts. Marché en mars 2023. Avril-mai phase de précision.

Bib stbg, INHA, Partenaires isntitutionnelsRavaness ?

Développer chaîne de ttmt constituée de deux briques : illustration pour extraction des illustrations, et création de descripteurs complémentaires. Données enregistrées dans bdd interrogeable via API.

- Parcours des documents Gallica page à page
- Repère et identité des illustrations sur la page
- Générer une idnexation dédiée pour chauqe image
- insertion des données dans la bdd Gallica Images

Pistes approche méthodologique (actuelle)

- constituer des jeux de donnés pour lancer des entraînements
- Approche centrée sur les données plutôt que les modèles (rester ouvert par rapport à l’état de l’art des modèles disponibles). 
  Partant de Detectron2, CNN, Cascade RCNN Yolo-NAS = modèle vocabulaire fermé. Pour le moment modèles ouverts de description textuelles CLIP, etc. exclu car ne génèrent pas de mots clefs. Cependant pas complètement écartés, pourront être utilisé pour enrichir autres jeux de données ou pré-entraînement
  Compte tenu évolution rapide des modèles, rester en mode veille sur les modèles à fort potentiel pour pouvoir les introduire ou les utiliser dans les chaînes.
- Modèles pré-entraîné open source pour les cas simple. Réentraînement spécifique pour les cas plus complexes. Compromis : complexité et performance. 
- Possible de rester ouvert aux évolutions des modèles pour envisager la mise en place d’une architecture modulaire pour pouvoir introduire nouveaux modèles si nécessaire
- Approche itérative, part de collections iconographiques, puis élargissement documentaires avec monographies et périodiques puis objets.

Développements en interne chez le partenaire indus. Phase déploiement premier trimestre 2025

Premiers enrtaînement septembre cette année

Démonstrateur API recherche 1er semestre 2025.

### Discussion

Plateforme qui sera assynchrone. 

Pour le temps de traitement une estimation a été faite. Mais pas possible de répondre.

Je comprends qu’il s’agit surtout avec ce projet d’engager une industrialisation, toutefois quelle est la place réservée dans le projet à l’annotation collaborative ? Les corpus traités sont d’une grande richessse (typologique, historique, formelle) par rapport aux données d’entraînement habituellement utilisés dans le domaine de la reconnaissance d’image. Aussi, pouvoir bénéficier d’une postvalidation pourrait permettre de générer de nouvelles données de terrain sans doute très utiles.

Pour le moment non car traitement automatique mais dans l’avenir post-projet pourrait sans doute imaginer des projets autour de la correction. Ici concentrés sur l’automatisation des processus.

Question sur l’évaluation par les humains. Et validation du bénéfice par rapport à un traitement humain.

Des comparaisons seront faites. Mais ne prévoit pas de validation unitaires sur la qualité des données qui restortiront de la chaîne.

## Du master TNAH au réseau des MSH. Formations et services à la recherche autour de l’IA pour les humanités numériques

Emmanuel Bermès, Elsa Van Kote

Deux masters numériques ENS, ce master ingénieurie, un autre orienté recherche.

Trois types de profils en ressortent :

- archives numériques, opération dématérialisation fonds
- bibliothécaires numériques : ts type bib et traitement num
- ingénieurs de données : entreprises privées, dev applications

Nouvelles maquettes cette année

- 1ère année focus outils et méthodes, traditionnels ou numérique
- histoire et connaissance patrimoine écrit documentaire
- initiation outils documentaires
- 2e année gros semestre techno numérique
- stage de mise en pratique

Étudiante qui explique que formation ne fait pas d’eux pour autant des informaticiens. Travail nécessaire avec des spécialistes. Besoin d’une culture numérique. Clefs de compréhension et vocabulaire qui va permettre d’aborder diversité d’acteurs concernés par de tels projets pour les faire travailler ensemble.

Compétences dont a besoin

- traduction : parler à la fois langue shs et celle informaticiens, permettre aux deux de communiquer en utilisant un vocabulaire que
- pédagogie, expliquer choses complexes
- comprendre contexte
- gestion de projet, enjeux de données, qualité et évaluation

Considérer l’acompagnement à la recherche comme une compétence sur laquelle a la prétention de former des étudiants en leur donnant des compétences généralistes. précieux dans les MSH

Compétences IA. Mascotte ADEMEC mouton à 5 pattes ! IA ≠ édition ≠ bdd ≠ programmation logicielle : attention à l’accumulation des tâches.

Ne peut pas tout faire. Informatique SHS plus ce qu’elle était il y a 15 ans. Ne peut pas former des gens qui auront toutes ces compétences à haut niveau. Il faut donc accepter que les profils se spécialisent.

IA Ajouts théoriques. Mise en pratique HTR dès le M1. En M2 atelier LLM et annotation. Création atelier Python pour données de la recherche optionnel (où SPACY). Acquisition de données et annotations en dataScience commun avec master recherche. Aussi possibilité certification Data. Mineure Data sur dossier.

### Discussion

Quelle spécialisation ? Cours similaires, mais formation spécifique aussi apportée par l’environnement de l’ENC qui est une école très tournée vers les institutions patrimoniales. Bibliothèques et archives. Un peu moins les musées. Donc profils formation spécifique pour travailler sur les collections patrimoniales dans les institutions. Master d’ailleurs sous la mention archives et pas HN.

Vu qu’une spécialisation. Parfois chercheurs qui considère que ne fait pas partie des compétences d’un chercheur mais relève de l’ingénieurie. Ne devrait-on pas intégrer dans l’enseignement disciplinaire directement ?

En effet le revers de la médaille de la structuration des DH. Plus se rapproche d’une pratique disciplinaire plus justifie les autres de ne pas s’en occuper. Toutefois une approche spécifique à l’ENC, proche des collections. Considère que pas de distinction entres les objets et les données. Un continuum documentaire. Un continuum collection, raison pour laquelle parle avec Jean-Baptiste Camps d’humanités computationnelles. Ici besoins spécifiques. Par contre clairement besoin d’une littératie numérique commune.

## Frédéric Clavert, Les sources de l’historien à l’ère numérique

Histoire internationale, étude à Stbg

Aujourd’hui histoire collective. Archives en ligne, et aujourd’hui IA et mémoire collective Sara B.

X mois après ChatGPT. Aujourd’hui naturalisé

Benoît Majerus pour analyser registres commerce. XX pour chatbot. Questions éthiques sur lesquelles doit réfléchir.

Utilisation la plus répandue, écriture de code en Python. Exemple Scraping.

À terme, et change sans doute la manière dont envisageait les HN où collaboration avec informaticiens. Aujourd’hui moment beaucoup plus loin car beaucoup de choses que peut faire soi-même. Quelque chose qui a changé par rapport à 2010. Surtout une nouvelle agentivité, qui est celle de l’IA non-humaine. Devra y réfléchir. De même que voit de plus en plus des collègues écrire avec des chatbot ou de grands modèles de données, parfois sans y réfléchir. Nbx choses qui vont devenir « des pratiques numériques discrètes » cf. travail avec Caroline Muller.

Voir les systèmes d’IA générative comme des produits de quelque chose de commun et sources primaires et parfois secondaires. LLM besoin d’entraînement, peut différer selon les sociétés.

Différentes étapes, coprus, entraînement, renforcement, travailleurs du clic.

- Common crawl proche archives du web. Difficulté pour évaluer les biais, etc.
- Tunninge et jeux d’instructions. Lui soumet des questions et des réponses que pense bonnes

Enjeux éthiques. Car tous des compétences pour corriger. 

Grands systèmes génératifs qui produisent également des sources. Les modèles proprement dit, mais aussi les textes et les images qu’ils produisent dont se demande si n’entraîneront pas des problèmes à termes, mais aussi les prompts (données utilisateurs).

Produit des modèles aussi intéressants car pour l’historiens une manière d’évaluer les données d’entraînement (leurs biais) mais aussi de savoir ce que les utilisateurs de ces systèmes.

Les prompts intéressants en eux-même car une possibilité d’accéder à l’imagination des utilisateurs. Résultat plaire au plus grand monde, possibilité hégémonique. Mais aussi négociation avec le système.

Mais quel est le statut de ces archives ? Souvent pas là. Quid des prompts (quantité, pb de droits).

### Discussion

Exemple de prompts collectés dans des forums avec images sur des serveurs Discords. Petits moteurs de recherche mais données peu pérennes et souvent pas daté or rapports entre le présent et le passé. Rien en provenance de ChatGPT.

## Apollon : un projet d’humanités numériques

Un projet né du hasard, ne pensait pas ce projet. Les deux co-porteurs pouvaient imaginer un projet mais sans doute pas sur Aristote et des textes en grec ancien.

Histoire romaine. Présenté un réseau aristotellicien, pensée politique qui oblige à étudier des textes fondateurs comme celui d’Aristote. Alors sollicitée pour un tel projet. Risque. Enrichissement constant.

Idée de permettre, au-delà de la recherche par mot, des notions. Un projet qui met au service des chercheurs en pensée politique ancienne, un bagage d’information de passages qui devront ensuite être lus et réélaborés par les chercheurs eux-mêmes. Il s’agit grâce à des outils en IA d’offrir des outils méthodologiques au chercheur.

Difficulté apprendre langages différents.

Quel texte ? une traduction manuscrite qui permet d’établir un texte toujours provisoire.

Rôle des antiquisant, identifier des mots qui sont essentiels pour la compréhension des textes. Les informaticiens produiront une liste de passage qui permettront de retrouver pas seulement les passages où ces termes se retrouvent mais aussi toutes les notions relatives à ces termes. Passages qui permettront à l’historien de réfléchir à la complexité des termes choisis et de ces notions.

Un projet qui ne remplace pas les chercheurs mais valorise le travail des chercheurs. Lui permet d’arriver à plus. De produire une réflexion plus rapide et plus certaine dans les résultats.

Antiquisant besoin de reconstituer un monde qui n’existe plus : seulement 10% des textes parvenus, et par ailleurs doit travailler avec des lacunes. Pour la modélisation intéressant car ensemble qui nous parvient d’un temps arrêté.











